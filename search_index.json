[["index.html", "Reinforcement Learning for Business (RL) Course notes About the course notes Learning outcomes Purpose of the course Learning goals of the course Reinforcement learning textbook Course organization Programming software Acknowledgements and license Exercises", " Reinforcement Learning for Business (RL) Course notes Lars Relund Nielsen 2022-09-19 About the course notes This site contains course notes for the course “Reinforcement Learning for Business” held at Aarhus BSS. It consists of a set of learning modules. The course is an elective course mainly for the Operations and Supply Chain Analytics and Business Intelligence programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated. Learning outcomes By the end of this module, you are expected to: Understand the prerequisites and the goals for the course. Have downloaded the textbook. Know how the course is organized. Installed R and RStudio. Annotated the online notes. The learning outcomes relate to the overall learning goals number 3, 5 and 6 of the course. Purpose of the course The purpose of this course is to give an introduction and knowledge about reinforcement learning (RL). RL may be seen as An approach of modelling sequential decision making problems. An approach for learning good decision making under uncertainty from experience. Mathematical models for learning-based decision making. Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions. Estimating and finding near optimal decisions of a stochastic process with sequential decision making. A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience. RL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions. A classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge. The course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control. Learning goals of the course After having participated in the course, you must, in addition to achieving general academic skills, demonstrate: Knowledge of RL for Bandit problems Markov decision processes and ways to optimize them the exploration vs exploitation challenge in RL and approaches for addressing this challenge the role of policy evaluation with stochastic approximation in the context of RL Skills to define the key features of RL that distinguishes it from other machine learning techniques discuss fundamental concepts in RL describe the mathematical framework of Markov decision processes formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives apply fundamental techniques, results and concepts of RL on selected RL problems. given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model) Competences to identify areas where RL are valuable select and apply the appropriate RL model for a given business problem interpret and communicate the results from RL Reinforcement learning textbook The course uses the free textbook Reinforcement Learning: An Introduction by Sutton and Barto (2018). The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course. Sutton and Barto’s book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it’s totally free. Just follow the citation link to download it. Course organization Each week considers a learning module. A learning module is related to a chapter in the textbook. The learning path in a typical week are Before lectures: Read the chapter in the textbook and consider the extra module material. Lectures (at campus). After lectures: Module Exercises (in groups). Lectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials are given and we focus on a specific RL problem. This module gives a short introduction to the course. Next, the site consists of different parts each containing teaching modules about specific topics: Part I gives you a general introduction to RL and the bandit problem. Part II consider RL sequential decision problems where the state and state and action spaces are small enough so values can be represented as arrays, or tables. We start by considering bandit problems (Module 2) a RL problem in which there is only a single state. Next, Markov decision processes (the full model known) are considered as a general modelling framework (Module 3) and the concept of policies and value functions are discussed (Module 4). Model-based algorithms for finding the optimal policy (dynamic programming) are given in Module 5. The next modules consider model-free methods for finding the optimal policy, i.e. methods that do not require full knowledge of the transition probabilities and rewards of the process. Monte Carlo sampling methods are presented in Module 6 and … The appendix contains different modules that may be helpful for you including hints on how to work in groups, how to get help if you are stuck and how to annotate the course notes. Programming software We use R as programming software and it is assumed that you are familiar with using R. R is a programming language and free software environment. R can be run from a terminal but in general you use an IDE (integrated development environment) RStudio for running R and to saving your work. R and RStudio can either be run from your laptop or using RStudio Cloud which run R in the cloud using your browser. It is assumed as a prerequisite that you know how to use R. If you need a brush-up on your R programming skills then have a look at Module A in the appendix. Acknowledgements and license Materials are taken from various places: The notes are based on Sutton and Barto (2018). The bookdown skeleton and some notes are based on the Tools for Analytics course. Some notes are adopted from Scott Jeen, Bryn Elesedy and Peter Goldsborough. Some slides are inspired by the RL specialization at Coursera. Some exercises are taken from Sutton and Barto (2018) and modified slightly. I would like to thank all for their inspiration. This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! Exercise - How to annotate The online course notes can be annotated using hypothes.is. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help indicate spell errors, unclear content etc. in the notes. Sign-up at hypothes.is. If you are using Chrome you may also install the Chrome extension. Go back to this page and login in the upper right corner (there should be some icons e.g. &lt;). Select some text and try to annotate it using both a private and public annotation (you may delete it again afterwards). Go to the slides for this module and try to annotate the page with a private comment. Exercise - Templates A template in RMarkdown of the course notes and exercises are available at GitHub. You can download the repository and keep your own notes during the course by having an R project with it. Open R studio and do: File &gt; New Project … &gt; Version Control &gt; Git. Add https://github.com/bss-osca/rl-student as repository url and create the project. Run renv::restore() from the R command line to install needed packages (this may take some time). If you experience errors then try to install the packages one at a time using install.packages(\"pkg name\"). Open e.g. the file 01_rl-intro.Rmd and try to knit it using the Knit button in the upper left corner. A html file with the output will be made. You should be able to add you own notes and solve the exercises using the Rmd file for each module. References "],["mod-rl-intro.html", "Module 1 An introduction to RL 1.1 Learning outcomes 1.2 Textbook readings 1.3 What is reinforcement learning 1.4 RL and Business Analytics 1.5 RL in different research deciplines 1.6 RL and machine learning 1.7 The RL data-stream 1.8 States, actions, rewards and policies 1.9 Exploitation vs Exploration 1.10 RL in action (Tic-Tac-Toe) 1.11 Summary 1.12 Exercises", " Module 1 An introduction to RL This module gives a short introduction to Reinforcement learning. 1.1 Learning outcomes By the end of this module, you are expected to: Describe what RL is. Be able to identify different sequential decision problems. Know what Business Analytics are and identify RL in that framework. Memorise different names for RL and how it fits in a Machine Learning framework. Formulate the blocks of a RL model (environment, agent, data, states, actions, rewards and policies). Run your first RL algorithm and evaluate on its solution. The learning outcomes relate to the overall learning goals number 3, 5, 6, 9 and 11 of the course. 1.2 Textbook readings For this week, you will need to read Chapter 1-1.5 in Sutton and Barto (2018). Read it before continuing this module. Slides for this module can be seen here. You do not have to look at them before the lecture! 1.3 What is reinforcement learning RL can be seen as An approach of modelling sequential decision making problems. An approach for learning good decision making under uncertainty from experience. Mathematical models for learning-based decision making. Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions. Estimating and finding near optimal decisions of a stochastic process with sequential decision making. A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience. Sequential decision problems are problems where you take decisions/actions over time. As an agent, you base your decision on the current state of the system (a state is a function of the information/data available). At the next time-step, the system have moved (stochastically) to the next stage. Here new information may be available and you receive a reward and take a new action. Examples of sequential decision problems are (with possible actions): Playing backgammon (how to move the checkers). Driving a car (left, right, forward, back, break, stop, …). How to invest/maintain a portfolio of stocks (buy, sell, amount). Control an inventory (wait, buy, amount). Vehicle routing (routes). Maintain a spare-part (wait, maintain). Robot operations (sort, move, …) Dairy cow treatment/replacement (treat, replace, …) Recommender systems e.g. Netflix recommendations (videos) Since RL involves a scalar reward signal, the goal is to choose actions such that the total reward is maximized. Note actions have an impact on the future and may have long term consequences. As such, you cannot simply choose the action that maximize the current reward. It may, in fact, be better to sacrifice immediate reward to gain more long term reward. RL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance: totally random trials (in the start), sophisticated tactics and superhuman skills (in the end). That is, as the agent learn, the reward estimate of a given action becomes better. As humans, we often learn by trial and error too: Learning to walk (by falling/pain). Learning to play (strategy is based on the game rules and what we have experienced works based on previous plays). This can also be seen as learning the reward of our actions. 1.4 RL and Business Analytics Business Analytics (BA) (or just Analytics) refers to the scientific process of transforming data into insight for making better decisions in business. BA can both be seen as the complete decision making process for solving a business problem or as a set of methodologies that enable the creation of business value. As a process it can be characterized by descriptive, predictive, and prescriptive model building using “big” data sources. Descriptive Analytics: A set of technologies and processes that use data to understand and analyze business performance. Descriptive analytics are the most commonly used and most well understood type of analytics. Descriptive analytics categorizes, characterizes, consolidates, and classifies data. Examples are standard reporting and dashboards (KPIs, what happened or is happening now?) and ad-hoc reporting (how many/often?). Descriptive analytics often serves as a first step in the successful application of predictive or prescriptive analytics. Predictive Analytics: The use of data and statistical techniques to make predictions about future outputs/outcomes, identify patterns or opportunities for business performance. Examples of techniques are data mining (what data is correlated with other data?), pattern recognition and alerts (when should I take action to correct/adjust a spare part?), Monte-Carlo simulation (what could happen?), neural networks (which customer group are best?) and forecasting (what if these trends continue?). Prescriptive Analytics: The use of optimization and other decision modelling techniques using the results of descriptive and predictive analytics to suggest decision options with the goal of improving business performance. Prescriptive analytics attempt to quantify the effect of future decisions in order to advise on possible outcomes before the decisions are actually made. Prescriptive analytics predicts not only what will happen, but also why it will happen and provides recommendations regarding actions that will take advantage of the predictions. Prescriptive analytics are relatively complex to administer, and most companies are not yet using it in their daily course of business. However, when implemented correctly, it can have a huge impact on business performance and how businesses make decisions. Examples on prescriptive analytics are optimization in production planning and scheduling, inventory management, the supply chain and transportation planning. Since RL focus optimizing decisions it is Prescriptive Analytics also known as sequential decision analytics. Figure 1.1: Business Analytics and competive advantage. Companies who use BA focus on fact-based management to drive decision making and treats data and information as a strategic asset that is shared within the company. This enterprise approach generates a companywide respect for applying descriptive, predictive and prescriptive analytics in areas such as supply chain, marketing and human resources. Focusing on BA gives a company a competive advantage (see Figure 1.1). BA and related areas: In the past Business Intelligence traditionally focuses on querying, reporting, online analytical processing, i.e. descriptive analytics. However, a more modern definition of Business Intelligence is the union of descriptive and predictive analytics. Operations Research or Management Science deals with the application of advanced analytical methods to help make better decisions and can hence be seen as prescriptive analytics. However, traditionally it has been taking a more theoretical approach and focusing on problem-driven research while BA takes a more data-driven approach. Logistics is a cross-functional area focusing on the effective and efficient flows of goods and services, and the related flows of information and cash. Supply Chain Management adds a process-oriented and cross-company perspective. Both can be seen as prescriptive analytics with a more problem-driven research focus. Advanced Analytics is often used as a classification of both predictive and prescriptive analytics. Data science is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured and can be seen as Business analytics applied to a wider range of data. 1.5 RL in different research deciplines RL is used in many research fields using different names: RL (most used) originated from computer science and AI. Approximate dynamic programming (ADP) is mostly used within operations research. Neuro-dynamic programming (when states are represented using a neural network). RL is closely related to Markov decision processes (a mathematical model for a sequential decision problem). Figure 1.2: Adopted from Silver (2015). 1.6 RL and machine learning Different ways of learning: Supervised learning: Given data \\((x_i, y_i)\\) learn to predict \\(y\\) from \\(x\\), i.e. find \\(y \\approx f(x)\\) (e.g. regression). Unsupervised learning: Given data \\((x_i)\\) learn patterns using \\(x\\), i.e. find \\(f(x)\\) (e.g. clustering). RL: Given state \\(x\\) you take an action and observe the reward \\(r\\) and the new state \\(x&#39;\\). There is no supervisor \\(y\\), only a reward signal \\(r\\). Your goal is to find a policy that optimize the total reward function. Figure 1.3: Adopted from Silver (2015). 1.7 The RL data-stream RL considers an agent in an environment: Agent: The one who takes the action (computer, robot, decision maker). Environment: The system/world where observations and rewards are found. Data are revealed sequentially as you take actions \\[(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \\ldots).\\] At time \\(t\\) the agent have been taken action \\(A_{t-1}\\) and observed observation \\(O_t\\) and reward \\(R_t\\): Figure 1.4: Agent-environment representation. This gives us the history at time \\(t\\) is the sequence of observations, actions and rewards \\[H_t = (O_0, A_0, R_1, O_1, \\ldots, A_{t-1}, R_t, O_t).\\] 1.8 States, actions, rewards and policies The (agent) state \\(S_t\\) is the information used to take the next action \\(A_t\\): Figure 1.5: State and action. A state depends on the history, i.e. a state is a function of the history \\(S_t = f(H_t)\\). Different strategies for defining a state may be considered. Choosing \\(S_t = H_t\\) is bad since the size of a state representation grows very fast. A better strategy is to just store the information needed for taking the next action. Moreover, it is good to have Markov states where given the present state the future is independent of the past. That is, the current state holds just as much information as the history, i.e. it holds all useful information of the history. Symbolically, we call a state \\(S_t\\) Markov iff \\[\\Pr[S_{t+1} | S_t] = \\Pr[S_{t+1} | S_1,...,S_t].\\] That is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states. Note that we can always find some Markov state. Though the smaller the state, the more “valuable” it is. In the worst case, \\(H_t\\) is Markov, since it represents all known information about itself. The reward \\(R_t\\) is a number representing the reward at time \\(t\\) (negative if a cost). Examples of rewards are Playing backgammon (0 (when play), 1 (when win), -1 (when loose)). How to invest/maintain a portfolio of stocks (the profit). Control an inventory (inventory cost, lost sales cost). Vehicle routing (transportation cost). The goal is to find a policy that maximize the total future reward. A policy is the agent’s behaviour and is a map from state to action, i.e. a function \\[a = \\pi(s)\\] saying that given the agent is in state \\(s\\) we choose action \\(a\\). The total future reward is a currently not defined clearly. Let the value function denote the future reward in state \\(s\\) and define it as the expected discounted future reward: \\[V_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots | S = s).\\] Note the value function is defined using a specific policy and the goal is to find a policy that maximize the total future reward in all possible states \\[\\pi^* = \\arg\\max_{\\pi\\in\\Pi}(V_\\pi(s)).\\] The value of the discount factor is important: Discount factor \\(\\gamma=0\\): Only care about present reward. Discount factor \\(\\gamma=1\\): Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite. Discount factor \\(\\gamma&lt;1\\): Rewards near to the present more beneficial. Note \\(V(s)\\) will converge to a number even if the time-horizon is infinite. 1.9 Exploitation vs Exploration A key problem of reinforcement learning (in general) is the difference between exploration and exploitation. Should the agent sacrifice what is currently know as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? Exploitation takes the action assumed to be optimal with respect to the data observed so far. This, gives better predictions of the value function (given the current policy) but prevents the agent from discovering potential better decisions (a better policy). Exploration does not take the action that seems to be optimal. That is, the agent explore to find new states and update the value function for this state. Examples in the exploration and exploitation dilemma are for instance movie recommendations: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration) or oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration). 1.10 RL in action (Tic-Tac-Toe) The current state of the board is represented by a row-wise concatenation of the players’ marks in a 3x3 grid. For example, the 9 character long string \"......X.O\" denotes a board state in which player X has placed a mark in the first field of the third column whereas player O has placed a mark in the third field of the third column: .table-bordered th, .table-bordered td { border: 1px solid black !important; } . . . . . . X . O That is, we index the fields row-wise: 1 2 3 4 5 6 7 8 9 The game is continued until all fields are filled or the game is over (win or loose). The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. Reward for a player is 1 for ‘win’, 0.5 for ‘draw’, and 0 for ‘loss’. These values can be seen as the probability of winning. Examples of winning, loosing and a draw from player Xs point of view: . . X . X . X O O X . X . X . O O O X X O O O X X X O Note a state can be also be represented using a state vector of length 9: stateStr &lt;- function(sV) { str &lt;- str_c(sV, collapse = &quot;&quot;) return(str) } stateVec &lt;- function(s) { sV &lt;- str_split(s, &quot;&quot;)[[1]] return(sV) } sV &lt;- stateVec(&quot;X.X.X.OOO&quot;) sV #&gt; [1] &quot;X&quot; &quot;.&quot; &quot;X&quot; &quot;.&quot; &quot;X&quot; &quot;.&quot; &quot;O&quot; &quot;O&quot; &quot;O&quot; Given a state vector, we can check if we win or loose: #&#39; Check board state #&#39; #&#39; @param pfx Player prefix (the char used on the board). #&#39; @param sV Board state vector. #&#39; @return A number 1 (win), 0 (loose) or 0.5 (draw/unknown) win &lt;- function(pfx, sV) { idx &lt;- which(sV == pfx) mineV &lt;- rep(0, 9) mineV[idx] &lt;- 1 mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE) if (any(rowSums(mineM) == 3) || # win any(colSums(mineM) == 3) || sum(diag(mineM)) == 3 || sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 3) return(1) idx &lt;- which(sV == &quot;.&quot;) mineV[idx] &lt;- 1 mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE) if (any(rowSums(mineM) == 0) || # loose any(colSums(mineM) == 0) || sum(diag(mineM)) == 0 || sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 0) return(0) return(0.5) # draw } win(&quot;O&quot;, sV) #&gt; [1] 1 win(&quot;X&quot;, sV) #&gt; [1] 0 We start with an empty board and have at most 9 moves (a player may win before). If the opponent start and a state denote the board before the opponent makes a move, then then a draw game may look as in Figure 1.6. We start with an empty board state \\(S_0\\), and the opponent makes a move, next we choose a move \\(A_0\\) (among the empty fields) and we end up in state \\(S_1\\). This continues until the game is over. Figure 1.6: A draw. 1.10.1 Players and learning to play Assume that we initially define a value \\(V(S)\\) of each state \\(S\\) to be 1 if we win, 0 if we loose and 0.5 otherwise. Most of the time we exploit our knowledge, i.e. choose the action which gives us the highest estimated reward (probability of winning). However, some times (with probability \\(\\epsilon\\)) we explore and choose another action/move than what seems optimal. These moves make us experience states we may otherwise never see. If we exploit we update the value of a state using \\[V(S_t) = V(S_t) + \\alpha(V(S_{t+1})-V(S_t))\\] where \\(\\alpha\\) is the step-size parameter which influences the rate of learning. Let us implement a RL player using a R6 class and store the values using a hash list. We keep the hash list minimal by dynamically adding only states which has been explored or needed for calculations. Note using R6 is an object oriented approach and objects are modified by reference. The internal method move takes the previous state (from our point of view) and the current state (before we make a move) and returns the next state (after our move) and update the value function (if exploit). The player explore with probability epsilon if there is not a next state that makes us win. PlayerRL &lt;- R6Class(&quot;PlayerRL&quot;, public = list( pfx = &quot;&quot;, # player prefix hV = NA, # empty hash list (states are stored using a string key) control = list(epsilon = 0.2, alpha = 0.3), clearLearning = function() clear(self$hV), initialize = function(pfx = &quot;&quot;, control = list(epsilon = 0.2, alpha = 0.3)) { self$pfx &lt;- pfx self$control &lt;- control self$hV &lt;- hash() }, finalize = function() { # cat(&quot;FIN\\n&quot;) clear(self$hV) }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) # possible places to place our move state &lt;- stateStr(sP) # state as a string if (!has.key(state, self$hV)) self$hV[[state]] &lt;- 0.5 # if the state hasn&#39;t a value then set it to 0.5 (default) # find possible moves and add missing states keys &lt;- c() keysV &lt;- NULL for (i in idx) { # find possible moves sV[i] &lt;- self$pfx str &lt;- str_c(sV, collapse = &quot;&quot;) keys &lt;- c(keys, str) keysV &lt;- rbind(keysV, sV) sV[i] &lt;- &quot;.&quot; # set the value back to default } # add missing states of next sP idx &lt;- which(!has.key(keys, self$hV)) if (length(idx) &gt; 0) { for (i in 1:nrow(keysV)) { self$hV[keys[i]] &lt;- win(self$pfx, keysV[i,]) } } # cat(&quot;Player&quot;, pfx, &quot;\\n&quot;) # print(self$hV) # update and find next state val &lt;- values(self$hV[keys]) # cat(&quot;Moves:&quot;); print(val) m &lt;- max(val) if (rbinom(1,1, self$control$epsilon) &gt; 0 &amp; any(val &lt; m) &amp; m &lt; 1) { # explore idx &lt;- which(val &lt; m) idx &lt;- idx[sample(length(idx), 1)] nextS &lt;- names(val)[idx] # cat(&quot;Explore - &quot;) } else { # exploit idx &lt;- which(val == m) idx &lt;- idx[sample(length(idx), 1)] nextS &lt;- names(val)[idx] # pick one self$hV[[state]] &lt;- self$hV[[state]] + self$control$alpha * (m - self$hV[[state]]) # cat(&quot;Exploit - &quot;) } # cat(&quot;Next:&quot;, nextS, &quot;\\n&quot;) return(str_split(nextS, &quot;&quot;)[[1]]) } ) ) We then can define a player using: playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0.5, alpha = 0.1)) Other players may be defined similarly, e.g. a player which moves randomly (if can not win in the next move): PlayerRandom &lt;- R6Class(&quot;PlayerRandom&quot;, public = list( pfx = NA, initialize = function(pfx) { self$pfx &lt;- pfx }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) state &lt;- stateStr(sV) keys &lt;- c() keysV &lt;- NULL for (i in idx) { # find possible moves sV[i] &lt;- self$pfx str &lt;- str_c(sV, collapse = &quot;&quot;) keys &lt;- c(keys, str) keysV &lt;- rbind(keysV, sV) sV[i] &lt;- &quot;.&quot; } # check if can win in one move for (i in 1:nrow(keysV)) { if (win(self$pfx, keysV[i,]) == 1) { return(keysV[i,]) # next state is the win state } } # else pick one random return(keysV[sample(nrow(keysV), 1),]) } ) ) A player which always place at the lowest field index: PlayerFirst &lt;- R6Class(&quot;PlayerFirst&quot;, public = list( pfx = NA, initialize = function(pfx) { self$pfx &lt;- pfx }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) sV[idx[1]] &lt;- self$pfx return(sV) } ) ) 1.10.2 Gameplay We define a game which returns the prefix of the winner (NA if a draw): #&#39; @param player1 A player R6 object. This player starts the game #&#39; @param player2 A player R6 object. #&#39; @param verbose Print gameplay. #&#39; @return The winners prefix or NA if a tie. playGame &lt;- function(player1, player2, verbose = FALSE) { sP2 &lt;- rep(&quot;.&quot;, 9) # start state / game state sP1 &lt;- sP2 # state from player 1s viewpoint for (i in 1:5) { # at most 4.5 rounds ## player 1 if (verbose) cat(&quot;Player &quot;, player1$pfx, &quot;:\\n&quot;, sep=&quot;&quot;) sP1 &lt;- player1$move(sP1, sP2) # new state from player 1s viewpoint # states &lt;- c(states, stateChr(sV)) # cat(stateStr(sV), &quot; | &quot;, sep = &quot;&quot;) if (verbose) plot_board_state_cat(stateStr(sP1)) if (win(player1$pfx, sP1) == 1) { return(player1$pfx) break } if (i == 5) break # a draw ## player 2 if (verbose) cat(&quot;Player &quot;, player2$pfx, &quot;:\\n&quot;, sep=&quot;&quot;) sP2 &lt;- player2$move(sP2, sP1) # states &lt;- c(states, stateChr(sV)) # cat(stateStr(sV), &quot; | &quot;, sep = &quot;&quot;) if (verbose) plot_board_state_cat(stateStr(sP2)) if (win(player2$pfx, sP2) == 1) { return(player2$pfx) break } } return(NA) } Let us play a game between playerA and playerR: playerR &lt;- PlayerRandom$new(pfx = &quot;R&quot;) playGame(playerA, playerR, verbose = T) #&gt; Player A: #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; | A | . | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player R: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | . | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player A: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player R: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | . | #&gt; |------------------| #&gt; | . | R | . | #&gt; |------------------| #&gt; Player A: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | A | #&gt; |------------------| #&gt; | . | R | . | #&gt; |------------------| #&gt; [1] &quot;A&quot; Note playerA has been learning when playing the game. The current estimates that are stored in the hash list are: playerA$hV #&gt; &lt;hash&gt; containing 22 key-value pair(s). #&gt; ......... : 0.5 #&gt; ........A : 0.5 #&gt; .......A. : 0.5 #&gt; ......A.. : 0.5 #&gt; .....A... : 0.5 #&gt; ....A.... : 0.5 #&gt; ...A..... : 0.5 #&gt; ..A...... : 0.5 #&gt; .A....... : 0.5 #&gt; A........ : 0.5 #&gt; R..A....A : 0.5 #&gt; R..A...A. : 0.5 #&gt; R..A..A.. : 0.5 #&gt; R..A.A... : 0.5 #&gt; R..AA.... : 0.55 #&gt; R..AA..RA : 0.5 #&gt; R..AA.AR. : 0.5 #&gt; R..AAA.R. : 1 #&gt; R.AA..... : 0.5 #&gt; R.AAA..R. : 0.5 #&gt; RA.A..... : 0.5 #&gt; RA.AA..R. : 0.5 1.10.3 Learning by a sequence of games With a single game only a few states are explored and estimates are not good. Let us instead play a sequence of games and learn along the way: #&#39; @param playerA Player A (R6 object). #&#39; @param playerB Player B (R6 object). #&#39; @param games Number of games #&#39; @param prA Probability of `playerA` starts. #&#39; @return A list with results (a data frame and a plot). playGames &lt;- function(playerA, playerB, games, prA = 0.5) { winSeq &lt;- rep(NA, games) for (g in 1:games) { # find start player if (sample(0:1, 1, prob = c(prA, 1-prA)) == 0) { player1 &lt;- playerA player2 &lt;- playerB } else { player2 &lt;- playerA player1 &lt;- playerB } winSeq[g] &lt;- playGame(player1, player2) } # process the data dat &lt;- tibble(game = 1:length(winSeq), winner = winSeq) %&gt;% mutate( players = str_c(playerA$pfx, playerB$pfx), winA := case_when( winner == playerA$pfx ~ 1, winner == playerB$pfx ~ 0, TRUE ~ 0.5 ), winsA_r = rollapply(winA, ceiling(games/10), mean, align = &quot;right&quot;, fill = NA) #, fill = 0, partial = T ) # make a plot pt &lt;- dat %&gt;% ggplot(aes(x = game, y = winA)) + geom_line(aes(y = winsA_r), size = 0.2) + geom_smooth(se = F) + labs(y = str_c(&quot;Avg. wins player &quot;, playerA$pfx), title = str_c(&quot;Wins &quot;, playerA$pfx, &quot; = &quot;, round(mean(dat$winA), 2), &quot; &quot;, playerB$pfx, &quot; = &quot;, round(1-mean(dat$winA), 2))) return(list(dat = dat, plot = pt)) } Let us now play games against a player who moves randomly using \\(\\epsilon = 0.1\\) (explore probability) and \\(\\alpha = 0.1\\) (step size). playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0.1, alpha = 0.1)) playerR &lt;- PlayerRandom$new(pfx = &quot;R&quot;) res &lt;- playGames(playerA, playerR, games = 2000) res$plot The black curve is the moving average of winning with a trend line. Note the values of the parameters have an effect on our learning: In general we do not need to explore (\\(\\epsilon = 0\\)) (the other player explore enough for us) and a high explore probability (\\(\\epsilon = 0.9\\)) make us loose. Moreover, using a high step size seems to work best. Other players may give different results. If the RL player play against a player which always move to first free field index: Here a high step size and a low exploration probability is good and the RL player will soon figure out how to win all the time. This is different if the RL player A play against another clever (RL) player B. playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0, alpha = 0.1)) playerB &lt;- PlayerRL$new(pfx = &quot;B&quot;, control = list(epsilon = 0, alpha = 0.1)) If both players play using the same control parameters, one would expect that they after learning should win/loose with probability 0.5. However if there is no exploration (\\(\\epsilon = 0\\)) this is not always true: Depending on how the game starts a player may learn a better strategy and win/loose more. That is, exploration is important. Finally let us play against a player B with fixed control parameters. In general it is best to explore using the same probability otherwise you loose more and a higher step size than your opponent will make you win. 1.11 Summary Read Chapter 1.6 in Sutton and Barto (2018). 1.12 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 1.12.1 Exercise - Self-Play × Solution If the exploration parameter is non-zero, the algorithm will continue to adapt until it reaches an equilibrium (either fixed or cyclical). Close Solution Consider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing? 1.12.2 Exercise - Symmetries Many tic-tac-toe positions appear different but are really the same because of symmetries. × Solution It is possible to use 4 axis of symmetry to essentially fold the board down to a quarter of the size. Close Solution How might we amend the reinforcement learning algorithm described above to take advantage of this? × Solution A smaller state space would increase the speed of learning and reduce the memory required. Close Solution In what ways would this improve the algorithm? × Solution If the opponent did not use symmetries then it could result in a worse learning. For example, if the opponent always played correct except for 1 corner, then using symmetries would mean you never take advantage of that information. That is, we should not use symmetries too since symmetrically equivalent positions do not always hold the same value in such a game. Close Solution Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value? 1.12.3 Exercise - Greedy Play × Solution As seen in Section 1.10.3 using \\(\\epsilon = 0\\) may be okay for this game if the opponent use a simple strategy (e.g. random or first index). However, in general the RL player would play worse. The chance the optimal action is the one with the current best estimate of winning is low and depending on the gameplay the RL player might win or loose. The RL player would also be unable to adapt to an opponent that slowly alter behaviour over time. Close Solution Consider Tic-Tac-Toe and suppose the RL player is only greedy (\\(\\epsilon = 0\\)), that is, always playing the move that that gives the highest probability of winning. Would it learn to play better, or worse, than a non-greedy player? What problems might occur? 1.12.4 Exercise - Learning from Exploration Consider Tic-Tac-Toe and suppose the RL player is playing against an opponent with a fixed strategy. Suppose learning updates occur after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. × Solution The probability set \\(V(s)\\) found by applying no learning from exploration is the probability of winning when using the optimal policy. The probability set \\(V(s)\\) found by applying learning from exploration is the probability of winning including the active exploration policy. Close Solution What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? × Solution The probability set found by applying no learning from exploration would result in more wins. The probability set found by applying learning from exploration is better to learn, as it reduces variance from sub-optimal future states. Close Solution Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins? 1.12.5 Exercise - Other Improvements × Solution Altering the exploration rate/learning based on the variance in the opponent's actions. If the opponent is always making the same moves and you are winning from it then using a non-zero exploration rate will make you lose you games. If the agent is able to learn how the opponent may react to certain moves, it will be easier for it to win as it can influence the opponent to make moves that leads it to a better state. Close Solution Consider Tic-Tac-Toe. Can you think of other ways to improve the reinforcement learning player? References "],["mod-bandit.html", "Module 2 Multi-armed bandits 2.1 Learning outcomes 2.2 Textbook readings 2.3 The k-armed bandit problem 2.4 Estimating the value of an action 2.5 The role of the step-size 2.6 Optimistic initial values 2.7 Upper-Confidence Bound Action Selection 2.8 Summary 2.9 Exercises", " Module 2 Multi-armed bandits This module consider the k-armed bandit problem which is a sequential decision problem with one state and \\(k\\) actions. The problem is used to illustrate different learning methods used in RL. The module is also the first module in the Tabular methods part of the notes. This part describe almost all the core ideas of reinforcement learning algorithms in their simplest forms where the state and action spaces are small enough for the approximate value functions to be represented as arrays or tables. 2.1 Learning outcomes By the end of this module, you are expected to: Define a k-armed bandit and understand the nature of the problem. Define the reward of a action (action-reward). Describe different methods for estimating the action-reward. Explain the differences between exploration and exploitation. Formulate an \\(\\epsilon\\)-greedy algorithm for selecting the next action. Interpret the sample-average (variable step-size) versus exponential recency-weighted average (constant step-size) action-reward estimation. Argue why we might use a constant stepsize in the case of non-stationarity. Understand the effect of optimistic initial values. Formulate an upper confidence bound action selection algorithm. The learning outcomes relate to the overall learning goals number 1, 3, 6, 9, 10 and 12 of the course. 2.2 Textbook readings For this week, you will need to read Chapter 2 - 2.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 2.3 The k-armed bandit problem Multi-armed bandits attempt to find the best option among a collection of alternatives by learning through trial and error. The name derives from “one-armed bandit,” a slang term for a slot machine — which is a perfect analogy for how these algorithms work. Figure 2.1: A 4-armed bandit. Imagine you are facing a wall with \\(k\\) slot machines (see Figure 2.1), and each one pays out at a different rate. A natural way to figure out how to make the most money (rewards) would be to try each at random for a while (exploration), and start playing the higher paying ones once you have gained some experience (exploitation). That is, from an agent/environment point of view the agent considers a single state \\(s\\) at time \\(t\\) and have to choose among \\(k\\) actions given the environment representing the \\(k\\) bandits. Only the rewards from the \\(k\\) bandits are unknown, but the agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. The objective is to find an optimal policy that maximize the total expected reward. Note since the process only have a single state, this is the same as finding an optimal policy \\(\\pi^*(s) = \\pi^* = a^*\\) that chooses the action with the highest expected reward. Due to uncertainty, there is an exploration vs exploitation dilemma. The agent have one action that seems to be most valuable at a time point, but it is highly likely, at least initially, that there are actions yet to explore that are more valuable. Multi-armed bandits can be used in e.g. digital advertising. Suppose you are an advertiser seeking to optimize which ads (\\(k\\) to choose among) to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time. Figure 2.2: Which ad to choose? It is reasonable to assume that each of these ads will have different effects, and some will be more engaging than others. That is, each ad has some theoretical — but unknown — click-through-rate (CTR) that is assumed to not change over time. How do we go about solving which ad we should choose (see Figure 2.2)? 2.4 Estimating the value of an action How can the value of an action be estimated, i.e. the expected reward of an action \\(q_*(a) = \\mathbb{E}[R_t | A_t = a]\\). Assume that at time \\(t\\) action \\(a\\) has been chosen \\(N_t(a)\\) times. Then the estimated action value is \\[\\begin{equation} Q_t(a) = \\frac{R_1+R_2+\\cdots+R_{N_t(a)}}{N_t(a)}, \\end{equation}\\] Storing \\(Q_t(a)\\) this way is cumbersome since memory and computation requirements grow over time. Instead an incremental approach is better. If we assume that \\(N_t(a) = n-1\\) and set \\(Q_t(a) = Q_n\\) then \\(Q_{n+1}\\) becomes: \\[\\begin{align} Q_{n+1} &amp;= \\frac{1}{n}\\sum_{i=1}^{n}R_i \\nonumber \\\\ &amp;= \\frac{1}{n}\\left( R_{n} + \\sum_{i=1}^{n-1} R_i \\right) \\nonumber \\\\ &amp;= \\frac{1}{n}\\left( R_{n} + (n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1} R_i \\right) \\nonumber \\\\ &amp;= \\frac{1}{n}\\left( R_{n} + (n-1)Q_n \\right) \\nonumber \\\\ &amp;= Q_n + \\frac{1}{n} \\left[R_n - Q_n\\right]. \\tag{2.1} \\end{align}\\] That is, we can update the estimate of the value of \\(a\\) using the previous estimate, the observed reward and how many times the action has occurred (\\(n\\)). A greedy approach for selecting the next action is \\[\\begin{equation} A_t =\\arg \\max_a Q_t(a). \\end{equation}\\] Here \\(\\arg\\max_a\\) means the value of \\(a\\) for which \\(Q_t(a)\\) is maximised. A pure greedy approach do not explore other actions. Instead an \\(\\varepsilon\\)-greedy approach is used in which with probability \\(\\varepsilon\\) we take a random draw from all of the actions (choosing each action with equal probability) and hereby providing some exploration. Let us try to implement the algorithm using an agent and environment class. First we define the agent that do actions based on an \\(\\epsilon\\)-greedy strategy, stores the estimated \\(Q\\) values and the number of times an action has been chosen: #&#39; R6 Class representing the RL agent RLAgent &lt;- R6Class(&quot;RLAgent&quot;, public = list( #&#39; @field qV Q estimates. qV = NULL, #&#39; @field nV Action counter. nV = NULL, #&#39; @field k Number of bandits. k = NULL, #&#39; @field epsilon Epsilon used in epsilon greed action selection. epsilon = NULL, #&#39; @description Create an object (when call new). #&#39; @param k Number of bandits. #&#39; @param epsilon Epsilon used in epsilon greed action selection. #&#39; @param ini Initial qV values. #&#39; @return The new object. initialize = function(k = 10, epsilon = 0.01, ini = 0) { self$epsilon &lt;- epsilon self$qV &lt;- rep(ini, k) # k-length vector self$nV &lt;- rep(0, k) # k-length vector self$k &lt;- k }, #&#39; @description Clear learning. clearLearning = function() { self$qV &lt;- 0 self$nV &lt;- 0 }, #&#39; @description Select next action using an epsilon greedy strategy. #&#39; @return Action (index). selectActionEG = function() { if (runif(1) &lt;= self$epsilon) { # explore a &lt;- sample(1:self$k, 1) } else { # exploit a &lt;- which(self$qV == max(self$qV)) a &lt;- a[sample(length(a), 1)] # choose a action random if more than one } return(a) }, #&#39; @description Update learning values (including action counter). #&#39; @param a Action. #&#39; @param r Reward. #&#39; @return NULL (invisible) updateQ = function(a, r) { self$nV[a] &lt;- self$nV[a] + 1 self$qV[a] &lt;- self$qV[a] + 1/self$nV[a] * (r - self$qV[a]) return(invisible(NULL)) } ) ) Next, the environment generating rewards. The true mean reward \\(q_*(a)\\) of an action is selected according to a normal (Gaussian) distribution with mean 0 and variance 1. The observed reward is then generated using a normal distribution with mean \\(q_*(a)\\) and variance 1: #&#39; R6 Class representing the RL environment #&#39; #&#39; Assume that bandits are normal distributed with a mean and std.dev of one. RLEnvironment &lt;- R6Class(&quot;RLEnvironment&quot;, public = list( #&#39; @field mV Mean values mV = NULL, #&#39; @field k Number of bandits. k = NULL, #&#39; @description Create an object (when call new). #&#39; @param k Number of bandits. #&#39; @return The new object. initialize = function(k = 10) { self$mV &lt;- rnorm(k) # means are from a N(0,1) }, #&#39; @description Sample reward of a bandit. #&#39; @param a Bandit (index). #&#39; @return The reward. reward = function(a) { return(rnorm(1, self$mV[a])) # pick a random value from N(self$mV[a],1) }, #&#39; @description Returns action with best mean. optimalAction = function() return(which.max(self$mV)) ) ) To test the RL algorithm we use a function returning two plots that compare the performance: #&#39; Performance of the bandit algorithm using different epsilons. #&#39; #&#39; @param k Bandits. #&#39; @param steps Time steps. #&#39; @param runs Number of runs with a new environment generated. #&#39; @param epsilons Epsilons to be tested. #&#39; @param ini Initial value estimates. #&#39; @return Two plots in a list. performance &lt;- function(k = 10, steps = 1000, runs = 500, epsilons = c(0, 0.01, 0.1), ini = 0) { rew &lt;- matrix(0, nrow = steps, ncol = length(epsilons)) # rewards (one col for each eps) best &lt;- matrix(0, nrow = steps, ncol = length(epsilons)) # add 1 if find the best action for (run in 1:runs) { env &lt;- RLEnvironment$new(k) oA &lt;- env$optimalAction() # print(oA); print(env$mV) for (i in 1:length(epsilons)) { agent &lt;- RLAgent$new(k, epsilons[i], ini) for (t in 1:steps) { a &lt;- agent$selectActionEG() r &lt;- env$reward(a) agent$updateQ(a, r) rew[t, i] &lt;- rew[t, i] + r # sum of rewards generated at t best[t, i] &lt;- best[t, i] + (a == oA) # times find best actions } } } colnames(rew) &lt;- epsilons colnames(best) &lt;- epsilons dat1 &lt;- tibble(t = 1:steps) %&gt;% bind_cols(rew) %&gt;% # bind data together pivot_longer(!t, values_to = &quot;reward&quot;, names_to = &quot;epsilon&quot;) %&gt;% # move rewards to a single column group_by(epsilon) %&gt;% mutate(All = cumsum(reward/runs)/t, `Moving avg (50)` = rollapply(reward/runs, 50, mean, align = &quot;right&quot;, fill = NA)) %&gt;% select(-reward) %&gt;% pivot_longer(!c(t, epsilon)) dat2 &lt;- tibble(t = 1:steps) %&gt;% bind_cols(best) %&gt;% # bind data together pivot_longer(!t, values_to = &quot;optimal&quot;, names_to = &quot;epsilon&quot;) %&gt;% group_by(epsilon) %&gt;% mutate(All = cumsum(optimal/runs)/t, `Moving avg (50)` = rollapply(optimal/runs, 50, mean, align = &quot;right&quot;, fill = NA)) %&gt;% select(-optimal) %&gt;% pivot_longer(!c(t, epsilon)) # calc average pt1 &lt;- dat1 %&gt;% ggplot(aes(x = t, y = value, col = epsilon, linetype = name)) + geom_line() + labs(y = &quot;Average reward per time unit&quot;, x = &quot;Time&quot;, title = str_c(&quot;Average over &quot;, runs, &quot; runs &quot;), col = &quot;Epsilon&quot;, linetype = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;) pt2 &lt;- dat2 %&gt;% ggplot(aes(x = t, y = value, col = epsilon, linetype = name)) + geom_line() + labs(y = &quot;Average number of times optimal action chosen&quot;, x = &quot;Time&quot;, title = str_c(&quot;Average over &quot;, runs, &quot; runs&quot;), col = &quot;Epsilon&quot;, linetype = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;) return(list(ptR = pt1, ptO = pt2)) } We test the performance using 2000 runs over 1000 time steps. pts &lt;- performance(runs = 2000, steps = 1000) pts$ptR pts$ptO The solid line shows averages over all the runs from \\(t=1\\) to the considered time-step while the dotted line is a moving average over the last 50 time-steps. Since we are expected to learn over the time-steps the moving averages will in general be higher than the overall averages. Note that if we have 1000 time-steps a greedy approach in general is bad and an \\(\\epsilon\\)-greedy approach is better (\\(\\epsilon = 0.1\\) is best). That is, exploration is beneficial. 2.5 The role of the step-size In general we update the reward estimate of an action using \\[\\begin{equation} Q_{n+1} = Q_n +\\alpha_n(a) \\left[R_n - Q_n\\right] \\end{equation}\\] Until now we have used the sample average \\(\\alpha_n(a)= 1/n\\); however, other choices of \\(\\alpha_n(a)\\) is possible. In general we will converge to the true reward if \\[\\begin{equation} \\sum_n \\alpha_n(a) = \\infty \\quad\\quad \\mathsf{and} \\quad\\quad \\sum_n \\alpha_n(a)^2 &lt; \\infty. \\end{equation}\\] Meaning that the coefficients must be large enough to recover from initial fluctuations, but not so large that they do not converge in the long run. However, if the process is non-stationary, i.e. the expected reward of an action change over time, then convergence is undesirable and we may want to use a constant \\(\\alpha_n(a)= \\alpha \\in (0, 1]\\) instead. This results in \\(Q_{n+1}\\) being a weighted average of the past rewards and the initial estimate \\(Q_1\\): \\[\\begin{align} Q_{n+1} &amp;= Q_n +\\alpha \\left[R_n - Q_n\\right] \\nonumber \\\\ &amp;= \\alpha R_n + (1 - \\alpha)Q_n \\nonumber \\\\ &amp;= \\alpha R_n + (1 - \\alpha)[\\alpha R_{n-1} + (1 - \\alpha)Q_{n-1}] \\nonumber \\\\ &amp;= \\alpha R_n + (1 - \\alpha)\\alpha R_{n-1} + (1 - \\alpha)^2 Q_{n-1} \\nonumber \\\\ &amp;= \\vdots \\nonumber \\\\ &amp;= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^{n-i} R_i \\\\ \\end{align}\\] Because the weight given to each reward depends on how long ago it was observed, we can see that more recent rewards are given more weight. Note the weights \\(\\alpha\\) sum to 1 here, ensuring it is indeed a weighted average where more weight is allocated to recent rewards. Since the weight given to each reward decays exponentially into the past. This sometimes called an exponential recency-weighted average. 2.6 Optimistic initial values The methods discussed so far are dependent to some extent on the initial action-value estimate i.e. they are biased by their initial estimates. For methods with constant \\(\\alpha\\) this bias is permanent. We may set initial value estimates artificially high to encourage exploration in the short run. For instance, by setting initial values of \\(Q\\) to 5 rather than 0 we encourage exploration, even in the greedy case. Here the agent will almost always be disappointed with it’s samples because they are less than the initial estimate and so will explore elsewhere until the values converge. 2.7 Upper-Confidence Bound Action Selection An \\(\\epsilon\\)-greed algorithm choose the action to explore with equal probability in an exploration step. It would be better to select among non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. One way to do this is to select actions using the upper-confidence bound: \\[\\begin{equation} A_t = \\arg\\max_a \\left(Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\right), \\end{equation}\\] Note the square root term is a measure of the uncertainty in our estimate (see Figure 2.3). It is proportional to \\(t\\) i.e. how many time-steps have passed and inversely proportional to \\(N_t(a)\\) i.e. how many times that action has been visited. The more time has passed, and the less we have sampled an action, the higher our upper-confidence-bound. As the timesteps increases, the denominator dominates the numerator as the ln term flattens. Each time we select an action our uncertainty decreases because \\(N\\) is the denominator of this equation. If \\(N_t(a) = 0\\) then we consider \\(a\\) as a maximal action, i.e. we select first among actions with \\(N_t(a) = 0\\). The parameter \\(c&gt;0\\) controls the degree of exploration. Higher \\(c\\) results in more weight on the uncertainty. Since upper-confidence bound action selection select actions according to their potential, it is expected to perform better than \\(\\epsilon\\)-greedy methods. Figure 2.3: Square root term for an action using different \\(c\\)-values. 2.8 Summary Read Chapter 2.10 in Sutton and Barto (2018). 2.9 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 2.9.1 Exercise - Advertising Suppose you are an advertiser seeking to optimize which ads to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time. Assume that: You have \\(k=5\\) adds to choose among. If add \\(A\\) is chosen then the user clicks the add with probability \\(p_A\\) which can be seen as the unknown click trough rate CTR (or an average reward). The CTRs are unknown and samples can be picked using the RLAdEnv class and the reward function which returns 1 if click on ad and 0 otherwise. #&#39; R6 Class representing the RL advertising environment RLAdEnv &lt;- R6Class(&quot;RLAdEnv&quot;, public = list( #&#39; @field mV Click trough rates (unknown to you) mV = c(0.1, 0.83, 0.85, 0.5, 0.7), #&#39; @field k Number of ads. k = 5, #&#39; @description Sample reward of a bandit. #&#39; @param a Bandit/ad (index). #&#39; @return One if click on ad and zero otherwise. reward = function(a) { return(rbinom(1, 1, self$mV[a])) }, #&#39; @description Returns action with best mean. optimalAction = function() return(which.max(self$mV)) ) ) env &lt;- RLAdEnv$new() env$reward(2) # click on ad number two (return 0 or 1)? #&gt; [1] 1 env$optimalAction() # the best ad #&gt; [1] 3 env$mV # true CTRs #&gt; [1] 0.10 0.83 0.85 0.50 0.70 In the class the true CTRs can be observed but in practice this is hidden from the agent (you). Consider an \\(\\epsilon\\)-greedy algorithm to find the best ad. Assume the webpage is visited by 10000 users per day. × Solution set.seed(327) # to get same results #&#39; Performance of the bandit algorithm. #&#39; @param steps Steps (users). #&#39; @param epsilon Epsilon to be tested. #&#39; @return A list with statistics. testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(k = 5, epsilon = epsilon) rew &lt;- 0 for (t in 1:steps) { a &lt;- agent$selectActionEG() r &lt;- env$reward(a) rew &lt;- rew + r agent$updateQ(a, r) } return(list(qV = agent$qV, avgReward = rew/steps)) } testEG(0.01) #&gt; $qV #&gt; [1] 0.000 0.800 0.852 0.409 0.654 #&gt; #&gt; $avgReward #&gt; [1] 0.849 testEG(0.1) #&gt; $qV #&gt; [1] 0.119 0.843 0.849 0.516 0.685 #&gt; #&gt; $avgReward #&gt; [1] 0.824 testEG(0.5) #&gt; $qV #&gt; [1] 0.110 0.844 0.851 0.484 0.703 #&gt; #&gt; $avgReward #&gt; [1] 0.725 # True values env$optimalAction() #&gt; [1] 3 env$mV #&gt; [1] 0.10 0.83 0.85 0.50 0.70 Epsilon = 0.01 seems to give the best average number of clicks. Close Solution × Hint set.seed(327) # to get same results #&#39; Performance of the bandit algorithm. #&#39; @param steps Steps (users). #&#39; @param epsilon Epsilon to be tested. #&#39; @return A list with statistics. testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(___) rew &lt;- 0 for (t in 1:steps) { a &lt;- agent$___ r &lt;- env$___ rew &lt;- rew + r agent$updateQ(___) } return(list(qV = ___, avgReward = ___)) } testEG(0.01) testEG(0.1) testEG(0.5) # True values env$optimalAction() env$mV Close Hint Run the \\(\\epsilon\\)-greedy algorithm with \\(\\epsilon = 0.01, 0.1, 0.5\\) over the 10000 steps. What are the estimated CTRs for each action (\\(Q_t(a)\\))? What is the average number of clicks per user? × Solution ## Test function modified with plot feature testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(k = 5, epsilon = epsilon) rew &lt;- 0 qVal &lt;- matrix(0, nrow = steps, ncol = 5) # store qV in a row for each t colnames(qVal) = str_c(&quot;A&quot;, 1:5) for (t in 1:steps) { a &lt;- agent$selectActionEG() r &lt;- env$reward(a) rew &lt;- rew + r agent$updateQ(a, r) qVal[t,] &lt;- agent$qV } # make plot dat &lt;- tibble(t = 1:steps) %&gt;% bind_cols(qVal) %&gt;% # bind data together pivot_longer(!t, values_to = &quot;ctr&quot;, names_to = &quot;action&quot;) pt &lt;- dat %&gt;% ggplot(aes(x = t, y = ctr, col = action)) + geom_line() + labs(y = &quot;Empirical CTRs&quot;, x = &quot;Time&quot;, title = str_c(&quot;CTRs eps = &quot;, epsilon), col = &quot;Action&quot;) + theme(legend.position = &quot;bottom&quot;) return(list(qV = agent$qV, avgReward = rew/steps, plt = pt)) } testEG(0.01)$plt testEG(0.5)$plt As epsilon grows we estimate the true values better for all actions. Close Solution × Hint ## Test function modified with plot feature testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(k = 5, epsilon = epsilon) rew &lt;- 0 qVal &lt;- matrix(0, nrow = steps, ncol = 5) # store qV in a row for each t colnames(qVal) = str_c(&quot;A&quot;, 1:5) for (t in 1:steps) { ___ } # make plot dat &lt;- tibble(t = 1:steps) %&gt;% bind_cols(qVal) %&gt;% # bind data together pivot_longer(!t, values_to = &quot;ctr&quot;, names_to = &quot;action&quot;) pt &lt;- dat %&gt;% ggplot(aes(x = t, y = ctr, col = action)) + geom_line() + labs(y = &quot;Empirical CTRs&quot;, x = &quot;Time&quot;, title = str_c(&quot;CTRs eps = &quot;, epsilon), col = &quot;Action&quot;) + theme(legend.position = &quot;bottom&quot;) return(list(qV = agent$qV, avgReward = rew/steps, plt = pt)) } testEG(0.01)$plt testEG(0.5)$plt Close Hint Make a plot of the empirical CTRs for \\(\\epsilon = 0.01, 0.5\\) over 10000 time-steps, i.e. plot \\(Q_t(a)\\). × Solution ## Test function modified with rewards testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(k = 5, epsilon = epsilon) rewards &lt;- c(10, 8, 5, 15, 2) rew &lt;- 0 qVal &lt;- matrix(0, nrow = 10000, ncol = 5) colnames(qVal) = str_c(&quot;A&quot;, 1:5) for (t in 1:steps) { a &lt;- agent$selectActionEG() r &lt;- env$reward(a) * rewards[a] rew &lt;- rew + r agent$updateQ(a, r) qVal[t,] &lt;- agent$qV } return(list(qV = agent$qV, avgReward = rew/steps)) } testEG(0.01) #&gt; $qV #&gt; [1] 0.909 6.591 4.583 7.610 1.111 #&gt; #&gt; $avgReward #&gt; [1] 7.15 testEG(0.5) #&gt; $qV #&gt; [1] 0.951 6.646 4.286 7.459 1.361 #&gt; #&gt; $avgReward #&gt; [1] 5.74 # True average reward values env$mV * c(10, 8, 5, 15, 2) #&gt; [1] 1.00 6.64 4.25 7.50 1.40 The best action is now 4 and eps = 0.01 seems to give the best overall average reward. Close Solution × Hint ## Test function modified with rewards testEG &lt;- function(epsilon, steps = 10000) { agent &lt;- RLAgent$new(k = 5, epsilon = epsilon) rewards &lt;- ___ rew &lt;- 0 qVal &lt;- matrix(0, nrow = 10000, ncol = 5) colnames(qVal) = str_c(&quot;A&quot;, 1:5) for (t in 1:steps) { a &lt;- agent$selectActionEG() r &lt;- ___ rew &lt;- rew + r agent$updateQ(a, r) qVal[t,] &lt;- agent$qV } return(list(qV = agent$qV, avgReward = rew/steps)) } testEG(0.01) testEG(0.5) # True average reward values env$mV * c(10, 8, 5, 15, 2) Close Hint Assume that the rewards of ad clicks is equal to (10, 8, 5, 15, 2). Modify the algorithm so you look at rewards instead of CTRs. What is the best action to choose? We now modify the RLAgent and add an upper-confidence bound function selectActionUCB: #&#39; R6 Class representing the RL agent RLAgent &lt;- R6Class(&quot;RLAgent&quot;, public = list( #&#39; @field qV Q estimates. qV = NULL, #&#39; @field nV Action counter. nV = NULL, #&#39; @field k Number of bandits. k = NULL, #&#39; @field epsilon Epsilon used in epsilon greed action selection. epsilon = NULL, #&#39; @description Create an object (when call new). #&#39; @param k Number of bandits. #&#39; @param epsilon Epsilon used in epsilon greed action selection. #&#39; @return The new object. initialize = function(k = 10, epsilon = 0.01, ini = 0) { self$epsilon &lt;- epsilon self$qV &lt;- rep(ini, k) self$nV &lt;- rep(0, k) self$k &lt;- k }, #&#39; @description Clear learning. #&#39; @param eps Epsilon. #&#39; @return Action (index). clearLearning = function() { self$qV &lt;- 0 self$nV &lt;- 0 }, #&#39; @description Select next action using an epsilon greedy strategy. #&#39; @return Action (index). selectActionEG = function() { if (runif(1) &lt;= self$epsilon) { # explore a &lt;- sample(1:self$k, 1) } else { # exploit a &lt;- which(self$qV == max(self$qV)) a &lt;- a[sample(length(a), 1)] } return(a) }, #&#39; @description Select next action using UCB #&#39; @return Action (index). selectActionUCB = function(c, t) { val &lt;- self$qV + c * sqrt(log(t + 0.01)/self$nV) a &lt;- which.max(val) return(a) }, #&#39; @description Update learning values (including action counter). #&#39; @param a Action. #&#39; @param r Reward. #&#39; @return NULL (invisible) updateQ = function(a, r) { self$nV[a] &lt;- self$nV[a] + 1 self$qV[a] &lt;- self$qV[a] + 1/self$nV[a] * (r - self$qV[a]) return(invisible(NULL)) } ) ) × Solution testUCB &lt;- function(c = 2, steps = 10000) { agent &lt;- RLAgent$new(k = 5) rewards &lt;- c(10, 8, 5, 15, 2) rew &lt;- 0 for (t in 1:steps) { a &lt;- agent$selectActionUCB(c, t) r &lt;- env$reward(a) * rewards[a] rew &lt;- rew + r agent$updateQ(a, r) } return(list(qV = agent$qV, avgReward = rew/steps)) } testUCB(0.1) #&gt; $qV #&gt; [1] 0.00 6.67 5.00 7.48 0.00 #&gt; #&gt; $avgReward #&gt; [1] 7.48 testUCB(5) #&gt; $qV #&gt; [1] 1.43 6.08 4.64 7.51 1.33 #&gt; #&gt; $avgReward #&gt; [1] 7.48 testUCB(10) #&gt; $qV #&gt; [1] 0.556 6.429 4.560 7.489 1.048 #&gt; #&gt; $avgReward #&gt; [1] 7.38 testUCB(20) #&gt; $qV #&gt; [1] 0.758 6.668 4.061 7.570 1.512 #&gt; #&gt; $avgReward #&gt; [1] 7.26 A value \\(c = 10\\) seems to be a good choice. Close Solution × Hint testUCB &lt;- function(c = 2, steps = 10000) { agent &lt;- RLAgent$new(k = 5) rewards &lt;- c(10, 8, 5, 15, 2) rew &lt;- 0 for (t in 1:steps) { ___ } return(list(qV = agent$qV, avgReward = rew/steps)) } testUCB(0.1) testUCB(5) testUCB(10) testUCB(20) Close Hint Test the UCB algorithm for \\(c\\) values \\((0.1, 5, 10, 20)\\). Which algorithm seems to find the best average reward? 2.9.2 Exercise - A coin game Consider a game where you choose to flip one of two (possibly unfair) coins. You win 1 if your chosen coin shows heads and lose 1 if it shows tails. × Solution This is a 2-bandit problem with actions of choosing coin 1 or 2. Close Solution Model this as a K-armed bandit problem: define the action set. × Solution The reward is stochastic. If consider coin \\(i\\) then \\(\\mathbb{E}[R_t | a_i] = \\Pr(H)\\cdot 1.\\) Close Solution Is the reward a deterministic or stochastic function of your action? × Solution The estimates are \\[Q_t(a_1) = (-1+1+1-1-1-1)/6 = -1/3\\] and \\[Q_t(a_2) = (1-1+1+1+1-1)/6 = 1/3\\]. Close Solution You do not know the coin flip probabilities. Instead, you are able to view 6 sample flips for each coin respectively: (T,H,H,T,T,T) and (H,T,H,H,H,T). Use the sample average formula (2.1) to compute the estimates of the value of each action. × Solution Coin 2 is chosen since the best action-value. Close Solution Decide on which coin to flip next assuming that you exploit. References "],["mod-mdp-1.html", "Module 3 Markov decision processes (MDPs) 3.1 Learning outcomes 3.2 Textbook readings 3.3 An MDP as a model for the agent-environment 3.4 Rewards and the objective function (goal) 3.5 Summary 3.6 Exercises", " Module 3 Markov decision processes (MDPs) This module gives an introduction to Markov decision processes (MDPs) with a finite number of states and actions. This gives us a full model of a sequential decision problem. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also what will be the next state, and hence future rewards. Thus MDPs involve delayed reward and the need to consider the trade-off between immediate and delayed reward. MDPs are a mathematically idealized form of the RL problem where a full description is known and the optimal policy can be found. Often in a RL problem some parts of this description is unknown and we hereby have to estimate the best policy by learning. For example, in the bandit problem the rewards was unknown. 3.1 Learning outcomes By the end of this module, you are expected to: Identify the different elements of a Markov Decision Processes (MDP). Describe how the dynamics of an MDP are defined. Understand how the agent-environment RL description relates to an MDP. Interpret the graphical representation of a Markov Decision Process. Describe how rewards are used to define the objective function (expected return). Interpret the discount factor and its effect on the objective function. Identify episodes and how to formulate an MDP by adding an absorbing state. The learning outcomes relate to the overall learning goals number 2, 7, 10, and 12 of the course. 3.2 Textbook readings For this week, you will need to read Chapter 3-3.4 in Sutton and Barto (2018). Read it before continuing this module. You do not have to put to much focus on the examples. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 3.3 An MDP as a model for the agent-environment Let us recall the RL problem which considers an agent in an environment: Agent: The one who takes the action (computer, robot, decision maker), i.e. the decision making component of a system. Everything else is the environment. A general rule is that anything that the agent does not have absolute control over forms part of the environment. Environment: The system/world where observations and rewards are found. At time step \\(t\\) the agent is in state \\(S_t\\) and takes action \\(A_{t}\\) and observe the new state \\(S_{t+1}\\) and reward \\(R_{t+1}\\): Figure 1.3: Agent-environment representation. Note we here assume that the Markov property is satisfied and the current state holds just as much information as the history of observations. That is, given the present state the future is independent of the past: \\[\\Pr(S_{t+1} | S_t, A_t) = \\Pr(S_{t+1} | S_1,...,S_t, A_t).\\] That is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states. A Markov decision process (MDP) is a mathematical model that for each time-step \\(t\\) have defined states \\(S_t \\in \\mathcal{S}\\), possible actions \\(A_t \\in \\mathcal{A}(s)\\) given a state and rewards \\(R_t \\in \\mathcal{R} \\subset \\mathbb{R}\\). Consider the example in Figure 3.1. Each time-step have five states \\(\\mathcal{S} = \\{1,2,3,4,5\\}\\). Assume that the agent start in state \\(s_0\\) with two actions to choose among \\(\\mathcal{A}(s_0) = \\{a_1, a_2\\}\\). After choosing \\(a_1\\) a transition to \\(s_1\\) happens with reward \\(R_1 = r_1\\). Next, in state \\(s_1\\) the agent chooses action \\(a_2\\) and a transition to \\(s_2\\) happens with reward \\(r_2\\). This continues as time evolves. Figure 3.1: State-expanded hypergraph In a finite MDP, the sets of states, actions, and rewards all have a finite number of elements. In this case, the random variables have well defined discrete probability distributions dependent only on the preceding state and action which defines the dynamics of the system: \\[\\begin{equation} p(s&#39;, r | s, a) = \\Pr(S_t = s&#39;, R_t = r | S_{t-1} = s, A_{t-1} = a), \\end{equation}\\] which can be used to find the transition probabilities: \\[\\begin{equation} p(s&#39; | s, a) = \\Pr(S_t = s&#39;| S_{t-1} = s, A_{t-1}=A) = \\sum_{r \\in \\mathcal{R}} p(s&#39;, r | s, a), \\end{equation}\\] and the expected reward: \\[\\begin{equation} r(s, a) = \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s&#39; \\in \\mathcal{S}} p(s&#39;, r | s, a). \\end{equation}\\] That is, to define an MDP the following are needed: A finite number of states and actions. That is, we can store values using tabular methods. All states \\(S \\in \\mathcal{S}\\) and actions \\(A \\in \\mathcal{A}(s)\\) are known. The transition probabilities \\(p(s&#39; | s, a)\\) and expected rewards \\(r(s, a)\\) are given. Alternatively, \\(p(s&#39;, r | s, a)\\). Moreover, for now a stationary MDP is considered, i.e. at each time-step all states, actions and probabilities are the same and hence the time index can be dropped. 3.4 Rewards and the objective function (goal) The reward hypothesis is a central assumption in reinforcement learning: All of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal (called reward). This assumption can be questioned but in this course we assume it holds. The reward signal is our way of communicating to the agent what we want to achieve not how we want to achieve it. The return \\(G_t\\) can be defined as the sum of future rewards; however, if the time horizon is infinite the return is also infinite. Hence we use a discount factor \\(0 \\leq \\gamma \\leq 1\\) and define the return as \\[\\begin{equation} G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\end{equation}\\] Discounting is important since it allows us to work with finite returns because if \\(\\gamma &lt; 1\\) and the reward is bounded by a number \\(B\\) then the return is always finite: \\[\\begin{equation} G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\leq B \\sum_{k=0}^{\\infty} \\gamma^k = B \\frac{1}{1 - \\gamma} \\end{equation}\\] Note gamma close to one put weight on future rewards while a gamma close to zero put weight on present rewards. Moreover, an infinite time-horizon is assumed. An MDP modelling a problem over a finite time-horizon can be transformed into an infinite time-horizon using an absorbing state with transitions only to itself and a reward of zero. This breaks the agent-environment interaction into episodes (e.g playing a board game). Each episode ends in the absorbing state, possibly with a different reward. Each starts independently of the last, with some distribution of starting states. Sequences of interaction without an absorbing state are called continuing tasks. The objective function is to choose actions such that the expected return is maximized. We will formalize this mathematically in the next module. 3.5 Summary MDPs formalize the problem of an agent interacting with an environment. The agent and environment interact at discrete time steps. At each time, the agent observes the current state of the environment. Then selects an action and the the environment transitions to a new state with a reward. An agent’s choices have long-term consequences (delayed reward). Selected actions influences future states and rewards. The objective is to maximize the expected discounted return. With a discount factor less than one, we can guarantee the return remains finite. The value of the discount factor defines how much we care about short-term rewards versus long-term rewards. A first step in applying reinforcement learning is to formulate the problem as an MDP. 3.6 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 3.6.1 Exercise - Sequential decision problems × Solution Examples could be: Ludo - State: position on the board. Actions: Possible movements. Rewards: In a win state e.g. 1, in a loose state -1 and 0 otherwise. Inventory management - State: inventory level. Actions: Order \\(x\\) units, wait. Rewards: a negative number representing inventory holding cost plus ordering cost. Investment - State: current portfolio, KPI's from considered companies. Actions: Buy/sell \\(x\\) stocks of company \\(y.\\) Rewards: returns - costs. Close Solution Think of two sequential decision problems and try to formulate them as MDPs. Describe the states, actions and rewards in words. × Solution For the k-bandit problem we only have a single state representing before we chose an action. We have \\(k\\) actions and the rewards are the probability distribution from each slot machine. Note the k-bandit problem is trivial if we know the MDP, since then we know the expected reward of each action and hence the action with best expected reward will be optimal. Close Solution How do the states, actions and rewards look like for the bandit problem? Try drawing the state-expanded hypergraph. 3.6.2 Exercise - Expected return × Solution gam &lt;- 0.8 g &lt;-0 r &lt;- c(-3, 5, 2, 7, 1) for (i in 4:0) { g &lt;- r[i+1] + gam*g } g #&gt; [1] 6.27 Close Solution Suppose \\(\\gamma=0.8\\) and we observe the following sequence of rewards: \\(R_1 = -3\\), \\(R_2 = 5\\), \\(R_3=2\\), \\(R_4 = 7\\), and \\(R_5 = 1\\) with a finite time-horizon of \\(T=5\\). What is \\(G_0\\)? Hint: work backwards and recall that \\(G_t = R_{t+1} + \\gamma G_{t+1}\\). × Solution # recall sum_k g^k = 1/(1-g) g1 &lt;- 7 * 1/(1-0.9) g1 #&gt; [1] 70 g0 &lt;- 2 + 0.9 * 70 g0 #&gt; [1] 65 Close Solution Suppose \\(\\gamma=0.9\\) and we observe rewards: \\(R_1 = 2\\), \\(R_t = 7\\), \\(t&gt;1\\) given a infinite time-horizon. What is \\(G_0\\) and \\(G_1\\)? 3.6.3 Exercise - Gambler’s problem A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. The coin may be an unequal coin where there is not equal probability \\(p_H\\) for a head (H) and a tail (T). If the coin comes up heads, the gambler wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler reaches his goal of a capital equal 100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP, where we assume that the gambler starts with a capital \\(0 &lt; s_0 &lt; 100\\). × Solution Capital of the gambler: \\[\\mathcal{S} = \\{0, \\ldots, 100 \\}.\\] Terminal states are 0 and 100 (loose or win). Close Solution Define the state space \\(\\mathcal{S}\\). Which states are terminal states? × Solution Given his capital choose to gamble \\(a\\): \\[\\mathcal{A}(s) = \\{ a\\in \\mathcal{S} | 0 \\leq a \\leq \\min(s, 100-s) \\}.\\] Close Solution Define the action space \\(\\mathcal{A}(s)\\). × Solution The expected reward is: \\[r(s,a) = \\mathbb{E}[R_a] = p_H a\\] where \\(p_H\\) denote the probability of head. The state-value denote the expected reward. Close Solution Let \\(R_a\\) denote the reward given bet \\(a\\). Calculate the expected rewards. If the state-value for the terminal states is set to zero, what do the state-value of a policy mean? × Solution The expected reward is: \\[r(s,a) = \\mathbb{E}[R_a] = p_H a\\] where \\(p_H\\) denote the probability of head. The state-value is the probability of winning. Close Solution Let \\(R_a\\) be zero for all bets \\(a\\) and set the state-value for the terminal state 0 to zero and for state 100 to one. What do the state-value of a policy mean? × Solution If \\(C\\) denote a Bernoulli variable equal 1 if head. Then \\[p(s&#39; | s, a) = \\Pr(s&#39; = max(0, min(100, s + Ca - (1-C)a))).\\] Hence there are two transitions: if \\(s&#39; = max(0, s - a)\\) then \\(p(s&#39; | s, a) = 1-p_H\\) and if \\(s&#39; = min(100, s + a)\\) then \\(p(s&#39; | s, a) = p_H\\). Close Solution Calculate the transition probabilities. 3.6.4 Exercise - Factory storage A factory has a storage tank with a capacity of 4 \\(\\mathrm{m}^{3}\\) for temporarily storing waste produced by the factory. Each week the factory produces \\(0,1\\), 2 or 3 \\(\\mathrm{m}^{3}\\) waste with respective probabilities \\[p_{0}=\\displaystyle \\frac{1}{8},\\ p_{1}=\\displaystyle \\frac{1}{2},\\ p_{2}=\\displaystyle \\frac{1}{4} \\text{ and } p_{3}=\\displaystyle \\frac{1}{8}.\\] If the amount of waste produced in one week exceeds the remaining capacity of the tank, the excess is specially removed at a cost of $30 per cubic metre. At the end of each week there is a regular opportunity to remove all waste from the storage tank at a fixed cost of $25 and a variable cost of $5 per cubic metre. The problem can be modelled as a finite MDP where a state denote the amount of waste in the tank at the end of week \\(n\\) just before the regular removal opportunity. × Solution \\[\\mathcal{S} = \\{ 0,1,2,3,4 \\}\\] Close Solution Define the state space \\(\\mathcal{S}\\). × Solution Let \\(e\\) and \\(k\\) denote empty and keep the waste from the tank. Then the action space is \\[\\mathcal{A}(s) = \\{ e, k \\}.\\] Close Solution Define the action space \\(\\mathcal{A}(s)\\). × Solution The expected cost of a given state and action is the cost of empting the container and the expected cost of a special removal during the next week. Hence \\[r(s, e) = -(25 + 5s)\\]and\\[r(s,k) = -30\\sum_{i&gt;4-s} (s+i-4)p_i\\] Close Solution Calculate the expected rewards \\(r(s,a)\\). × Solution The transition probabilities are: \\[p(s&#39;|s,k) = p_{s&#39;-s}\\text{ if } s\\leq s&#39; \\leq 3\\] \\[p(4|s,k) = \\sum_{i\\geq 4-s} p_i\\] \\[p(s&#39;|s,e) = p_{s&#39;}\\text{ if } 0\\leq s&#39; \\leq 4\\] \\[p(s&#39;|s,k) = 0 \\text{ otherwise.}\\] Close Solution Calculate the transition probabilities \\(p(s&#39;|s,a)\\). References "],["mod-mdp-2.html", "Module 4 Policies and value functions for MDPs 4.1 Learning outcomes 4.2 Textbook readings 4.3 Policies and value functions 4.4 Optimal policies and value functions 4.5 Optimality vs approximation 4.6 Semi-MDPs (non-fixed time length) 4.7 Summary 4.8 Exercises", " Module 4 Policies and value functions for MDPs This module go deeper in the theory of finite Markov decision processes (MDPs). The concept of a policy and value functions is considered. Once the problem is formulated as an MDP, finding the optimal policy can be found using value functions. 4.1 Learning outcomes By the end of this module, you are expected to: Identify a policy as a distribution over actions for each possible state. Define value functions for a state and action. Derive the Bellman equation for a value function. Understand how Bellman equations relate current and future values. Define an optimal policy. Derive the Bellman optimality equation for a value function. The learning outcomes relate to the overall learning goals number 2, 7, 10, and 12 of the course. 4.2 Textbook readings For this week, you will need to read Chapter 3.5-3.7 in Sutton and Barto (2018). Read it before continuing this module. You do not have to put to much focus on the examples. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 4.3 Policies and value functions A policy \\(\\pi\\) is a distribution over actions, given some state: \\[\\pi(a | s) = \\Pr(A_t = a | S_t = s).\\] Since the MDP is stationary the policy is time-independent, i.e. given a state, we choose the same action no matter the time-step. If \\(\\pi(a | s) = 1\\) for a single state, i.e. an action is chosen with probability one always then the policy is called deterministic. Otherwise a policy is called stochastic. Given a policy we can define some value functions. The state-value function \\(v_\\pi(s)\\) denote the expected return starting from state \\(s\\) when following the policy \\(\\pi\\): \\[ \\begin{align} v_\\pi(s) &amp;= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s]. \\end{align} \\] Note the last equal sign comes from \\(G_t = R_{t+1} + \\gamma G_{t+1}\\). The action-value function \\(q_\\pi(s, a)\\), denote the expected return starting from state \\(s\\), taking action \\(a\\) and from thereon following policy \\(\\pi\\): \\[ \\begin{align} q_\\pi(s, a) &amp;= \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]. \\end{align} \\] This action-value, also known as “q-value”, is very important, as it tells us directly what action to pick in a particular state. Given the definition of q-values, the state-value function is an average over the q-values of all actions we could take in that state: \\[\\begin{equation} v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_\\pi(s, a) \\tag{4.1} \\end{equation}\\] A q-value (action-value) is equal to the expected reward \\(r(s,a)\\) that we get from choosing action \\(a\\) in state \\(s\\), plus a discounted amount of the average state-value of all the future states: \\[q_\\pi(s, a) = r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\] Joining the equations, the state-value of a particular state \\(s\\) now becomes the sum of weighted state-values of all possible subsequent states \\(s&#39;\\), where the weights are the policy probabilities: \\[ \\begin{align} v_\\pi(s) &amp;= \\sum_{a \\in \\mathcal{A}}\\pi(a | s)q_\\pi(s, a) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\right), \\end{align} \\tag{4.2} \\] which is known as the Bellman equation. 4.4 Optimal policies and value functions The objective function of an MDP can now be stated mathematically which is to find an optimal policy \\(\\pi_*\\) with state-value function: \\[v_*(s) = \\max_\\pi v_\\pi(s).\\] That is, a policy \\(\\pi&#39;\\) is defined as better than policy \\(\\pi\\) if its expected return is higher for all states. Note the objective function is not a scalar here but if the agent start in state \\(s_0\\) then we may reformulate the objective function maximize the expected return to \\[v_*(s_0) = \\max_\\pi \\mathbb{E}_\\pi[G_0 | S_0 = s_0] = \\max_\\pi v_\\pi(s_0)\\] If the MDP has the right properties (details are not given here), there exists an optimal deterministic policy \\(\\pi_*\\) which is better than or just as good as all other policies. For all such optimal policies (there may be more than one), we only need to find one optimal policy that have the optimal state-value function \\(v_*\\). We may rewrite \\(v_*(s)\\) using Eq. (4.1): \\[ \\begin{align} v_*(s) &amp;= \\max_\\pi v_\\pi(s) \\\\ &amp;= \\max_\\pi \\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_\\pi(s, a) \\\\ &amp;= \\max_\\pi \\max_a q_\\pi(s, a)\\qquad \\text{(set $\\pi(a|s) = 1$ where $q_\\pi$ is maximal)} \\\\ &amp;= \\max_a \\max_\\pi q_\\pi(s, a) \\\\ &amp;= \\max_a q_*(s, a), \\\\ \\end{align} \\] where the optimal q-value/action-value function \\(q_*\\) is: \\[ \\begin{align} q_*(s, a) &amp;= \\max_\\pi q_\\pi(s, a) \\\\ &amp;= r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_*(s&#39;) \\\\ &amp;= r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) \\max_{a&#39;} q_*(s&#39;, a&#39;). \\end{align} \\] This is the the Bellman optimality equation for \\(q_*\\) and the optimal policy is: \\[ \\pi_*(a | s) = \\begin{cases} 1 \\text{ if } a = \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\ 0 \\text { otherwise.} \\end{cases} \\] Or we may define a deterministic policy as \\[ \\begin{align} \\pi_*(s) &amp;= \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\ &amp;= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)\\right). \\end{align} \\tag{4.3} \\] Similar we can write the Bellman optimality equation for \\(v_*\\): \\[ \\begin{align} v_*(s) &amp;= \\max_a q_*(s, a) \\\\ &amp;= \\max_a r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_*(s&#39;) \\end{align} \\tag{4.4} \\] Note the Bellman equations define our state-value and q-value function, while the Bellman optimality equations define how to find the optimal value functions. Using \\(v_*\\), the optimal expected long term return is turned into a quantity that is immediately available for each state. On the other hand if we do not store \\(v_*\\), we can find \\(v_*\\) by a one-step-ahead search using \\(q_*\\), acting greedy. 4.5 Optimality vs approximation In Section 4.4 optimal policies and value functions was found; however solving the Bellman optimality equations can be expensive, e.g. if the number of states is huge. Consider a state \\(s = (x_1,\\ldots,x_n)\\) with state variables \\(x_i\\) each taking two possible values, then the number of states is \\(|\\mathcal{S}| = 2^n\\). That is, the state space grows exponentially with the number of state variables also known as the curse of dimensionality. Large state or action spaces may happen in practice; moreover, they may also be continuous. As a result we need to approximate the value functions because calculation of optimality is too expensive. This is indeed what happens in RL where we approximate the expected return. Furthermore, often we focus on states with high encountering probability while allowing the agent to make sub-optimal decisions in states that have a low probability. 4.6 Semi-MDPs (non-fixed time length) So far we have considered MDPs with a fixed length between each time-step. The model can be extended to MDPs with non-fixed time-lengths known as semi-MDPs. Let \\(l(s&#39;|s,a)\\) denote the length of a time-step given that the system is in state \\(s\\), action \\(a\\) is chosen and makes a transition to state \\(s&#39;\\). Then the discount rate over a time-step with length \\(l(s&#39;|s,a)\\) is then \\[\\gamma(s&#39;|s,a) = \\gamma^{l(s&#39;|s,a)},\\] and the Bellman optimality equations becomes: \\[ v_*(s) = \\max_a r(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) \\gamma(s&#39;|s,a) v_*(s&#39;), \\] and \\[ q_*(s, a) = r(s,a) + \\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) \\gamma(s&#39;|s,a) \\max_{a&#39;} q_*(s&#39;, a&#39;). \\] That is, the discount rate now is a part of the sum since it depends on the length which depends on the transition. 4.7 Summary Read Chapter 3.8 in Sutton and Barto (2018). 4.8 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 4.8.1 Exercise - Optimal policy Figure 4.1: A simple MDP. × Solution Let \\(\\pi_L\\) and \\(\\pi_R\\) denote the left and right policy, respectively. Recall the Bellman equation: \\[v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\right).\\] For the left policy this reduces to \\[v_{\\pi_L}(s) = 1 + \\gamma(v_\\pi(s&#39;)) = 1 + \\gamma(0 + \\gamma v_{\\pi_L}(s)).\\] Isolating \\(v_{\\pi_L}(s)\\) gives us \\[v_{\\pi_L}(s) = 1/(1-\\gamma^2).\\] Similar for the right policy we get \\[v_{\\pi_R}(s) = 0 + \\gamma(v_\\pi(s&#39;)) = 0 + \\gamma(2 + \\gamma v_{\\pi_R}(s)).\\] Isolating \\(v_{\\pi_R}(s)\\) gives us \\[v_{\\pi_R}(s) = 2\\gamma/(1-\\gamma^2).\\] Now for \\(\\gamma=0\\) we get \\(v_{\\pi_L}(s) = 1\\) and \\(v_{\\pi_R}(s) = 0\\), i.e. left policy optimal. for \\(\\gamma=0.9\\) we get \\(v_{\\pi_L}(s) = 5.26\\) and \\(v_{\\pi_R}(s) = 9.47\\), i.e. right policy optimal. for \\(\\gamma=0.5\\) we get \\(v_{\\pi_L}(s) = 1.33\\) and \\(v_{\\pi_R}(s) = 1.33\\), i.e. both policies optimal. Close Solution Consider the transition diagram for an MDP shown in Figure 4.1 with 3 states (white circles). The only decision to be made is that in the top state \\(s\\), where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies left and right. Which policy is optimal if \\(\\gamma = 0, 0.9\\) and \\(0.5\\)? 4.8.2 Exercise - Car rental Consider a rental company with two locations, each with a capacity of 20 cars. Each day, customers arrive at each location to rent cars. If a car is available, it is rented out with a reward of $10. Otherwise the opportunity is lost. Cars become available for renting the day after they are returned. The number of cars rental requests \\(D_i\\) at Location \\(i=1,2\\) are Poisson distributed with mean 3 and 4. Similar, the number of cars returned \\(H_i\\) at Location \\(i=1,2\\) are Poisson distributed with mean 3 and 2. Cars returned resulting in more cars than the capacity are lost (and thus disappear from the problem). To ensure that cars are available where they are needed, they can be moved between the two locations overnight, at a cost of $2 per car. A maximum of five cars can be moved from one location to the other in one night. Formulate the problem as an finite MDP where the time-steps are days. × Solution \\[\\mathcal{S} = \\{ (x,y) | 0 \\leq x \\leq 20, 0 \\leq y \\leq 20 \\}\\] Close Solution Define the state space (with states \\((x,y)\\)) equal the number of cars at each location at the end of the day. × Solution \\[\\mathcal{A}(s) = \\{ a | -\\min(5,y,20-x) \\leq a \\leq min(5,x,20-y) \\}\\] Close Solution Define the action space equal the net numbers of cars moved from Location 1 to Location 2 overnight, i.e. negative if move from Location 2 to 1. × Solution The reward equals the reward of rentals minus the cost of movements. Note we have \\(\\tilde{x} = x - a\\) and \\(\\tilde{y} = x + a\\) after movement. Hence \\[r(s,a) = \\mathbb{E}[10(\\min(D_1, \\tilde{x}) + \\min(D_2, \\tilde{y}) )-2\\mid a \\mid]\\] where \\[\\mathbb{E}[\\min(D, z)] = \\sum_{i=0}^z ip(D = i) + (1-p(D\\leq z))z.\\] Close Solution Calculate the expected reward \\(r(s,a)\\). Note the inventory dynamics (number of cars) at each parking lot is independent of the other given an action \\(a\\). Let us consider Location 1 and assume that we are in state \\(x\\) and chose action \\(a\\). Then the number of cars after movement is \\(x - a\\) and after rental requests \\(x - a - \\min(D_1, x-a)\\). Next, the number of returned cars are added: \\(x - a - \\min(D_1, x-a) + H_1\\). Finally, note that if this number is above 20 (parking lot capacity), then we only have 20 cars, i.e. the inventory dynamics (number of cars at the end of the day) is \\[X = \\min(20, x-a - \\min(D_1, x-a) + H_1))).\\] × Solution Only difference is that cars moved to Location 2 is \\(a\\) (and not \\(-a\\)): \\[Y = \\min(20, y + a - \\min(D_2, y+a) + H_2)).\\] Close Solution Give the inventory dynamics for Location 2. References "],["mod-dp.html", "Module 5 Dynamic programming 5.1 Learning outcomes 5.2 Textbook readings 5.3 Policy evaluation 5.4 Policy Improvement 5.5 Policy Iteration 5.6 Value Iteration 5.7 Generalized policy iteration 5.8 Summary 5.9 Exercises", " Module 5 Dynamic programming The term Dynamic Programming (DP) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the principle of optimality. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations. DP do both policy evaluation (prediction) and control. Policy evaluation give us the value function \\(v_\\pi\\) given a policy \\(\\pi\\). Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations. Two main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions. Note the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a “program”. 5.1 Learning outcomes By the end of this module, you are expected to: Describe the distinction between policy evaluation and control. Identify when DP can be applied, as well as its limitations. Explain and apply iterative policy evaluation for estimating state-values given a policy. Interpret the policy improvement theorem. Explain and apply policy iteration for finding an optimal policy. Explain and apply value iteration for finding an optimal policy. Describe the ideas behind generalized policy iteration. Interpret the distinction between synchronous and asynchronous dynamic programming methods. The learning outcomes relate to the overall learning goals number 2, 4, 6, 7, 8, 10 and 12 of the course. 5.2 Textbook readings For this week, you will need to read Chapter 4-4.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 5.3 Policy evaluation The state-value function can be represented using the Bellman equation (4.2): \\[ v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\right). \\tag{5.1} \\] If the dynamics are known perfectly, this becomes a system of \\(|\\mathcal{S}|\\) simultaneous linear equations in \\(|\\mathcal{S}|\\) unknowns \\(v_\\pi(s), s \\in \\mathcal{S}\\). This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations \\(v_0, v_1, v_2, \\ldots\\), with initial approximation \\(v_0\\) chosen arbitrarily e.g. \\(v_0(s) = 0 \\: \\forall s\\) (ensuring terminal state = 0). We can use a sweep with the Bellman equation to update the values: \\[\\begin{equation} v_{k+1}(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_k(s&#39;)\\right) \\end{equation}\\] We call this update an expected update because it is based on the expectation over all possible next states, rather than a sample of reward from the next state. This update will converge to \\(v_\\pi\\) after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold \\(\\theta\\) (see Figure 5.1). Note the algorithm uses two arrays to maintain the state-value (\\(v\\) and \\(V\\)). Alternatively, a single array could be used that update values in place, i.e. \\(V\\) is used in place of \\(v\\). Hence, state-values are updated faster. Figure 5.1: Iterative policy evaluation (Sutton and Barto 2018). 5.4 Policy Improvement From the Bellman optimality equation (4.4) we have \\[ \\begin{align} \\pi_*(s) &amp;= \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\ &amp;= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)\\right). \\end{align} \\tag{5.2} \\] That is, a deterministic optimal policy can be found by choosing greedy the best action given the optimal value function. If we apply this greed action selection to the value function for a policy \\(\\pi\\) and pick the action with most \\(q\\): \\[ \\begin{align} \\pi&#39;(s) &amp;= \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s, a) \\\\ &amp;= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\right), \\end{align} \\tag{5.3} \\] then \\[ q_\\pi(s, \\pi&#39;(s)) \\geq q_\\pi(s, \\pi(s)) = v_\\pi(s) \\quad \\forall s \\in \\mathcal{S}. \\] Note if \\(\\pi&#39;(s) = \\pi(s), \\forall s\\in\\mathcal{S}\\) then the Bellman optimality equation (4.4) holds and \\(\\pi\\) must be optimal; Otherwise, \\[ \\begin{align} v_\\pi(s) &amp;\\leq q_\\pi(s, \\pi&#39;(s)) = \\mathbb{E}_{\\pi&#39;}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\ &amp;\\leq \\mathbb{E}_{\\pi&#39;}[R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi&#39;(S_{t+1})) | S_t = s] \\\\ &amp;\\leq \\mathbb{E}_{\\pi&#39;}[R_{t+1} + \\gamma (R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})) | S_t = s] \\\\ &amp;\\leq \\mathbb{E}_{\\pi&#39;}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2}, \\pi&#39;(S_{t+2})) | S_t = s] \\\\ &amp;\\leq \\mathbb{E}_{\\pi&#39;}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...)) | S_t = s] \\\\ &amp;= v_{\\pi&#39;}(s), \\end{align} \\] That is, policy \\(\\pi&#39;\\) is strictly better than policy \\(\\pi\\) since there is at least one state \\(s\\) for which \\(v_{\\pi&#39;}(s) &gt; v_\\pi(s)\\). We can formalize the above deductions in a theorem. Theorem 5.1 (Policy improvement theorem) Let \\(\\pi\\), \\(\\pi&#39;\\) be any pair of deterministic policies, such that \\[\\begin{equation} q_\\pi(s, \\pi&#39;(s)) \\geq v_\\pi(s) \\quad \\forall s \\in \\mathcal{S}. \\end{equation}\\] That is, \\(\\pi&#39;\\) is as least as good as \\(\\pi\\). 5.5 Policy Iteration Given the policy improvement theorem we can now improve policies iteratively until we find an optimal policy: Pick an arbitrary initial policy \\(\\pi\\). Given a policy \\(\\pi\\), estimate \\(v_\\pi(s)\\) via the policy evaluation algorithm. Generate a new, improved policy \\(\\pi&#39; \\geq \\pi\\) by greedily picking \\(\\pi&#39; = \\text{greedy}(v_\\pi)\\) using Eq. (5.3). If \\(\\pi&#39;=\\pi\\) then stop (\\(\\pi_*\\) has been found); otherwise go to Step 2. The algorithm is given in Figure 5.2. The sequence of calculations will be: \\[\\pi_0 \\xrightarrow[]{E} v_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} v_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} v_{\\pi_2} \\ldots \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} v_{*}\\] The number of steps of policy iteration needed to find the optimal policy are often low. Figure 5.2: Policy iteration (Sutton and Barto 2018). 5.6 Value Iteration Policy iteration requires full policy evaluation at each iteration step. This could be an computationally expensive process which requires may sweeps of the state space. In value iteration, the policy evaluation is stopped after one sweep of the state space. Value iteration is achieved by turning the Bellman optimality equation into an update rule: \\[ v_{k+1}(s) = \\max_a \\left(r(s,a) + \\gamma\\sum_{s&#39;} p(s&#39;|s, a)v_k(s&#39;)\\right) \\] Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement, since it performs a greedy update while also evaluating the current policy. Also, it is important to understand that the value-iteration algorithm does not require a policy to work. No actions have to be chosen. Rather, the state-values are updated and after the last step of value-iteration the optimal policy \\(\\pi_*\\) is found: \\[ \\pi_*(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)\\right), \\] The algorithm is given in Figure 5.3. Since we do not want an infinite number of iterations we introduce a threshold \\(\\theta\\). The sequence of calculations will be (where G denotes greedy action selection): \\[v_{0} \\xrightarrow[]{EI} v_{1} \\xrightarrow[]{EI} v_{2} \\ldots \\xrightarrow[]{EI} v_{*} \\xrightarrow[]{G} \\pi_{*}\\] Figure 5.3: Value iteration (Sutton and Barto 2018). 5.7 Generalized policy iteration Generalised Policy Iteration (GPI) is the process of letting policy evaluation and policy improvement interact, independent of granularity. For instance, improvement/evaluation can be performed by doing complete sweeps of the state space (policy iteration), or improve the state-value using a single sweep of the state space (value iteration). GPI can also do asynchronous updates of the state-value where states are updated individually, in any order. This can significantly improve computation. Examples on asynchronous DP are In-place DP mentioned in Section 5.3 where instead of keeping a copy of the old and new value function in each value-iteration update, you can just update the value functions in-place. Hence asynchronous updates in other parts of the state-space will directly be affected resulting in faster updates. Prioritized sweeping where we keep track of how “effective” or “significant” updates to our state-values are. States where the updates are more significant are likely further away from converging to the optimal value. As such, we would like to update them first. For this, we would compute the Bellman error: \\[|v_{k+1}(s) - v_k(s)|,\\] and keep these values in a priority queue. You can then efficiently pop the top of it to always get the state you should update next. Prioritize local updates where you update nearby states given the current state, e.g. if your robot is in a particular region of the grid, it is much more important to update nearby states than faraway ones. GPI works and will convergence to the optimal policy and optimal value function if the states are visited (in theory) an infinite number of times. That is, you must explore the whole state space for GPI to work. 5.8 Summary Read Chapter 4.8 in Sutton and Barto (2018). 5.9 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 5.9.1 Exercise - Gambler’s problem Consider the gambler’s problem in Exercise ??. Solve the problem using … 5.9.2 Exercise - Car rental Consider the car rental problem in Exercise 4.8.2 with inventory dynamics: \\[X = \\min(20, \\max(0, x&#39; - a - D_1) + H_1))),\\] and \\[Y = \\min(20, \\max(0, y&#39; + a - D_2) + H_2))),\\] for Location 1 and 2, respectively. The transition probabilities can be split due to independence: \\[ p((x,y) | (x&#39;,y&#39;), a) = p(x | x&#39;, a) p(y | y&#39;, a) \\] Solve the problem using … References "],["mod-mc.html", "Module 6 Monte Carlo methods for prediction and control 6.1 Learning outcomes 6.2 Textbook readings 6.3 MC prediction (evaluation) 6.4 MC control (improvement) 6.5 Off-policy MC prediction 6.6 Off-policy control (improvement) 6.7 Summary 6.8 Exercises", " Module 6 Monte Carlo methods for prediction and control The term “Monte Carlo” (MC) is often used for an estimation method which involves a random component. MC methods of RL learn state and action values by sampling and averaging returns. MC do not use dynamics where we estimate the value in the current state using the value in the next state (like in dynamic programming). Instead the MC methods estimate the values by considering different sample-paths (state, action and reward realizations). Compared to a Markov decision process, MC methods are model-free since they not require full knowledge of the transition probabilities and rewards (a model of the environment) instead MC methods learn the value function directly from experience. Often though, the sample-path is generated using simulation, i.e. some knowledge about the environment is given, but it is only used to generate sample transitions. For instance, consider an MDP model for the game Blackjack. Here calculating all the transition probabilities may be tedious and error-prone in terms of coding and numerical precision. Instead we can simulate a game (a sample-path) and use the simulations to evaluate/predict the value function of a policy and then use control to find a good policy. That is, we still use a generalised policy iteration framework, but instead of computing the value function using the MDP model a priori, we learn it from experience. MC methods can be used for processes with episodes, i.e. where there is a terminal state. This reduces the length of the sample-path and the value of the states visited on the path can be updated based on the reward received. 6.1 Learning outcomes By the end of this module, you are expected to: Identify the difference between model-based and model-free RL. Identify problems that can be solved using Monte-Carlo methods. Describe how MC methods can be used to estimate value functions from sample data. Do MC prediction to estimate the value function for a given policy. Explain why it is important to maintain exploration in MC algorithms. Do policy improvement (control) using MC in a generalized policy improvement algorithm. Compare different ways of exploring the state-action space. Argue why off-policy learning can help deal with the exploration problem. Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution. Use importance sampling in off-policy learning to predict the value-function of a target policy. Explain how to modify the MC prediction and improvement algorithm for off-policy learning. The learning outcomes relate to the overall learning goals number 3, 4, 9 and 12 of the course. 6.2 Textbook readings For this week, you will need to read Chapter 5-5.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 6.3 MC prediction (evaluation) Given a policy \\(\\pi\\), we want to estimate the state-value function. Recall that the state value function is \\[ v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]. \\] where the return is \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1} \\] Now given policy \\(\\pi\\) and a sample-path (episode) \\(S_0, A_0, R_1, S_1, A_1, \\ldots, S_{T-1}, A_{T-1}, R_T\\) ending in the terminal state at time \\(T\\), we can calculate the realized return for each state in the sample-path. Each time we have a new sample-path a new realized return for the states is given and the average for the returns in a state is an estimate of the state-value. With enough observations, the sample average converges to the true state-value under the policy \\(\\pi\\). Given a policy \\(\\pi\\) and a set of sample-paths, there are two ways to estimate the state values \\(v_\\pi(s)\\): First visit MC: average returns from first visit to state \\(s\\). Every visit MC: average returns following every visit to state \\(s\\). First visit MC generates iid estimates of \\(v_\\pi(s)\\) with finite variance, so the sequence of estimates converges to the expected value by the law of large numbers as the number of observations grow. Every visit MC does not generate independent estimates, but still converges. An algorithm for first visit MC is given in Figure 6.1. The state-value estimate is stored in a vector \\(V\\) and the returns for each state in a list. Given a sample-path we add the return to the states on the path by scanning the path backwards and updating \\(G\\). Note since the algorithm considers first visit MC, a check of occurrence of the state earlier in the path done. If this check is dropped, we have a every visit MC algorithm instead. Moreover, the computation needed to update the state-value does not depend on the size of the process/MDP but only of the length of the sample-path. Figure 6.1: MC policy prediction (Sutton and Barto 2018). The algorithm maintains a list of all returns for each state which may require a lot of memory. Instead as incremental update of \\(V\\) can be done. Adapting Eq. (2.1), we have that the sample average can be updated using: \\[ V(s) \\leftarrow V(s) + \\frac{1}{n} \\left[G - V(s)\\right]. \\] where \\(n\\) denote the number of realized returns found for state \\(s\\) and \\(G\\) the current realized return. The state-value vector must be initialized to zero and a vector counting the number of returns found for each state must be stored. 6.3.1 MC prediction of action-values With a model of the environment we only need to estimate the state-value function, since it is easy to determine the policy from the state-values using the Bellman optimality equations (4.3). However, if we do not know the expected reward and transition probabilities state values are not enough. In that case, it is useful to estimate action-values since the optimal policy can be found using \\(q_*\\) (see Eq. (4.3)). To find \\(q_*\\), we first need to predict action-values for a policy \\(\\pi\\). This is essentially the same as for state-values, only we now talk about state-action pairs being visited, i.e. taking action \\(a\\) in state \\(s\\) instead. If \\(\\pi\\) is deterministic, then we will only estimate the values of actions that \\(\\pi\\) dictates. Therefore some exploration are needed in order to have estimates for all action-values. Two possibilities are: Make \\(\\pi\\) stochastic, e.g. \\(\\varepsilon\\)-soft that that have non-zero probability of selecting each state-action pair. Use exploring starts, which specifies that ever state-action pair has non-zero probability of being selected as the starting state of an sample-path. 6.4 MC control (improvement) We are now ready to formulate a generalized policy iteration (GPI) algorithm using MC to predict the action-values \\(q(s,a)\\). Policy improvement is done by selecting the next policy greedy with respect to the action-value function: \\[ \\pi(s) = \\arg\\max_a q(s, a). \\] That is, we generate a sequence of policies and action-value functions \\[\\pi_0 \\xrightarrow[]{E} q_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} q_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} q_{\\pi_2} \\xrightarrow[]{I} \\ldots \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} q_{*}.\\] Hence the policy improvement theorem applies for all \\(s \\in \\mathcal{S}\\): \\[\\begin{align} q_{\\pi_k}(s, a=\\pi_{k+1}(s)) &amp;= q_{\\pi_k}(s, \\arg\\max_a q_{\\pi_k}(s, a)) \\\\ &amp;= \\max_a q_{\\pi_k}(s, a) \\\\ &amp;\\geq q_{\\pi_k}(s, \\pi_k(s))\\\\ &amp;= v_{\\pi_k}(s) \\end{align}\\] That is, \\(\\pi_{k+1}\\) is better than \\(\\pi_k\\) or optimal. It is important to understand the major difference between model-based GPI (remember that a model means the transition probability matrix and reward distribution are known) and model-free GPI. We cannot simply use a 100% greedy strategy all the time, since all our action-values are estimates. As such, we now need to introduce an element of exploration into our algorithm to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy: Infinite exploration: all state-action \\((s,a)\\) pairs should be explored infinitely many times as the number of iterations go to infinity (in the limit), i.e. as the number of iterations \\(k\\) goes to infinity the number of visits \\(n_k\\) does too \\[\\lim_{k\\rightarrow\\infty} n_k(s, a) = \\infty.\\] Greedy in the limit: while we maintain infinite exploration, we do eventually need to converge to the optimal policy: \\[\\lim_{k\\rightarrow\\infty} \\pi_k(a|s) = 1 \\text{ for } a = \\arg\\max_a q(s, a).\\] 6.4.1 GPI with exploring starts An algorithm using exploring starts and first visit MC is given in Figure 6.2. It satisfies the convergence properties and and incremental implementation can be used to update \\(Q\\). Note that to predict the action-values for a policy, we in general need a large number of sample-paths. However, much like we did with value iteration, we do not need to fully evaluate the value function for a given policy. Instead we can merely move the value toward the correct value and then switch to policy improvement thereafter. To stop the algorithm from having infinitely many sample-paths we may stop the algorithm once the \\(q_{\\pi_k}\\) stop moving within a certain error. Figure 6.2: GPI using MC policy prediction with exploring starts (Sutton and Barto 2018). 6.4.2 GPI using \\(\\epsilon\\)-soft policies Note by using exploring starts in Algorithm 6.2, the ‘infinite exploration’ convergence assumption is satisfied. However exploring starts may be hard to use in practice. Another approach to ensure infinite exploration is to use a soft policy, i.e. assign a non-zero probability to each possible action in a state. An on-policy algorithm using \\(\\epsilon\\)-greedy policies is given in Figure 6.3. Here we put probability \\(1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\) on the maximal action and \\(\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\) on each of the others. Note using \\(\\epsilon\\)-greedy policy selection will improve the current policy; otherwise we have found best policy amongst the \\(\\epsilon\\)-soft policies. If we want to find the optimal policy we have to ensure the ‘greedy in the limit’ convergence assumption. This can be done by decreasing \\(\\epsilon\\) as the number of iterations increase (e.g. \\(\\epsilon = 1/k\\)). Figure 6.3: On-policy GPI using MC policy prediction (Sutton and Barto 2018). 6.5 Off-policy MC prediction Until now we have only considered what is denoted on-policy algorithms for finding the optimal policy. Here we both evaluate or improve the policy that is used to make decisions. To ensure infinite exploration we use for instance exploring starts or \\(\\epsilon\\)-soft policies. Off-policy methods use a different approach by considering two policies: a policy \\(b\\) used to generate the sample-path (behaviour policy) and a policy \\(\\pi\\) that is learned for control (target policy). We update the target policy using the sample-paths from the behaviour policy. The behaviour policy explores the environment for us during training and must ensure infinite exploration. Moreover, the coverage assumption must be satisfied: \\[\\pi(a|s) &gt; 0 \\rightarrow b(a|s) &gt; 0\\] That is, every action in \\(\\pi\\) must also be taken, at least occasionally, by \\(b\\). Put differently, to learn \\(\\pi\\) we must sample paths that occur when using \\(\\pi\\). Note target policy \\(\\pi\\) may be deterministic by using greedy selection with respect to action-value estimates (greedy in the limit satisfied). Off-policy learning methods are powerful and more general than on-policy methods (on-policy methods being a special case of off-policy where target and behaviour policies are the same). They can be used to learn from data generated by a conventional non-learning controller or from a human expert. But how do we estimate the expected return using the target policy when we only have sample-paths from the behaviour policy? For this we need to introduce importance sampling, a general technique for estimating expected values under one distribution given samples from another. Let us first explain it using two distributions \\(a\\) and \\(b\\) where we want to estimated the mean of \\(a\\) given data/samples from \\(b\\), then \\[ \\begin{align} \\mathbb{E}_{a}[X] &amp;= \\sum_{x\\in X} a(x)x \\\\ &amp;= \\sum_{x\\in X} a(x)\\frac{b(x)}{b(x)}x \\\\ &amp;= \\sum_{x\\in X} b(x)\\frac{a(x)}{b(x)}x \\\\ &amp;= \\sum_{x\\in X} b(x)\\rho(x)x \\\\ &amp;= \\mathbb{E}_{b}\\left[\\rho(X)X\\right]. \\end{align} \\] Hence to the mean of \\(a\\) can be found by finding the mean of \\(\\rho(X)X\\) where \\(X\\) is has a \\(b\\) distribution and \\(\\rho(x) = a(x)/b(x)\\) denote the importance sampling ratio. Note given samples \\((x_1,\\ldots,x_n)\\) from \\(b\\) we then can calculate the sample average using \\[ \\begin{align} \\mathbb{E}_{a}[X] &amp;= \\mathbb{E}_{b}\\left[\\rho(X)X\\right] \\\\ &amp;\\approx \\frac{1}{n}\\sum_{i = 1}^n \\rho(x_i)x_i \\\\ \\end{align} \\tag{6.1} \\] Now let us use importance sampling on the target policy \\(\\pi\\) and behaviour policy \\(b\\). Given state \\(S_t\\) and sample path, we want to find \\[v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s] = \\mathbb{E}_{b}[\\rho(G_t)G_t|S_t = s],\\] or since we base our estimates on sample-paths, we are in fact interested in estimating the action-values \\[q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] = \\mathbb{E}_{b}[\\rho(G_t)G_t|S_t = s, A_t = a].\\] For this we need the importance sampling ratio given a certain sample-path \\(S_t, A_t, R_{t+1}, \\ldots, R_T, S_T\\) with return \\(G_t\\): \\[ \\begin{align} \\rho(G_t) &amp;= \\frac{\\Pr{}(S_t, A_t, \\dots S_T| S_t = s, A_t = a, \\pi)}{\\Pr{}(S_t, A_t, \\dots, S_T)| S_t = s, A_t = a, b)} \\\\ &amp;= \\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)\\Pr{}(S_{k+1}|S_k, A_k)}{\\prod_{k=t}^{T-1}b(A_k|S_k)\\Pr{}(S_{k+1}|S_k, A_k)}\\\\ &amp;=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}. \\end{align} \\tag{6.2} \\] Note the transition probabilities cancel out, i.e. the ratio does not depend on the MDP dynamics by only the policies. Moreover, importance sampling ratios are only non-zero for sample-paths where the target-policy has non-zero probability of acting exactly like the behaviour policy \\(b\\). So, if the behaviour policy takes 10 steps in an sample-path, each of these 10 steps have to have been possible by the target policy, else \\(\\pi(a|s) = 0\\) and \\(\\rho_{t:T-1} = 0\\). We can now approx. \\(q_\\pi(s,a)\\) by rewriting Eq. (6.1) for \\(\\pi\\) given returns from \\(b\\) to \\[ q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\approx \\frac{1}{n} \\sum_{i = 1}^n \\rho_iG_i, \\tag{6.3} \\] where we assume that given the sample-paths (episodes), have \\(n\\) observations of the return \\((G_1, \\ldots, G_n)\\) in state \\(s\\) taking action \\(a\\) with the importance sampling ratio \\(\\rho_i\\) calculated using Eq. (6.2). As a result if we consider the prediction algorithm in Figure 6.1 it must be modified by: Generate an sample-path using policy \\(b\\) instead of \\(\\pi\\). Add a variable W representing the importance sampling ratio which must be set to 1 on line containing \\(G \\leftarrow 0\\). Modify line \\(G \\leftarrow \\gamma G + R_{t+1}\\) to \\(G \\leftarrow \\gamma WG + R_{t+1}\\) since we now need to multiply with the importance sampling ratio. Add a line after the last with \\(W \\leftarrow W \\pi(A_t|S_t)/b(A_t|S_t)\\), i.e. we update the importance sampling ratio. Note if \\(\\pi(A_t|S_t) = 0\\) then we may stop the inner loop earlier (\\(W=0\\) for the remaining \\(t\\)). Finally, an incremental update of \\(V\\) can be done having a vector counting the number of of returns found for each state. Then the incremental update is \\[ V(s) \\leftarrow V(s) + \\frac{1}{n} \\left[WG - V(s)\\right]. \\tag{6.4} \\] where \\(n\\) denote the number of realized returns found for state \\(s\\) and \\(G\\) the current realized return. 6.5.1 Weighted importance sampling When using a sample average the importance sampling method is called ordinary importance sampling. Ordinary importance sampling may result in a high variance which is not good. As a result we may use other weights and instead of Eq. (6.3) use the estimate (weighted importance sampling): \\[ q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\approx \\frac{1}{\\sum_{i = 1}^n \\rho_i} \\sum_{i = 1}^n \\rho_iG_i. \\] An incremental update then becomes: \\[ \\begin{align} q_\\pi(s,a) &amp;\\approx V_{n+1} \\\\ &amp;= \\frac{1}{\\sum_{i = 1}^n \\rho_i} \\sum_{i = 1}^n \\rho_iG_i \\\\ &amp;= \\frac{1}{C_n} \\sum_{i = 1}^n W_iG_i \\\\ &amp;= \\frac{1}{C_n} (W_nG_n + C_{n-1}\\frac{1}{C_{n-1}} \\sum_{i = 1}^{n-1} W_iG_i) \\\\ &amp;= \\frac{1}{C_n} (W_nG_n + C_{n-1}V_n) \\\\ &amp;= \\frac{1}{C_n} (W_nG_n + (C_{n} - W_{n}) V_n) \\\\ &amp;= \\frac{1}{C_n} (W_nG_n + C_{n}V_n - W_{n} V_n) \\\\ &amp;= V_n + \\frac{W_n}{C_n} (G_n - V_n), \\end{align} \\tag{6.5} \\] where \\(C_n = \\sum_{i = 1}^n \\rho_i\\) is the sum of the ratios and and \\(W_n\\) the ratio for the n’th return. Using weighted importance sampling gives a smaller variance and hence faster convergence. An off-policy prediction algorithm using weighted importance sampling and incremental updates is given in Figure 6.4. Figure 6.4: Off-policy MC prediction (Sutton and Barto 2018). Note both Eq. (6.4) and Eq. (6.5) follows the general incremental formula: \\[\\begin{equation} New Estimate \\leftarrow Old Estimate + Step Size \\left[Observation - Old Estimate \\right]. \\end{equation}\\] For ordinary importance sampling the step-size is \\(1/n\\) and for weighted importance sampling the step-size is \\(W_n/C_n\\). 6.6 Off-policy control (improvement) Having a discussed a framework for off-policy MC prediction, we can now give a GPI algorithm for off-policy MC control that estimate \\(\\pi_*\\) and \\(q_*\\) by using rewards obtained through behaviour policy \\(b\\). We will focus on using weighted importance sampling with incremental updates. The algorithm is given in Figure 6.5. The target policy \\(\\pi\\) is the greedy policy with respect to \\(Q\\), which is an estimate of \\(q_\\pi\\). This algorithm converges to \\(q_\\pi\\) as long as an infinite number of returns are observed for each state-action pair. This can be achieved by making \\(b\\) \\(\\varepsilon\\)-soft. The policy \\(\\pi\\) converges to \\(\\pi_*\\) at all encountered states even if \\(b\\) changes (to another \\(\\varepsilon\\)-soft policy) between or within sample-paths. Note we exit the inner loop if \\(A_t \\neq \\pi(S_t)\\) which implies \\(W=0\\). Figure 6.5: Off-policy GPI (Sutton and Barto 2018). Notice that this policy only learns from sample-paths in which \\(b\\) selects only greedy actions after some timestep. This can greatly slow learning. 6.7 Summary Read Chapter 5.10 in Sutton and Barto (2018). 6.8 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! References "],["mod-td-pred.html", "Module 7 Temporal difference methods for prediction 7.1 Learning outcomes 7.2 Textbook readings 7.3 What is TD learning? 7.4 TD prediction 7.5 Benefits of TD methods 7.6 Exercises", " Module 7 Temporal difference methods for prediction One of the most fundamental concepts in reinforcement learning is temporal difference (TD) learning. TD learning is a combination of Monte Carlo (MC) and dynamic programming (DP) ideas: Like MC, TD can predict using a model-free environment and learn from experience. Like DP, TD update estimates based on other learned estimates, without waiting for a final outcome (bootstrap). That is, TD can learn on-line and do not need to wait until the whole sample-path is found. TD is in general learn more efficiently than MC due to bootstrapping. In this module prediction using TD is considered. 7.1 Learning outcomes By the end of this module, you are expected to: Describe what Temporal Difference (TD) learning is. Formulate the incremental update formula for TD learning. Define the temporal-difference error. Interpret the role of a fixed step-size. Identify key advantages of TD methods over DP and MC methods. Explain the TD(0) prediction algorithm. Understand the benefits of learning online with TD compared to MC methods. The learning outcomes relate to the overall learning goals number 3, 4, 6, 9, and 12 of the course. 7.2 Textbook readings For this week, you will need to read Chapter 6-6.3 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 7.3 What is TD learning? Given a policy \\(\\pi\\), we want to estimate the state-value function. Recall that the state value function is \\[ v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]. \\] where the return is \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1} \\] Let \\(V\\) denote the state-value estimate. Under MC prediction we used an incremental update formula: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha_n\\left[G_t - V(S_t)\\right], \\] where \\(n\\) denote the number of observations and \\(\\alpha_n\\) the step-size. Different values of \\(\\alpha_n\\) was discussed in Module 6. Here we assumed a stationary environment (state set, transition probabilities etc. is the same for each stage \\(t\\)) e.g. for the sample average \\(\\alpha_n = 1/n\\). If the environment is non-stationary (e.g. transition probabilities change over time) then a fixed step-size may be appropriate. Let us for the remaining of this module consider a non-stationary process with fixed step-size: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha\\left[G_t - V(S_t)\\right], \\] Note as pointed out in Section 2.5, a fixed step-size corresponds to a weighted average of the past observed returns and the initial estimate of \\(S_t\\): \\[ \\begin{align} V_{n+1} &amp;= V_n +\\alpha \\left[G_n - V_n\\right] \\nonumber \\\\ &amp;= \\alpha G_n + (1 - \\alpha)V_n \\nonumber \\\\ &amp;= \\alpha G_n + (1 - \\alpha)[\\alpha G_{n-1} + (1 - \\alpha)V_{n-1}] \\nonumber \\\\ &amp;= \\alpha G_n + (1 - \\alpha)\\alpha G_{n-1} + (1 - \\alpha)^2 V_{n-1} \\nonumber \\\\ &amp; \\vdots \\nonumber \\\\ &amp;= (1-\\alpha)^n V_1 + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^{n-i} G_i \\\\ \\end{align} \\] That is, a larger weight is used for recent observations compared to old observations. For MC prediction we needed the sample path to get the realized return \\(G_t\\). However, since \\[ \\begin{align} v_\\pi(s) &amp;= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1}| S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1}| S_t = s] + \\gamma v_\\pi(S_{t+1}), \\end{align} \\] then, given a realized reward \\(R_{t+1}\\), an estimate for the return \\(G_t\\) is \\(R_{t+1} + \\gamma V(S_{t+1})\\) and the incremental update becomes: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha\\left[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\right]. \\tag{7.1} \\] As a result, we do not have to generate a whole sample-path (as for MC) for updating the state-value estimate of \\(s = S_t\\) to \\(V(S_t)\\). Instead we only have to wait until the next state is observed and update the estimate of \\(S_t\\) given the estimate of the next state \\(S_{t+1}\\). As the estimate of \\(S_{t+1}\\) improve the estimate of \\(S_t\\) also improve. The incremental update in Eq. (7.1) is called TD(0) or one-step TD because it use a one-step lookahead to update the estimate. Note updating the estimates using TD resembles the way we did for DP: \\[ V(s = S_t) \\leftarrow \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) V(s&#39;)\\right) \\] Here we updated the value by considering the expectation of all the next states. This was possible since we had a model. Now, by using TD, we do not need a model to estimate the state-value. The term \\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t), \\] is denoted the temporal difference error (TD error) since it is the difference between the current estimate \\(V(S_t)\\) and the better estimate \\(R_{t+1} + \\gamma V(S_{t+1})\\). 7.4 TD prediction We can now formulate a TD(0) algorithm for predicting state-values of a policy (see Figure 7.1). No stopping criterion is given but could stop when small differences in state-values are observed. Figure 7.1: TD(0) policy prediction (Sutton and Barto 2018). The algorithm is given for a process with episodes; however, also works for continuing processes. In this case the inner loop runs over an infinite number of time-steps. 7.4.1 TD prediction for action-values Later we will use TD to for improving the policy (control). Since we do not have a model we need to estimate action-values instead and the optimal policy can be found using \\(q_*\\) (see Eq. (4.3)). To find \\(q_*\\), we first need to predict action-values for a policy \\(\\pi\\) and the incremental update Eq. (7.1) must be modified to use \\(Q\\) values: \\[ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\left[R_{t+1} + \\gamma Q(S_{t+1}, A_t) - Q(S_t, A_t)\\right]. \\] Note given a policy \\(\\pi\\) you need to know \\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}\\) or short SARSA before you can make an update. This acronym is used to name the SARSA algorithm for control in Module 8. Note to ensure exploration of all action-values we need e.g. an \\(\\epsilon\\)-soft behavioural policy. 7.5 Benefits of TD methods Let us try to summarize the benefits of TD prediction TD methods do not require a model of the environment (compared to DP). TD methods can be implemented online, which can speed convergence (compared to MC methods which must wait until the end of the sample-path). TD methods learn from all actions, whereas MC methods require the sample-path to have a tail equal to the target policy. TD methods do converge on the value function with a sufficiently small step-size parameter, or with a decreasing step-size. TD methods generally converge faster than MC methods, although this has not been formally proven. TD methods are extremely useful for continuing tasks that cannot be broken down into episodes as required by MC methods. TD can be seen as a method for prediction learning where you try to predict what happens next given you current action, get new information and make a new prediction. That is, you do not need a training set (as in supervised learning) instead the reward signal is observed as time goes by. TD methods are good for sequential decision problems (multi-step prediction). TD methods are scalable in the sense that computations do not grow exponentially with the problem size. An example illustrating that TD methods converge faster than MC methods is given in Exercise 7.6.1 7.6 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 7.6.1 Exercise - A randow walk Consider a MDP with states A-E and two terminal states. Possible transitions are given in Figure 7.2. All episodes start in the centre state, C, then proceed either left or right by one state on each step. We assume the stochastic policy \\(\\pi\\) is used where each direction has equal probability. Episodes terminate either on the left (T1) or the right (T2). When an episode terminates on the right, reward of 1 occurs; all other rewards are zero. If the discount factor equals 1, the state-value of each state is the probability of terminating on the right if starting from that state. Figure 7.2: Possible transitions between states and rewards. × Solution The state space is \\(\\mathcal{S} = \\{ T1, A, \\ldots, E, T2 \\}\\) with \\(\\mathcal{A}(s) = \\{ \\text{left}, \\text{right}\\}\\) (transition to the neighbour states) except for terminating states which have no actions (see Figure 7.2). Rewards are deterministic \\(\\mathcal{R} = \\{0, 1\\}\\) (see Figure 7.2) which also holds for the transition probabilities. The state-value can be found using the Bellman equations (5.1) \\[v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} p(s&#39; | s, a) v_\\pi(s&#39;)\\right),\\] which becomes \\[\\begin{align}v_\\pi(A) &amp;= 0.5v_\\pi(T1) + 0.5v_\\pi(B) = 0.5v_\\pi(B) \\\\ v_\\pi(B) &amp;= 0.5v_\\pi(A) + 0.5v_\\pi( C ) \\\\ v_\\pi( C ) &amp;= 0.5v_\\pi(B) + 0.5v_\\pi(D) \\\\ v_\\pi(D) &amp;= 0.5v_\\pi( C ) + 0.5v_\\pi(E) \\\\ v_\\pi(E) &amp;= 0.5v_\\pi(D) + 0.5(1 + v_\\pi(T2)) = 0.5v_\\pi(D) + 0.5\\\\ \\end{align}\\] Solving the equations with a state-value equal to 0 (you may also use that by symmetry \\(v_\\pi(C ) = 0.5\\)) for the terminating states gives state-values \\(\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}, \\frac{4}{6}\\) and \\(\\frac{5}{6}\\) for A-E, respectively. Close Solution Formulate the MDP model and calculate the state-value \\(v_\\pi\\) for each state using the Bellman equations (5.1). Consider an episode with sequence \\(C, 0, B, 0, C, 0, D, 0, E, 1\\). Let the initial state-value estimates be 0.5 and update the state-values using TD(0) with \\(\\alpha = 0.1\\). It appears that only \\(V(A)\\) change. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed? Generate 100 episodes and run the TD(0) prediction algorithm with \\(\\alpha = 0.1\\) (see Figure 7.1). Make a plot of the state-value estimate (y-axis) given state A-E (x-axis) for TD(0) running for 1, 10 and 100 episodes. You may use the code below as a starting point. set.seed(875) Run an MC prediction algorithm with \\(\\alpha = 0.1\\) (see Figure 6.1 running for 1, 10 and 100 episodes. The results are dependent on the value of the step-size parameter. Try estimating the state-values using TD(0) and MC for \\(\\alpha = 0.2, 0.1, 0.05\\) and 0.025. Plot the root mean square (RMS) error \\[\\sqrt{\\frac{1}{5}\\sum_{s=A}^E(V(s)-v_\\pi(s))^2}\\] (y-axis) given the number of episodes (x-axis).Do you think the conclusions about that TD(0) is better than MC is affected by different values? In the right graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high \\(\\alpha\\)’s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized? 7.6.2 Exercise - Off-policy TD Design an off-policy version of the TD(0) update that can be used with arbitrary target policy \\(\\pi\\) and covering behaviour policy b, using at each step t the importance sampling ratio $_{t:t}. References "],["mod-td-control.html", "Module 8 Temporal difference methods for control 8.1 Learning outcomes 8.2 Textbook readings 8.3 SARSA - On-policy GPI using TD 8.4 Q-learning - Off-policy GPI using TD 8.5 Expected SARSA - GPI using TD 8.6 Summary 8.7 Exercises", " Module 8 Temporal difference methods for control In Module 7 temporal difference (TD) was used to estimate state-values. In this module we focus on improving the policy (control) by applying generalized policy iteration (GPI) using TD methods. GPI repeatedly apply policy evaluation and policy improvement. Since we do not have a model (the transition probability matrix and reward distribution are known) all our action-values are estimates. Hence an element of exploration are needed to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy: Infinite exploration: all state-action \\((s,a)\\) pairs should be explored infinitely many times as the number of iterations go to infinity (in the limit), i.e. as the number of iterations \\(k\\) goes to infinity the number of visits \\(n_k\\) does too \\[\\lim_{k\\rightarrow\\infty} n_k(s, a) = \\infty.\\] Greedy in the limit: while we maintain infinite exploration, we do eventually need to converge to the optimal policy: \\[\\lim_{k\\rightarrow\\infty} \\pi_k(a|s) = 1 \\text{ for } a = \\arg\\max_a q(s, a).\\] 8.1 Learning outcomes By the end of this module, you are expected to: Describe how generalized policy iteration (GPI) can be used with TD to find improved policies. Identify the properties that must the satisfied for GPI to converge to the optimal policy. Derive and explain SARSA an on-policy GPI algorithm using TD. Describe the relationship between SARSA and the Bellman equations. Derive and explain Q-learning an off-policy GPI algorithm using TD. Argue how Q-learning can be off-policy without using importance sampling. Describe the relationship between Q-learning and the Bellman optimality equations. Derive and explain expected SARSA an on/off-policy GPI algorithm using TD. Describe the relationship between expected SARSA and the Bellman equations. Explain how expected SARSA generalizes Q-learning. List the differences between Q-learning, SARSA and expected SARSA. Apply the algorithms to an MDP to find the optimal policy. The learning outcomes relate to the overall learning goals number 3, 4, 6, 9, and 12 of the course. 8.2 Textbook readings For this week, you will need to read Chapter 6.4-6.6 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here. Slides for this module can be seen here. You do not have to look at them before the lecture! 8.3 SARSA - On-policy GPI using TD The first GPI algorithm we will consider is SARSA. Since we do not have a model we need to estimate action-values so the optimal policy can be found using \\(q_*\\) (see Eq. (4.3)). Hence to predict action-values for a policy \\(\\pi\\), the incremental update Eq. (7.1) must be modified to use \\(Q\\) values: \\[ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] \\] Note given a policy \\(\\pi\\) you need to know \\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}\\) or short SARSA before you can make an update. This acronym is used to name the algorithm. The algorithm is given in Figure 8.1. To ensure infinite exploration of all action-values, we need e.g. an \\(\\epsilon\\)-greedy policy. The algorithm can also be applied for processes with continuing tasks. To ensure greedy in the limit a decreasing epsilon can be used (e.g. \\(\\epsilon = 1/t\\)). No stopping criterion is given but could stop when small differences in action-values are observed. Figure 8.1: SARSA - On-policy GPI using TD (Sutton and Barto 2018). SARSA is a sample based algorithm that do updates based on the Bellman equation for action-values (\\(q\\)): \\[ \\begin{align} q_\\pi(s, a) &amp;= \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma v_\\pi(s&#39;)\\right) \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma \\sum_{a&#39;} \\pi(a&#39;|s) q_\\pi(s&#39;, a&#39;)\\right). \\end{align} \\] That is, we update the estimate based on samples \\(r\\) and the estimate \\(q_\\pi\\) in \\(s&#39;\\). This is the same approach as policy iteration in DP: we first calculate new estimates of \\(q_\\pi\\) given the current policy \\(\\pi\\) and then improve. Hence SARSA is a sample based version of policy iteration in DP. 8.4 Q-learning - Off-policy GPI using TD Q-learning resembles SARSA; however there are some differences. The algorithm is given in Figure 8.2. Note the incremental update equation is now: \\[\\begin{equation} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right] \\end{equation}\\] That is, the next action used to update \\(Q\\) is selected greedy. That is, we are no longer following an \\(\\epsilon\\)-greedy policy for our updates. Figure 8.2: Q-learning - Off-policy GPI using TD (Sutton and Barto 2018). SARSA is an on-policy algorithm, meaning that the behavioural and target policy is the same, e.g. an \\(\\epsilon\\)-greedy policy to ensure exploration. That is, for fixed \\(\\epsilon\\) the greedy in the limit assumption is not fulfilled. Q-learning, on the other hand, is an off-policy algorithm where the behavioural policy is an \\(\\epsilon\\)-greedy and the target policy is the (deterministic) greedy policy. That is, Q-learning fulfil both the ‘infinite exploration’ and ‘greedy in the limit’ assumptions. Note under MC prediction an off-policy algorithm needed to use importance sampling to estimate the action-value of the target policy (see Section 6.5). This is not necessary for one-step TD, since \\[ \\begin{align} q_\\pi(s,a) &amp;= \\mathbb{E}_{\\pi}[R_t + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma \\sum_{a&#39;} \\pi(a&#39;|s) q_\\pi(s&#39;, a&#39;)\\right) \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma \\max_{a&#39;} q_\\pi(s&#39;, a&#39;)\\right) \\\\ \\end{align} \\tag{8.1} \\] That is, because the target policy is greedy and deterministic expectation the \\(G_{t+1}\\) becomes a maximum. Hence we can update the action-value estimates \\(Q\\) for the target policy \\(\\pi\\) even though we sample from an \\(\\epsilon\\)-greedy behavioural policy. Q-learning is a sample based algorithm that do updates based on the Bellman optimality equation for action-values (\\(q_*\\)): \\[ \\begin{align} q_*(s, a) &amp;= \\max_\\pi q_\\pi(s, a) \\\\ &amp;= \\max_\\pi \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma v_\\pi(s&#39;)\\right) \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma \\max_\\pi v_\\pi(s&#39;)\\right) \\\\ &amp;= \\sum_{s&#39;,r} p(s&#39;, r | s, a) \\left(r + \\gamma \\max_{a&#39;} q_*(s&#39;, a&#39;)\\right) \\end{align} \\] That is, we update the estimate based on samples \\(r\\) and the estimate \\(q_*\\) in \\(s&#39;\\). This is the same approach as value iteration in DP: we update the estimates of \\(q_\\pi\\) and improve the policy in one operation. Hence Q-learning is a sample based version of value iteration in DP. 8.5 Expected SARSA - GPI using TD The expected SARSA, as SARSA, focus on the Bellman equation (8.1). SARSA generate action \\(A_{t+1}\\) from the policy \\(\\pi\\) and use the estimated action-value of \\((S_{t+1},A_{t+1})\\). However, since we know the current policy \\(\\pi\\), we might update based on the expected value instead: \\[ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\right] \\\\ \\] That is, we use a better estimate of the Bellman equation (8.1) by not sampling \\(A_{t+1}\\) but using the (deterministic) expectation over all actions instead. Doing so reduces the variance induced by selecting random actions according to an \\(\\epsilon\\)-greedy policy. As a result, given the same amount of experiences, expected SARSA generally performs better than SARSA, but has a higher computational cost. Expected SARSA is more robust to different step-size values. The incremental update formula can be written as \\[ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[T_t - Q(S_t, A_t) \\right] = (1-\\alpha)Q(S_t, A_t) + \\alpha T_t, \\] with step-size \\(\\alpha\\) and target \\(T_t\\). For SARSA the target is \\[T_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}),\\] and for expected SARSA the target is: \\[T_t = R_{t+1} + \\gamma \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a).\\] Now assume that we have run the algorithm over many time-steps so that our estimates \\(Q(S_t, A_t)\\) are close to \\(q_*(S_t, A_t)\\). Since the target in expected SARSA is deterministic (we do not sample \\(A_{t+1}\\)), the target \\(T_t \\approx Q(S_t, A_t)\\) and no matter the step-size \\(Q(S_t, A_t)\\) will be updated to the same value. On the other hand, the target in SARSA uses a sample action \\(A_{t+1}\\) that might have an action-value far from the expectation. This implies that for large step-sizes \\(Q(S_t, A_t)\\) will be updated to the target which is wrong. Hence SARSA is more sensitive to large step-sizes. Expected SARSA can be both on-policy and off-policy. If the behavioural policy and the target policy are different it is off-policy. If they are the same it is on-policy. For instance, expected SARSA is off-policy if the target policy is greedy and the behavioural policy \\(\\epsilon\\)-greedy. In which case expected SARSA becomes Q-learning since the expectation of a greedy policy is the maximum value (\\(\\pi(s|a) = 1\\) here). Hence expected SARSA can be seen as a generalisation of Q-learning that improves SARSA. 8.6 Summary Read Chapter 6.9 in Sutton and Barto (2018). 8.7 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! References "],["mod-r-setup.html", "A Setting up R", " A Setting up R R is a programming language and free software environment. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. For a further overview and description of the history of R see Chapter 2 in Peng (2018). R can be run from a terminal but in general you use an IDE (integrated development environment) RStudio for running R and to saving your work. R and RStudio can either be run from your laptop or using RStudio Cloud which run R in the cloud using your browser. During this course it is recommend to use RStudio on your laptop since there is no limit on CPU usage. If you need to install R and RStudio on your laptop check this out. If you need a brushup on R have a look at DataCamp. First, signup for a course group. Next, have a look at the free courses such as Introduction to R. DataCamp runs all the courses in your browser. That is, R is run on a server and you do not use RStudio here. References "],["groups.html", "B Working in groups", " B Working in groups During the course you have been allocated into groups. You are expected to solve the exercises and write the project report in your group. Before you start, it is a good idea to agree on a set of group rules. First, agree on a coding convention when you are going to use R. Most people in the R community use snake case but camel case is also okay. Next, setup rules on when to meet and how you will organize the work. For instance, it is a good idea that all try to solve some of the exercises before you meet and you then discuss the answers, problems etc. Finally, it is a good idea to have a common place for your code. You have different options: Use a cloud storage services such as Dropbox, OneDrive or Google Drive. Use a version control system such as Git together with GitHub. GitHub is a code sharing and publishing service and may be seen as a social networking site for programmers. If you use RStudio Cloud then one person in the group can create a shared workspace with projects: First create a new workspace named e.g. Shared. Press Members and add the group members as moderators. Now go back to Projects in the RL workspace and move one project to the shared workspace. Rename it to e.g. Group Project. Members will now have access to this project where you can share code. NOTE you can not work collectively on a file simultaneously. That is, only one member can change a file at a time! Hence it is a good idea to have your own private project to work on and use this project as a place where you can share code. If you want to download a project to your laptop then press the export button. The benefit of a cloud storage service is that it is well known to you and easy to setup. Cons are that you cannot work on the same file simultaneously. The benefit of Git and GitHub is that it manages the evolution of a set of files – called a repository – in a sane, highly structured way. If you have no idea what I’m talking about, think of it as the “Track Changes” features from Microsoft Word on steroids. Here you can work on files simultaneously. Moreover, it can be used from within RStudio. Cons are that it is harder to setup and learn. For a detailed description see Why Git? Why GitHub?. "],["coding-convention.html", "C Coding/naming convention C.1 Commenting your code", " C Coding/naming convention The main reason for using a consistent set of coding conventions is to standardize the structure and coding style of an application so that you and others can easily read and understand the code. Good coding conventions result in precise, readable, and unambiguous source code that is consistent with other language conventions and as intuitive as possible. Different ways of naming you variables exists. You are advised to adopt a naming convention; some use snake case others use camel case. The R community mostly use snake case but camel case is also okay. Choose the naming convention you like best in your study group. But stick only to one of them. A few examples: this_is_snake_case # note you do not use capital letters here thisIsCamelCase # you start each word with a capital letter (except the first) When defining variables and functions, it is in general good practice to use nouns for variables and verbs for functions. C.1 Commenting your code It is always good practice to comment your code. Such that others can get a fast overview and understand your code easier. We will use roxygen documentation comments which are widely known. #&#39; Subtract two vectors #&#39; #&#39; @param x First vector. #&#39; @param y Vector to be subtracted. #&#39; #&#39; @return The difference. #&#39; @export #&#39; #&#39; @examples #&#39; subtract(x = c(5,5), y = c(2,3)) subtract &lt;- function(x, y) { return(x-y) } You can add a roxygen skeleton automatically using Code &gt; Insert Roxygen Skeleton in RStudio. "],["annotate.html", "D Annotate the course notes", " D Annotate the course notes I recommend using hypothes.is to annotate the online course notes. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help me indicate spell errors, unclear content etc. in the notes. "],["help.html", "E Getting help", " E Getting help We all get stuck sometimes and need some help. Below are some advises on how to help yourself and ask for help: If you have a question related to the theory of RL: Ask it during the lecture or in the breaks Ask it at our course forum and we (the teacher and other students) will try to answer your question asap. If you have a question related to R: First try to understand the error message and solve the problem. You may try to debug your code by inserting browser() in your R code. Further details about debugging can be seen here. Google is your friend. This is always the next step. Try searches like “r dplyr filter”, “r tidyverse”, “r subset vector”, etc. Do you need help for a specific function in R then try ?[function-name] such as ?geom_line, ?mutate, etc. Mostly, focus on the last section with examples. Moreover, some packages may have written vignettes try browseVignettes(package = \"package_name\") to check. Have a look at Help &gt; Cheatsheets in RStudio. If you can’t find an answer then it is time to ask on-line. I recommend asking a question at stackoverflow. To make your question effective, the idea is to make things as easy as possible for someone to answer. This stack overflow thread How to make a great R reproducible example? give you some good hints. The process of providing a good minimal reproducible example (reprex) often causes you to answer your own question! See also Stack Exchange’s ‘How to ask’ and How to make a reprex at tidyverse. Ask it at our course forum and we (the teacher and other students) will try to answer your question asap. Note help using mail correspondence is not supported! "],["mod-lg-course.html", "F Learning goals", " F Learning goals The purpose of this course is to give you an introduction and knowledge about reinforcement learning (RL). After having participated in the course, you must, in addition to achieving general academic skills, demonstrate: Knowledge of RL for Bandit problems Markov decision processes and ways to optimize them the exploration vs exploitation challenge in RL and approaches for addressing this challenge the role of policy evaluation with stochastic approximation in the context of RL Skills to define the key features of RL that distinguishes it from other machine learning techniques discuss fundamental concepts in RL describe the mathematical framework of Markov decision processes formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives apply fundamental techniques, results and concepts of RL on selected RL problems. given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model) Competences to identify areas where RL are valuable select and apply the appropriate RL model for a given business problem interpret and communicate the results from RL "],["colophon.html", "G Colophon", " G Colophon These notes was written in bookdown inside RStudio. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.2.1 (2022-06-23) #&gt; os Ubuntu 20.04.5 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language (EN) #&gt; collate C.UTF-8 #&gt; ctype C.UTF-8 #&gt; tz UTC #&gt; date 2022-09-19 #&gt; pandoc 2.14.2 @ /usr/bin/ (via rmarkdown) Along with these packages: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
