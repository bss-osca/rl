[["index.html", "Reinforcement Learning for Business (RL) Course notes About the course notes Learning outcomes Purpose of the course Learning goals of the course Reinforcement learning textbook Course organization Programming software Acknowledgements and license", " Reinforcement Learning for Business (RL) Course notes Lars Relund Nielsen 2022-03-30 About the course notes This site contains course notes for the course “Reinforcement Learning for Business” held at Aarhus BSS. It consists of a set of modules containing learning path for the course modules. The course is an elective course mainly for the Operations and Supply Chain Analytics and Business Intelligence programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated. Learning outcomes By the end of this module, you are expected to: Understand the prerequisites and the goals for the course. Have downloaded the textbook. Describe what Business Analytics are. Know how the course is organized. The learning outcomes relate to the overall learning goals number 3, 5 and 6 of the course. Purpose of the course The purpose of this course is to give an introduction and knowledge about reinforcement learning (RL). RL may be seen as An approach of modelling sequential decision making problems. An approach for learning good decision making under uncertainty from experience. Mathematical models for learning-based decision making. Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions. Estimating and finding near optimal decisions of a stochastic process with sequential decision making. A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience. RL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions. A classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge. The course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control. Learning goals of the course After having participated in the course, you must, in addition to achieving general academic skills, demonstrate: Knowledge of RL for Bandit problems Markov decision processes and ways to optimize them the exploration vs exploitation challenge in RL and approaches for addressing this challenge the role of policy evaluation with stochastic approximation in the context of RL Skills to define the key features of RL that distinguishes it from other machine learning techniques discuss fundamental concepts in RL describe the mathematical framework of Markov decision processes formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives apply fundamental techniques, results and concepts of RL on selected RL problems. given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model) Competences to identify areas where RL are valuable select and apply the appropriate RL model for a given business problem interpret and communicate the results from RL Reinforcement learning textbook The course uses the free textbook Reinforcement Learning: An Introduction by Sutton and Barto (2018). The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course. Sutton and Barto’s book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it’s totally free. Just follow the citation link to download it. Course organization Each week considers a learning module. A learning module is related to a chapter in the textbook. The learning path in a typical week are Before lectures: Read the chapter in the textbook and consider the extra module material. Lectures (at campus). After lectures: Module Exercises (in groups). Lectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials are given and we focus on a specific RL problem. This module gives a short introduction to the course. Next, the site consists of different parts each containing teaching modules about specific topics: Part I gives you a general introduction to RL and the bandit problem. The appendix contains different modules that may be helpful for you including hints on how to work in groups, how to get help if you are stuck and how to annotate the course notes. Programming software We use R as programming software and it is assumed that you are familiar with using R. R is a programming language and free software environment. R can be run from a terminal but in general you use an IDE (integrated development environment) RStudio for running R and to saving your work. R and RStudio can either be run from your laptop or using RStudio Cloud which run R in the cloud using your browser. It is assumed as a prerequisite that you know how to use R. If you need a brush-up on your R programming skills then have a look at Module ??? in the appendix. Acknowledgements and license Some of the materials in this book are taken from various places The bookdown skeleton and some notes are based on the Tools for Analytics course. I would like to thank all for their inspiration. This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0. References "],["mod-rl-intro.html", "Module 1 An introduction to RL 1.1 Learning outcomes 1.2 Textbook readings 1.3 What is reinforcement learning 1.4 RL and Business Analytics 1.5 RL in different research deciplines 1.6 RL and machine learning 1.7 The RL data-stream 1.8 States, actions, rewards and policies 1.9 Exploitation vs Exploration 1.10 RL in action (Tic-Tac-Toe) 1.11 Exercises", " Module 1 An introduction to RL This module gives a short introduction to Reinforcement learning. 1.1 Learning outcomes By the end of this module, you are expected to: 1.2 Textbook readings For this week, you will need to read Chapter 1 in Sutton and Barto (2018). Read it before continuing this module. 1.3 What is reinforcement learning RL can be seen as An approach of modelling sequential decision making problems. An approach for learning good decision making under uncertainty from experience. Mathematical models for learning-based decision making. Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions. Estimating and finding near optimal decisions of a stochastic process with sequential decision making. A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience. Sequential decision problems are problems where you take decisions/actions over time. As an agent, you base your decision on the current state of the system (a state is a function of the information/data available). At the next time-step, the system have moved (stochastically) to the next stage. Here new information may be available and you receive a reward and take a new action. Examples of sequential decision problems are (with possible actions): Playing backgammon (how to move the checkers). Driving a car (left, right, forward, back, break, stop, …). How to invest/maintain a portfolio of stocks (buy, sell, amount). Control an inventory (wait, buy, amount). Vehicle routing (routes). Maintain a spare-part (wait, maintain). Robot operations (sort, move, …) Dairy cow treatment/replacement (treat, replace, …) Recommender systems e.g. Netflix recommendations (videos) Since RL involves a scalar reward signal, the goal is to choose actions such that the total reward is maximized. Note actions have an impact on the future and may have long term consequences. As such, you cannot simply choose the action that maximize the current reward. It may, in fact, be better to sacrifice immediate reward to gain more long term reward. RL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance: totally random trials (in the start), sophisticated tactics and superhuman skills (in the end). That is, as the agent learn, the reward estimate of a given action becomes better. As humans, we often learn by trial and error too: Learning to walk (by falling/pain). Learning to play (strategy is based on the game rules and what we have experienced works based on previous plays). This can also be seen as learning the reward of our actions. 1.4 RL and Business Analytics Business Analytics (BA) (or just Analytics) refers to the scientific process of transforming data into insight for making better decisions in business. BA can both be seen as the complete decision making process for solving a business problem or as a set of methodologies that enable the creation of business value. As a process it can be characterized by descriptive, predictive, and prescriptive model building using “big” data sources. Descriptive Analytics: A set of technologies and processes that use data to understand and analyze business performance. Descriptive analytics are the most commonly used and most well understood type of analytics. Descriptive analytics categorizes, characterizes, consolidates, and classifies data. Examples are standard reporting and dashboards (KPIs, what happened or is happening now?) and ad-hoc reporting (how many/often?). Descriptive analytics often serves as a first step in the successful application of predictive or prescriptive analytics. Predictive Analytics: The use of data and statistical techniques to make predictions about future outputs/outcomes, identify patterns or opportunities for business performance. Examples of techniques are data mining (what data is correlated with other data?), pattern recognition and alerts (when should I take action to correct/adjust a spare part?), Monte-Carlo simulation (what could happen?), neural networks (which customer group are best?) and forecasting (what if these trends continue?). Prescriptive Analytics: The use of optimization and other decision modelling techniques using the results of descriptive and predictive analytics to suggest decision options with the goal of improving business performance. Prescriptive analytics attempt to quantify the effect of future decisions in order to advise on possible outcomes before the decisions are actually made. Prescriptive analytics predicts not only what will happen, but also why it will happen and provides recommendations regarding actions that will take advantage of the predictions. Prescriptive analytics are relatively complex to administer, and most companies are not yet using it in their daily course of business. However, when implemented correctly, it can have a huge impact on business performance and how businesses make decisions. Examples on prescriptive analytics are optimization in production planning and scheduling, inventory management, the supply chain and transportation planning. Since reinforcement learning focus optimizing decisions it is Prescriptive Analytics. Figure 1.1: Business Analytics and competive advantage. Companies who use BA focus on fact-based management to drive decision making and treats data and information as a strategic asset that is shared within the company. This enterprise approach generates a companywide respect for applying descriptive, predictive and prescriptive analytics in areas such as supply chain, marketing and human resources. Focusing on BA gives a company a competive advantage (see Figure 1.1). BA and related areas: In the past Business Intelligence traditionally focuses on querying, reporting, online analytical processing, i.e. descriptive analytics. However, a more modern definition of Business Intelligence is the union of descriptive and predictive analytics. Operations Research or Management Science deals with the application of advanced analytical methods to help make better decisions and can hence be seen as prescriptive analytics. However, traditionally it has been taking a more theoretical approach and focusing on problem-driven research while BA takes a more data-driven approach. Logistics is a cross-functional area focusing on the effective and efficient flows of goods and services, and the related flows of information and cash. Supply Chain Management adds a process-oriented and cross-company perspective. Both can be seen as prescriptive analytics with a more problem-driven research focus. Advanced Analytics is often used as a classification of both predictive and prescriptive analytics. Data science is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured and can be seen as Business analytics applied to a wider range of data. 1.5 RL in different research deciplines RL is used in many research fields using different names: RL (most used) originated from computer science and AI. Approximate dynamic programming (ADP) is mostly used within operations research. Neuro-dynamic programming (when states are represented using a neural network). RL is closely related to Markov decision processes (a mathematical model for a sequential decision problem). Figure 1.2: Adopted from Silver (2015). 1.6 RL and machine learning Different ways of learning: Supervised learning: Given data \\((x_i, y_i)\\) learn to predict \\(y\\) from \\(x\\), i.e. find \\(y \\approx f(x)\\) (e.g. regression). Unsupervised learning: Given data \\((x_i)\\) learn patterns using \\(x\\), i.e. find \\(f(x)\\) (e.g. clustering). RL: Given state \\(x\\) you take an action and observe the reward \\(r\\) and the new state \\(x&#39;\\). There is no supervisor \\(y\\), only a reward signal \\(r\\). Your goal is to find a policy that optimize the total reward function. Figure 1.3: Adopted from Silver (2015). 1.7 The RL data-stream RL considers an agent in an environment: Agent: The one who takes the action (computer, robot, decision maker). Environment: The system/world where observations and rewards are found. Data are revealed sequentially as you take actions \\[(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \\ldots).\\] At time \\(t\\) the agent have been taken action \\(A_{t-1}\\) and observed observation \\(O_t\\) and reward \\(R_t\\): Figure 1.4: Agent-environment representation. This gives us the history at time \\(t\\) is the sequence of observations, actions and rewards \\[H_t = (O_0, A_0, R_1, O_1, \\ldots, A_{t-1}, R_t, O_t).\\] 1.8 States, actions, rewards and policies The (agent) state \\(S_t\\) is the information used to take the next action \\(A_t\\): Figure 1.5: State and action. A state depends on the history, i.e. a state is a function of the history \\(S_t = f(H_t)\\). Different strategies for defining a state may be considered. Choosing \\(S_t = H_t\\) is bad since the size of a state representation grows very fast. A better strategy is to just store the information needed for taking the next action. Moreover, it is good to have Markov states where given the present state the future is independent of the past. That is, the current state holds just as much information as the history, i.e. it holds all useful information of the history. Symbolically, we call a state \\(S_t\\) Markov iff \\[\\Pr[S_{t+1} | S_t] = \\Pr[S_{t+1} | S_1,...,S_t].\\] That is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states. Note that we can always find some Markov state. Though the smaller the state, the more “valuable” it is. In the worst case, \\(H_t\\) is Markov, since it represents all known information about itself. The reward \\(R_t\\) is a number representing the reward at time \\(t\\) (negative if a cost). Examples of rewards are Playing backgammon (0 (when play), 1 (when win), -1 (when loose)). How to invest/maintain a portfolio of stocks (the profit). Control an inventory (inventory cost, lost sales cost). Vehicle routing (transportation cost). The goal is to find a policy that maximize the total future reward. A policy is the agent’s behaviour and is a map from state to action, i.e. a function \\[a = \\pi(s)\\] saying that given the agent is in state \\(s\\) we choose action \\(a\\). The total future reward is a currently not defined clearly. Let the value function denote the future reward in state \\(s\\) and define it as the expected discounted future reward: \\[V_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots | S = s).\\] Note the value function is defined using a specific policy and the goal is to find a policy that maximize the total future reward in all possible states \\[\\pi^* = \\arg\\max_{\\pi\\in\\Pi}(V_\\pi(s)).\\] The value of the discount factor is important: Discount factor \\(\\gamma=0\\): Only care about present reward. Discount factor \\(\\gamma=1\\): Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite. Discount factor \\(\\gamma&lt;1\\): Rewards near to the present more beneficial. Note \\(V(s)\\) will converge to a number even if the time-horizon is infinite. 1.9 Exploitation vs Exploration A key problem of reinforcement learning (in general) is the difference between exploration and exploitation. Should the agent sacrifice what is currently know as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? Exploitation takes the action assumed to be optimal with respect to the data observed so far. This, gives better predictions of the value function (given the current policy) but prevents the agent from discovering potential better decisions (a better policy). Exploration does not take the action that seems to be optimal. That is, the agent explore to find new states and update the value function for this state. Examples in the exploration and exploitation dilemma are for instance movie recommendations: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration) or oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration). 1.10 RL in action (Tic-Tac-Toe) The current state of the board is represented by a row-wise concatenation of the players’ marks in a 3x3 grid. For example, the 9 character long string \"......X.O\" denotes a board state in which player X has placed a mark in the first field of the third column whereas player O has placed a mark in the third field of the third column: .table-bordered th, .table-bordered td { border: 1px solid black !important; } . . . . . . X . O That is, we index the fields row-wise: 1 2 3 4 5 6 7 8 9 The game is continued until all fields are filled or the game is over (win or loose). The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. Reward for a player is 1 for ‘win’, 0.5 for ‘draw’, and 0 for ‘loss’. These values can be seen as the probability of winning. Examples of winning, loosing and a draw from player Xs point of view: . . X . X . X O O X . X . X . O O O X X O O O X X X O Note a state can be also be represented using a state vector of length 9: stateStr &lt;- function(sV) { str &lt;- str_c(sV, collapse = &quot;&quot;) return(str) } stateVec &lt;- function(s) { sV &lt;- str_split(s, &quot;&quot;)[[1]] return(sV) } sV &lt;- stateVec(&quot;X.X.X.OOO&quot;) sV #&gt; [1] &quot;X&quot; &quot;.&quot; &quot;X&quot; &quot;.&quot; &quot;X&quot; &quot;.&quot; &quot;O&quot; &quot;O&quot; &quot;O&quot; Given a state vector, we can check if we win or loose: #&#39; Check board state #&#39; #&#39; @param pfx Player prefix (the char used on the board). #&#39; @param sV Board state vector. #&#39; @return A number 1 (win), 0 (loose) or 0.5 (draw/unknown) win &lt;- function(pfx, sV) { idx &lt;- which(sV == pfx) mineV &lt;- rep(0, 9) mineV[idx] &lt;- 1 mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE) if (any(rowSums(mineM) == 3) || # win any(colSums(mineM) == 3) || sum(diag(mineM)) == 3 || sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 3) return(1) idx &lt;- which(sV == &quot;.&quot;) mineV[idx] &lt;- 1 mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE) if (any(rowSums(mineM) == 0) || # loose any(colSums(mineM) == 0) || sum(diag(mineM)) == 0 || sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 0) return(0) return(0.5) # draw } win(&quot;O&quot;, sV) #&gt; [1] 1 win(&quot;X&quot;, sV) #&gt; [1] 0 We start with an empty board and have at most 9 moves (a player may win before). If the opponent start and a state denote the board before the opponent makes a move, then then a draw game may look as in Figure 1.6. We start with an empty board state \\(S_0\\), and the opponent makes a move, next we choose a move \\(A_0\\) (among the empty fields) and we end up in state \\(S_1\\). This continues until the game is over. Figure 1.6: A draw. 1.10.1 Players and learning to play Assume that we initially define a value \\(V(S)\\) of each state \\(S\\) to be 1 if we win, 0 if we loose and 0.5 otherwise. Most of the time we exploit our knowledge, i.e. choose the action which gives us the highest estimated reward (probability of winning). However, some times (with probability \\(\\epsilon\\)) we explore and choose another action/move than what seems optimal. These moves make us experience states we may otherwise never see. If we exploit we update the value of a state using \\[V(S_t) = V(S_t) + \\alpha(V(S_{t+1}-V(S_t)))\\] where \\(\\alpha\\) is the step-size parameter which influences the rate of learning. Let us implement a RL player using a R6 class and store the values using a hash list. We keep the hash list minimal by dynamically adding only states which has been explored or needed for calculations. Note using R6 is an object oriented approach and objects are modified by reference. The internal method move takes the previous state (from our point of view) and the current state (before we make a move) and returns the next state and update the value function (if exploit). The player explore with probability epsilon if there is not a next state that makes us win. PlayerRL &lt;- R6Class(&quot;PlayerRL&quot;, public = list( pfx = &quot;&quot;, hV = NA, control = list(epsilon = 0.2, alpha = 0.3), clearLearning = function() clear(self$hV), initialize = function(pfx = &quot;&quot;, control = list(epsilon = 0.2, alpha = 0.3)) { self$pfx &lt;- pfx self$control &lt;- control self$hV &lt;- hash() }, finalize = function() { # cat(&quot;FIN\\n&quot;) clear(self$hV) }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) state &lt;- stateStr(sP) if (!has.key(state, self$hV)) self$hV[[state]] &lt;- 0.5 keys &lt;- c() keysV &lt;- NULL for (i in idx) { # find possible moves sV[i] &lt;- self$pfx str &lt;- str_c(sV, collapse = &quot;&quot;) keys &lt;- c(keys, str) keysV &lt;- rbind(keysV, sV) sV[i] &lt;- &quot;.&quot; } # add missing states idx &lt;- which(!has.key(keys, self$hV)) if (length(idx) &gt; 0) { for (i in 1:nrow(keysV)) { self$hV[keys[i]] &lt;- win(self$pfx, keysV[i,]) } } # cat(&quot;Player&quot;, pfx, &quot;\\n&quot;) # print(self$hV) # update and find next state val &lt;- values(self$hV[keys]) # cat(&quot;Moves:&quot;); print(val) m &lt;- max(val) if (rbinom(1,1, self$control$epsilon) &gt; 0 &amp; any(val &lt; m) &amp; m &lt; 1) { # explore idx &lt;- which(val &lt; m) idx &lt;- idx[sample(length(idx), 1)] nextS &lt;- names(val)[idx] # cat(&quot;Explore - &quot;) } else { # exploit idx &lt;- which(val == m) idx &lt;- idx[sample(length(idx), 1)] nextS &lt;- names(val)[idx] # pick one self$hV[[state]] &lt;- self$hV[[state]] + self$control$alpha * (m - self$hV[[state]]) # cat(&quot;Exploit - &quot;) } # cat(&quot;Next:&quot;, nextS, &quot;\\n&quot;) return(str_split(nextS, &quot;&quot;)[[1]]) } ) ) We then can define a player using: playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0.5, alpha = 0.1)) Other players may be defined similarly, e.g. a player which moves randomly (if can not win in the next move): PlayerRandom &lt;- R6Class(&quot;PlayerRandom&quot;, public = list( pfx = NA, initialize = function(pfx) { self$pfx &lt;- pfx }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) state &lt;- stateStr(sV) keys &lt;- c() keysV &lt;- NULL for (i in idx) { # find possible moves sV[i] &lt;- self$pfx str &lt;- str_c(sV, collapse = &quot;&quot;) keys &lt;- c(keys, str) keysV &lt;- rbind(keysV, sV) sV[i] &lt;- &quot;.&quot; } # check if can win in one move for (i in 1:nrow(keysV)) { if (win(self$pfx, keysV[i,]) == 1) { return(keysV[i,]) # next state is the win state } } # else pick one random return(keysV[sample(nrow(keysV), 1),]) } ) ) A player which always place at the lowest field index: PlayerFirst &lt;- R6Class(&quot;PlayerFirst&quot;, public = list( pfx = NA, initialize = function(pfx) { self$pfx &lt;- pfx }, move = function(sP, sV) { # previous state (before opponent move) and current state (before we move) idx &lt;- which(sV == &quot;.&quot;) sV[idx[1]] &lt;- self$pfx return(sV) } ) ) 1.10.2 Gameplay We define a game which returns the prefix of the winner (NA if a draw): #&#39; @param player1 A player R6 object. This player starts the game #&#39; @param player2 A player R6 object. #&#39; @param verbose Print gameplay. #&#39; @return The winners prefix or NA if a tie. playGame &lt;- function(player1, player2, verbose = FALSE) { sP2 &lt;- rep(&quot;.&quot;, 9) # start state / game state sP1 &lt;- sP2 # state from player 1s viewpoint for (i in 1:5) { # at most 4.5 rounds ## player 1 if (verbose) cat(&quot;Player &quot;, player1$pfx, &quot;:\\n&quot;, sep=&quot;&quot;) sP1 &lt;- player1$move(sP1, sP2) # new state from player 1s viewpoint # states &lt;- c(states, stateChr(sV)) # cat(stateStr(sV), &quot; | &quot;, sep = &quot;&quot;) if (verbose) plot_board_state_cat(stateStr(sP1)) if (win(player1$pfx, sP1) == 1) { return(player1$pfx) break } if (i == 5) break # a draw ## player 2 if (verbose) cat(&quot;Player &quot;, player2$pfx, &quot;:\\n&quot;, sep=&quot;&quot;) sP2 &lt;- player2$move(sP2, sP1) # states &lt;- c(states, stateChr(sV)) # cat(stateStr(sV), &quot; | &quot;, sep = &quot;&quot;) if (verbose) plot_board_state_cat(stateStr(sP2)) if (win(player2$pfx, sP2) == 1) { return(player2$pfx) break } } return(NA) } Let us play a game between playerA and playerR: playerR &lt;- PlayerRandom$new(pfx = &quot;R&quot;) playGame(playerA, playerR, verbose = T) #&gt; Player A: #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; | A | . | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player R: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | . | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player A: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | . | #&gt; |------------------| #&gt; | . | . | . | #&gt; |------------------| #&gt; Player R: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | . | #&gt; |------------------| #&gt; | . | R | . | #&gt; |------------------| #&gt; Player A: #&gt; |------------------| #&gt; | R | . | . | #&gt; |------------------| #&gt; | A | A | A | #&gt; |------------------| #&gt; | . | R | . | #&gt; |------------------| #&gt; [1] &quot;A&quot; Note playerA has been learning when playing the game. The current estimates that are stored in the hash list are: playerA$hV #&gt; &lt;hash&gt; containing 22 key-value pair(s). #&gt; ......... : 0.5 #&gt; ........A : 0.5 #&gt; .......A. : 0.5 #&gt; ......A.. : 0.5 #&gt; .....A... : 0.5 #&gt; ....A.... : 0.5 #&gt; ...A..... : 0.5 #&gt; ..A...... : 0.5 #&gt; .A....... : 0.5 #&gt; A........ : 0.5 #&gt; R..A....A : 0.5 #&gt; R..A...A. : 0.5 #&gt; R..A..A.. : 0.5 #&gt; R..A.A... : 0.5 #&gt; R..AA.... : 0.55 #&gt; R..AA..RA : 0.5 #&gt; R..AA.AR. : 0.5 #&gt; R..AAA.R. : 1 #&gt; R.AA..... : 0.5 #&gt; R.AAA..R. : 0.5 #&gt; RA.A..... : 0.5 #&gt; RA.AA..R. : 0.5 1.10.3 Learning by a sequence of games With a single game only a few states are explored and estimates are not good. Let us instead play a sequence of games and learn along the way: #&#39; @param playerA Player A (R6 object). #&#39; @param playerB Player B (R6 object). #&#39; @param games Number of games #&#39; @param prA Probability of `playerA` starts. #&#39; @return A list with results (a data frame and a plot). playGames &lt;- function(playerA, playerB, games, prA = 0.5) { winSeq &lt;- rep(NA, games) for (g in 1:games) { # find start player if (sample(0:1, 1, prob = c(prA, 1-prA)) == 0) { player1 &lt;- playerA player2 &lt;- playerB } else { player2 &lt;- playerA player1 &lt;- playerB } winSeq[g] &lt;- playGame(player1, player2) } # process the data dat &lt;- tibble(game = 1:length(winSeq), winner = winSeq) %&gt;% mutate( players = str_c(playerA$pfx, playerB$pfx), winA := case_when( winner == playerA$pfx ~ 1, winner == playerB$pfx ~ 0, TRUE ~ 0.5 ), winsA_r = rollapply(winA, ceiling(games/10), mean, align = &quot;right&quot;, fill = NA) #, fill = 0, partial = T ) # make a plot pt &lt;- dat %&gt;% ggplot(aes(x = game, y = winA)) + geom_line(aes(y = winsA_r), size = 0.2) + geom_smooth(se = F) + labs(y = str_c(&quot;Avg. wins player &quot;, playerA$pfx), title = str_c(&quot;Wins &quot;, playerA$pfx, &quot; = &quot;, round(mean(dat$winA), 2), &quot; &quot;, playerB$pfx, &quot; = &quot;, round(1-mean(dat$winA), 2))) return(list(dat = dat, plot = pt)) } Let us now play games against a player who moves randomly using \\(\\epsilon = 0.1\\) (explore probability) and \\(\\alpha = 0.1\\) (step size). playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0.1, alpha = 0.1)) playerR &lt;- PlayerRandom$new(pfx = &quot;R&quot;) res &lt;- playGames(playerA, playerR, games = 2000) res$plot The black curve is the moving average of winning with a trend line. Note the values of the parameters have an effect on our learning: In general we do not need to explore (\\(\\epsilon = 0\\)) (the other player explore enough for us) and a high explore probability (\\(\\epsilon = 0.9\\)) make us loose. Moreover, using a high step size seems to work best. Other players may give different results. If the RL player play against a player which always move to first free field index: Here a high step size and a low exploration probability is good and the RL player will soon figure out how to win all the time. This is different if the RL player A play against another clever (RL) player B. playerA &lt;- PlayerRL$new(pfx = &quot;A&quot;, control = list(epsilon = 0, alpha = 0.1)) playerB &lt;- PlayerRL$new(pfx = &quot;B&quot;, control = list(epsilon = 0, alpha = 0.1)) If both players play using the same control parameters, one would expect that they after learning should win/loose with probability 0.5. However if there is no exploration (\\(\\epsilon = 0\\)) this is not always true: Depending on how the game starts a player may learn a better strategy and win/loose more. That is, exploration is important. Finally let us play against a player B with fixed control parameters. In general it is best to explore using the same probability otherwise you loose more and a higher step size than your opponent will make you win. 1.11 Exercises Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Some of the solutions to each exercise can be seen by pressing the button at each question. Beware, you will not learn by giving up too early. Put some effort into finding a solution! 1.11.1 Exercise - Self-Play × Solution If the exploration parameter is non-zero, the algorithm will continue to adapt until it reaches an equilibrium (either fixed or cyclical). Close Solution Consider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing? 1.11.2 Exercise - Symmetries Many tic-tac-toe positions appear different but are really the same because of symmetries. × Solution It is possible to use 4 axis of symmetry to essentially fold the board down to a quarter of the size. Close Solution How might we amend the reinforcement learning algorithm described above to take advantage of this? × Solution A smaller state space would increase the speed of learning and reduce the memory required. Close Solution In what ways would this improve the algorithm? × Solution If the opponent did not use symmetries then it could result in a worse learning. For example, if the opponent always played correct except for 1 corner, then using symmetries would mean you never take advantage of that information. That is, we should now use symmetries too since symmetrically equivalent positions do not always hold the same value in such a game. Close Solution Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value? 1.11.3 Exercise - Greedy Play × Solution As seen in Section 1.10.3 using \\(\\epsilon = 0\\) may be okay for this game if the opponent use a simple strategy (e.g. random or first index). However, in general the RL player would play worse. The chance the optimal action is the one with the current best estimate of winning is low and depending on the gameplay the RL player might win or loose. The RL player would also be unable to adapt to an opponent that slowly alter behaviour over time. Close Solution Consider Tic-Tac-Toe and suppose the RL player is only greedy (\\(\\epsilon = 0\\)), that is, always playing the move that that gives the highest probability of winning. Would it learn to play better, or worse, than a non-greedy player? What problems might occur? 1.11.4 Exercise - Learning from Exploration Consider Tic-Tac-Toe and suppose the RL player is playing against an opponent with a fixed strategy. Suppose learning updates occur after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. × Solution The probability set \\(V(s)\\) found by applying no learning from exploration is the probability of winning when using the optimal policy. The probability set \\(V(s)\\) found by applying learning from exploration is the probability of winning including the active exploration policy. Close Solution What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? × Solution The probability set found by applying no learning from exploration would result in more wins. The probability set found by applying learning from exploration is better to learn, as it reduces variance from sub-optimal future states. Close Solution Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins? 1.11.5 Exercise - Other Improvements × Solution Altering the exploration rate/learning based on the variance in the opponent's actions. If the opponent is always making the same moves and you are winning from it then using a non-zero exploration rate will make you lose you games. If the agent is able to learn how the opponent may react to certain moves, it will be easier for it to win as it can influence the opponent to make moves that leads it to a better state. Close Solution Consider Tic-Tac-Toe. Can you think of other ways to improve the reinforcement learning player? References "],["r-setup.html", "A Setting up R", " A Setting up R R is a programming language and free software environment. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. For a further overview and description of the history of R see Chapter 2 in Peng (2018). R can be run from a terminal but in general you use an IDE (integrated development environment) RStudio for running R and to saving your work. R and RStudio can either be run from your laptop or using RStudio Cloud which run R in the cloud using your browser. During this course it is recommend to use RStudio on your laptop as much as possible; however, you also need to have an RStudio Cloud account. Some pros and cons of using R in the cloud vs on the laptop are Cloud (RStudio Cloud) Pros: Log in and you are ready to use R. No need to download anything. Packages easier to install. Everything can be run using a browser. Cons: There is a limit on user time and CPU time. You need to pay if need more time. Often slower than the desktop version. Need an internet connection. Risky to use at the exam if the internet connection is slow or is down. Use the laptop version instead. Laptop (R and RStudio) Pros: Can be used without any internet connection. No limit on user time and CPU usage. Good if computations takes a lot of time. Cons: You need to install R and RStudio to get started. Packages must be installed. Other needed programs may have to be installed. Updates must be installed. If you need to install RStudio on your laptop check this out. RStudio Cloud works as your laptop version except that a workspace with projects for each module already is created. Join the Tools for Analytics workspace on RStudio Cloud (signup if you have not done it yet). Click the Projects link (in the top) and open the project 00-module-11. A personal copy of the project is loaded for you. Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object using code like x &lt;- 2 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 8 print to screen. Try to open a new file File &gt; New File &gt; New RMarkdown…. Use the defaults and press OK. Next save the file and compile it using Knit (Ctrl+Shift+K). You have now compiled a document with R code embedded. References "],["groups.html", "B Working in groups Joint R project structure", " B Working in groups During the course you have been allocated into groups. You are expected to solve the exercises and write the project report in your group. Before you start, it is a good idea to agree on a set of group rules. First, agree on a coding convention when you are going to use R. Most people in the R community use snake case but camel case is also okay. Next, setup rules on when to meet and how you will organize the work. For instance, it is a good idea that all try to solve some of the exercises before you meet and you then discuss the answers, problems etc. Finally, it is a good idea to have a common place for your code. You have different options: Use a cloud storage services such as Dropbox, OneDrive or Google Drive. Use a version control system such as Git together with GitHub. GitHub is a code sharing and publishing service and may be seen as a social networking site for programmers. If you use RStudio Cloud then one person in the group can create a shared workspace with projects: First create a new workspace named e.g. Shared. Press Members and add the group members as moderators. Now go back to Projects in the RL workspace and move one project to the shared workspace. Rename it to e.g. Group Project. Members will now have access to this project where you can share code. NOTE you can not work collectively on a file simultaneously. That is, only one member can change a file at a time! Hence it is a good idea to have your own private project to work on and use this project as a place where you can share code. If you want to download a project to your laptop then press the export button. The benefit of a cloud storage service is that it is well known to you and easy to setup. Cons are that you cannot work on the same file simultaneously. The benefit of Git and GitHub is that it manages the evolution of a set of files – called a repository – in a sane, highly structured way. If you have no idea what I’m talking about, think of it as the “Track Changes” features from Microsoft Word on steroids. Here you can work on files simultaneously. Moreover, it can be used from within RStudio. Cons are that it is harder to setup and learn. For a detailed description see Why Git? Why GitHub?. Joint R project structure I suggest to have an R project with subfolders joint, [student1 name], [student2 name], …, [student5 name]. Student folders contain files only a single student work on (good when you do some exercises before class). Folder joint contains joint work. That could for instance be a joint answer of an exercise (based on the work you did in the student folders) and a sub-folder with the project report. "],["coding-convention.html", "C Coding/naming convention C.1 Commenting your code", " C Coding/naming convention The main reason for using a consistent set of coding conventions is to standardize the structure and coding style of an application so that you and others can easily read and understand the code. Good coding conventions result in precise, readable, and unambiguous source code that is consistent with other language conventions and as intuitive as possible. Different ways of naming you variables exists. You are advised to adopt a naming convention; some use snake case others use camel case. The R community mostly use snake case but camel case is also okay. Choose the naming convention you like best in your study group. But stick only to one of them. A few examples: this_is_snake_case # note you do not use capital letters here thisIsCamelCase # you start each word with a capital letter (except the first) When defining variables and functions, it is in general good practice to use nouns for variables and verbs for functions. C.1 Commenting your code It is always good practice to comment your code. Such that others can get a fast overview and understand your code easier. We will use roxygen documentation comments which are widely known. A few examples in VBA are The top of a module file: &#39;&#39; Module description. &#39; Can be more than one line. &#39; &#39; @remarks Put your remarks on the module implementation here &#39; @author Lars Relund &lt;junk@relund.dk&gt; &#39; @date 2016-08-26 Before each sub, function etc. write: &#39;&#39; Sub description &#39; &#39; @pre Precondition &#39; @post Postcondition &#39; &#39; @param strA Explanation of input parameter strA &#39; @param intB Explanation of input parameter intB &#39; @return Return value (if a function) &#39; @remarks Further remarks Public Function MyFunc(strA As String, intB As Integer) As Integer { ... } Further tags (i.e. keywords starting with @) can be seen here. In R we use a ‘hash’ (#’) to comment functions: "],["annotate.html", "D Annotate the course notes", " D Annotate the course notes I recommend using hypothes.is to annotate the online course notes. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help me indicate spell errors, unclear content etc. in the notes. "],["help.html", "E Getting help", " E Getting help We all get stuck sometimes and need some help. Below are some advises on how to help yourself and ask for help: If you have a question related to the theory of RL: Ask it during the lecture or in the breaks Ask it at our course forum and we (the teacher and other students) will try to answer your question asap. If you have a question related to R: First try to understand the error message and solve the problem. You may try to debug your code by inserting browser() in your R code. See Chapter 11 in (wtf?) for further details. Google is your friend. This is always the next step. Try searches like “vba range”, “r dplyr filter”, “r tidyverse”, “r subset vector”, etc. Do you need help for a specific function in R then try ?[function-name] such as ?geom_line, ?mutate, etc. Mostly, focus on the last section with examples. Moreover, some packages may have written vignettes try browseVignettes(package = \"package_name\") to check. Have a look at Help &gt; Cheatsheets in RStudio. If you can’t find an answer then it is time to ask on-line. I recommend asking a question at stackoverflow. To make your question effective, the idea is to make things as easy as possible for someone to answer. This stack overflow thread How to make a great R reproducible example? give you some good hints. The process of providing a good minimal reproducible example (reprex) often causes you to answer your own question! See also Stack Exchange’s ‘How to ask’ and How to make a reprex at tidyverse. Ask it at our course forum and we (the teacher and other students) will try to answer your question asap. Note help using mail correspondence is not supported! "],["mod-lg-course.html", "F Learning goals", " F Learning goals The purpose of this course is to give you an introduction and knowledge about reinforcement learning (RL). After having participated in the course, you must, in addition to achieving general academic skills, demonstrate: Knowledge of RL for Bandit problems Markov decision processes and ways to optimize them the exploration vs exploitation challenge in RL and approaches for addressing this challenge the role of policy evaluation with stochastic approximation in the context of RL Skills to define the key features of RL that distinguishes it from other machine learning techniques discuss fundamental concepts in RL describe the mathematical framework of Markov decision processes formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives apply fundamental techniques, results and concepts of RL on selected RL problems. given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model) Competences to identify areas where RL are valuable select and apply the appropriate RL model for a given business problem interpret and communicate the results from RL "],["colophon.html", "G Colophon", " G Colophon This book was written in bookdown inside RStudio. This version of the book was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.1.3 (2022-03-10) #&gt; os Ubuntu 20.04.4 LTS #&gt; system x86_64, linux-gnu #&gt; ui X11 #&gt; language (EN) #&gt; collate C.UTF-8 #&gt; ctype C.UTF-8 #&gt; tz UTC #&gt; date 2022-03-30 #&gt; pandoc 2.14.2 @ /usr/bin/ (via rmarkdown) Along with these packages: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
