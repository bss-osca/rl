<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 5 Dynamic programming | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 5 Dynamic programming | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bss-osca.github.io/rl/" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 5 Dynamic programming | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-05-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mod-mdp-2.html"/>
<link rel="next" href="mod-r-setup.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.21/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li><a href="index.html#mod-intro">About the course notes<span></span></a>
<ul>
<li><a href="index.html#learning-outcomes">Learning outcomes<span></span></a></li>
<li><a href="index.html#purpose-of-the-course">Purpose of the course<span></span></a></li>
<li><a href="index.html#learning-goals-of-the-course">Learning goals of the course<span></span></a></li>
<li><a href="index.html#reinforcement-learning-textbook">Reinforcement learning textbook<span></span></a></li>
<li><a href="index.html#course-organization">Course organization<span></span></a></li>
<li><a href="index.html#programming-software">Programming software<span></span></a></li>
<li><a href="index.html#ack">Acknowledgements and license<span></span></a></li>
<li><a href="index.html#sec-intro-ex">Exercises<span></span></a>
<ul>
<li><a href="index.html#sec-intro-ex-annotate">Exercise - How to annotate<span></span></a></li>
<li><a href="index.html#sec-intro-ex-templates">Exercise - Templates<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#summary"><i class="fa fa-check"></i><b>1.11</b> Summary<span></span></a></li>
<li class="chapter" data-level="1.12" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.12</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.12.1</b> Exercise - Self-Play<span></span></a></li>
<li class="chapter" data-level="1.12.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---symmetries"><i class="fa fa-check"></i><b>1.12.2</b> Exercise - Symmetries<span></span></a></li>
<li class="chapter" data-level="1.12.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---greedy-play"><i class="fa fa-check"></i><b>1.12.3</b> Exercise - Greedy Play<span></span></a></li>
<li class="chapter" data-level="1.12.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---learning-from-exploration"><i class="fa fa-check"></i><b>1.12.4</b> Exercise - Learning from Exploration<span></span></a></li>
<li class="chapter" data-level="1.12.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---other-improvements"><i class="fa fa-check"></i><b>1.12.5</b> Exercise - Other Improvements<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods<span></span></b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#the-role-of-the-step-size"><i class="fa fa-check"></i><b>2.5</b> The role of the step-size<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.6</b> Optimistic initial values<span></span></a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#upper-confidence-bound-action-selection"><i class="fa fa-check"></i><b>2.7</b> Upper-Confidence Bound Action Selection<span></span></a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary<span></span></a></li>
<li class="chapter" data-level="2.9" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.9</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="mod-bandit.html"><a href="mod-bandit.html#exercise---advertising"><i class="fa fa-check"></i><b>2.9.1</b> Exercise - Advertising<span></span></a></li>
<li class="chapter" data-level="2.9.2" data-path="mod-bandit.html"><a href="mod-bandit.html#exercise---a-coin-game"><i class="fa fa-check"></i><b>2.9.2</b> Exercise - A coin game<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html"><i class="fa fa-check"></i><b>3</b> Markov decision processes (MDPs)<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#learning-outcomes-2"><i class="fa fa-check"></i><b>3.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#textbook-readings-2"><i class="fa fa-check"></i><b>3.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#an-mdp-as-a-model-for-the-agent-environment"><i class="fa fa-check"></i><b>3.3</b> An MDP as a model for the agent-environment<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#rewards-and-the-objective-function-goal"><i class="fa fa-check"></i><b>3.4</b> Rewards and the objective function (goal)<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#summary-2"><i class="fa fa-check"></i><b>3.5</b> Summary<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#sec-mdp-1-ex"><i class="fa fa-check"></i><b>3.6</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#exercise---sequential-decision-problems"><i class="fa fa-check"></i><b>3.6.1</b> Exercise - Sequential decision problems<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#exercise---expected-return"><i class="fa fa-check"></i><b>3.6.2</b> Exercise - Expected return<span></span></a></li>
<li class="chapter" data-level="3.6.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#mdp-2-ex-gambler"><i class="fa fa-check"></i><b>3.6.3</b> Exercise - Gambler’s problem<span></span></a></li>
<li class="chapter" data-level="3.6.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#mdp-1-ex-storage"><i class="fa fa-check"></i><b>3.6.4</b> Exercise - Factory storage<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html"><i class="fa fa-check"></i><b>4</b> Policies and value functions for MDPs<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#learning-outcomes-3"><i class="fa fa-check"></i><b>4.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#textbook-readings-3"><i class="fa fa-check"></i><b>4.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#policies-and-value-functions"><i class="fa fa-check"></i><b>4.3</b> Policies and value functions<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-opt"><i class="fa fa-check"></i><b>4.4</b> Optimal policies and value functions<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#optimality-vs-approximation"><i class="fa fa-check"></i><b>4.5</b> Optimality vs approximation<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#semi-mdps-non-fixed-time-length"><i class="fa fa-check"></i><b>4.6</b> Semi-MDPs (non-fixed time length)<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#summary-3"><i class="fa fa-check"></i><b>4.7</b> Summary<span></span></a></li>
<li class="chapter" data-level="4.8" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-2-ex"><i class="fa fa-check"></i><b>4.8</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#mdp-2-ex-policy"><i class="fa fa-check"></i><b>4.8.1</b> Exercise - Optimal policy<span></span></a></li>
<li class="chapter" data-level="4.8.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#mdp-2-ex-car"><i class="fa fa-check"></i><b>4.8.2</b> Exercise - Car rental<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mod-dp.html"><a href="mod-dp.html"><i class="fa fa-check"></i><b>5</b> Dynamic programming<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="mod-dp.html"><a href="mod-dp.html#learning-outcomes-4"><i class="fa fa-check"></i><b>5.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="mod-dp.html"><a href="mod-dp.html#textbook-readings-4"><i class="fa fa-check"></i><b>5.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="mod-dp.html"><a href="mod-dp.html#policy-evaluation"><i class="fa fa-check"></i><b>5.3</b> Policy evaluation<span></span></a></li>
<li class="chapter" data-level="5.4" data-path="mod-dp.html"><a href="mod-dp.html#policy-iteration"><i class="fa fa-check"></i><b>5.4</b> Policy Iteration<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="mod-dp.html"><a href="mod-dp.html#value-iteration"><i class="fa fa-check"></i><b>5.5</b> Value Iteration<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="mod-dp.html"><a href="mod-dp.html#summary-of-dp-algorithms-so-far"><i class="fa fa-check"></i><b>5.6</b> Summary of DP Algorithms (so far)<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="mod-dp.html"><a href="mod-dp.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions<span></span></a></li>
<li class="chapter" data-level="5.8" data-path="mod-dp.html"><a href="mod-dp.html#summary-4"><i class="fa fa-check"></i><b>5.8</b> Summary<span></span></a></li>
<li class="chapter" data-level="5.9" data-path="mod-dp.html"><a href="mod-dp.html#exercises"><i class="fa fa-check"></i><b>5.9</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="mod-dp.html"><a href="mod-dp.html#exercise---gamblers-problem"><i class="fa fa-check"></i><b>5.9.1</b> Exercise - Gambler’s problem<span></span></a></li>
<li class="chapter" data-level="5.9.2" data-path="mod-dp.html"><a href="mod-dp.html#exercise---car-rental"><i class="fa fa-check"></i><b>5.9.2</b> Exercise - Car rental<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R<span></span></a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups<span></span></a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code<span></span></a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes<span></span></a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help<span></span></a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals<span></span></a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon<span></span></a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-dp" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Module 5</span> Dynamic programming<a href="mod-dp.html#mod-dp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The term <em>Dynamic Programming</em> (<em>DP</em>) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the <em>principle of optimality</em>. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations.</p>
<p>DP do both <em>policy evaluation</em> (prediction) and <em>control</em>. Policy evaluation give us the value function <span class="math inline">\(v_\pi\)</span> given a policy <span class="math inline">\(\pi\)</span>. Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations.</p>
<p>Two main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions.</p>
<p>Note the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a “program”.</p>
<div id="learning-outcomes-4" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Learning outcomes<a href="mod-dp.html#learning-outcomes-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<ul>
<li>Describe the distinction between policy evaluation and control.</li>
</ul>
<!-- Explain the setting in which dynamic programming can be applied, as well as its limitations -->
<!-- Outline the iterative policy evaluation algorithm for estimating state values under a given policy -->
<!-- Apply iterative policy evaluation to compute value functions -->
<!-- Understand the policy improvement theorem -->
<!-- Use a value function for a policy to produce a better policy for a given MDP -->
<!-- Outline the policy iteration algorithm for finding the optimal policy -->
<!-- Understand “the dance of policy and value” -->
<!-- Apply policy iteration to compute optimal policies and optimal value functions -->
<!-- Understand the framework of generalized policy iteration -->
<!-- Outline value iteration, an important example of generalized policy iteration -->
<!-- Understand the distinction between synchronous and asynchronous dynamic programming methods -->
<!-- Describe brute force search as an alternative method for searching for an optimal policy -->
<!-- Describe Monte Carlo as an alternative method for learning a value function -->
<!-- Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy -->
<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings-4" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Textbook readings<a href="mod-dp.html#textbook-readings-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 4-4.7 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module. A summary of the book notation can be seen <a href="sutton-notation">here</a>.</p>
</div>
<div id="policy-evaluation" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Policy evaluation<a href="mod-dp.html#policy-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The state-value function can be represented using the Bellman equation <a href="mod-mdp-2.html#eq:bell-state">(4.2)</a>:
<span class="math display">\[\begin{equation}
v_\pi(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_\pi(s&#39;)\right).            
\end{equation}\]</span></p>
<p>If the dynamics are known perfectly, this becomes a system of <span class="math inline">\(|\mathcal{S}|\)</span> simultaneous linear equations in <span class="math inline">\(|\mathcal{S}|\)</span> unknowns <span class="math inline">\(v_\pi(s), s \in \mathcal{S}\)</span>. This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations <span class="math inline">\(v_0, v_1, v_2, \ldots\)</span>, with initial approximation <span class="math inline">\(v_0\)</span> chosen arbitrarily e.g. <span class="math inline">\(v_k(s) = 0 \: \forall s\)</span> (ensuring terminal state = 0). We can update it using the Bellman equation using (<em>a sweep</em>):</p>
<p><span class="math display">\[\begin{equation}
v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_k(s&#39;)\right) 
\end{equation}\]</span></p>
<p>We call this update an  because it is based on the expectation over all possible next states, rather than a sample of reward/value from the next state. Eventually this update will converge when <span class="math inline">\(v_k = v_\pi\)</span> after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold <span class="math inline">\(\theta\)</span> (see Figure <a href="mod-dp.html#fig:policy-eval-alg">5.1</a>). Note the algorithm uses two arrays to maintain the state-value (<span class="math inline">\(v\)</span> and <span class="math inline">\(V\)</span>). Alternatively, a single array could be used that update values in place, i.e. <span class="math inline">\(V\)</span> is used insted of <span class="math inline">\(v\)</span>. Hence, values are updated faster.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:policy-eval-alg"></span>
<img src="img/policy-evalution.png" alt="Iterative policy evaluation."  />
<p class="caption">
Figure 5.1: Iterative policy evaluation.
</p>
</div>
</div>
<div id="policy-iteration" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Policy Iteration<a href="mod-dp.html#policy-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that we have means to evaluate a policy iteratively, we can look into finding an optimal policy. In general, this is composed of two simple steps:</p>
<ol style="list-style-type: decimal">
<li>Given a policy <span class="math inline">\(\pi\)</span> (initially <span class="math inline">\(\pi_0\)</span>), estimate <span class="math inline">\(v_\pi\)</span> via the policy
evaluation algorithm (iterating a fixed number of times or until it stabilizes),
giving you
<span class="math display">\[v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]\]</span></li>
<li>Generate a new, improved policy <span class="math inline">\(\pi&#39; \geq \pi\)</span> by <em>greedily</em> picking
<span class="math display">\[\pi&#39; = \text{greedy}(v_\pi)\]</span>
Then go back to step (1) to evaluate the policy.</li>
</ol>
<p>Let’s try to understand this deeper. To do so, consider a deterministic policy
<span class="math inline">\(\pi(s) = a\)</span>. Then what we are doing in the above two steps is the following:</p>
<p><span class="math display">\[\pi&#39;(s) = \argmax_{a \in \mathcal{A}} q_\pi(s, a)\]</span></p>
<p>i.e. our new policy will, in each state, pick the action that “gives us the most
q”. Now, we can set up the following inequality:</p>
<p><span class="math display">\[
q_\pi(s, \pi&#39;(s)) = \max_{a \in \mathcal{A}} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)
\]</span></p>
<p>which proves that this greedy policy iteration strategy does indeed work, since
the return we get from starting in state <span class="math inline">\(s\)</span>, greedily choosing the locally best
action <span class="math inline">\(\argmax_{a \in \mathcal{A}} q(s, a)\)</span> and from thereon following the old
policy <span class="math inline">\(\pi\)</span>, must be at least as high as if we had chosen any particular action
<span class="math inline">\(\pi(s)\)</span> and not the optimal one (basically, the maximum of a sequence is at
least as big as any particular value of the sequence).</p>
<p>(Note: I assume the equality in <span class="math inline">\(q_\pi(s, \pi(s)) = v_\pi(s)\)</span> is meant in
expectation?)</p>
<p>What we can show now is that using this greedy strategy not only improves the
next step, but the entire value function. For this, we simply need to do some
expansions inside our definition of the state-value function as a Bellman
expectation equation:</p>
<p><span class="math display">\[
\begin{align}
  v_\pi(s) &amp;\leq q_\pi(s, \pi&#39;(s)) = \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi&#39;(S_{t+1})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma (R_{t+2} + \gamma^2 v_\pi(S_{t+2})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, \pi&#39;(S_{t+2})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...)) | S_t = s] \\
&amp;= v_{\pi&#39;}(s)
\end{align}
\]</span></p>
<p>So in total, we have <span class="math inline">\(v_\pi(s) \leq v_{\pi&#39;}(s)\)</span>. Furthermore, if at one point
the policy iteration stabilizes and we have equality in the previous equation</p>
<p><span class="math display">\[
q_\pi(s, \pi&#39;(s)) = \max_{a \in \mathcal{A}} q_\pi(s, a) = q_\pi(s, \pi(s)) = v_\pi(s)
\]</span></p>
<p>then we also have</p>
<p><span class="math display">\[\max_{a \in \mathcal{A}} q_\pi(s, a) = v_\pi(s)\]</span></p>
<p>which is precisely the Bellman optimality equation. So at this point, it holds that</p>
<p><span class="math display">\[v_\pi(s) = v_\star(s)\, \forall s \in \mathbb{S}.\]</span></p>
<p>The last question we must answer is how many steps of policy iteration we should
do to find the optimal policy? Definitely not infinitely many, since we often
notice that the value function stabilizes quite rapidly at some point. So there
are two basic methods:</p>
<ol style="list-style-type: decimal">
<li>Use <span class="math inline">\(\varepsilon\)</span>-convergence, meaning we stop when all values change less than some amount <span class="math inline">\(\varepsilon\)</span> or</li>
<li>Just use a fixed number of steps <span class="math inline">\(k\)</span> (thereby introducing another hyperparameter).</li>
</ol>
</div>
<div id="value-iteration" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Value Iteration<a href="mod-dp.html#value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The next dynamic programming method we want to consider is <em>value iteration</em>. In
this case, it is not directly our aim to improve the policy, but rather aims
directly at improving the value function (policy iteration does this as well,
but as a side effect). Basically, while policy iteration iterated on the Bellman
expectation equation, value iteration now iterates on the Bellman <em>optimality</em>
equation via the following update rule:</p>
<p><span class="math display">\[
v_\star(s) \gets \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s&#39; \in \mathcal{S}} \mathcal{P}_{ss&#39;}^a v_\star(s&#39;)
\]</span></p>
<p>or, for the step <span class="math inline">\(k \rightarrow k + 1\)</span>:</p>
<p><span class="math display">\[
v_{k+1} \gets \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s&#39; \in \mathcal{S}} \mathcal{P}_{ss&#39;}^a v_k(s&#39;)
\]</span></p>
<p>Notice how we assume we already know the solutions to the “subproblems”, i.e.
<span class="math inline">\(v_\star(s&#39;)\)</span> and then work backwards to find the best solution to the actual
problem (essence of dynamic programming). As such, practically, we can begin
with some initial estimate of the target state-value and then iteratively update
the previous state-values.</p>
<p>Note how value-iteration effectively combines one sweep of policy evaluation,
i.e. one “backup”, with one step of policy iteration (improvement), since it
performs a greedy update while also evaluating the current policy. Also, it is
important to understand that the value-iteration algorithm does not require a
policy to work. No actions have to be chosen. Rather, the q-values (rewards +
values of next states) are evaluated to update the state-values. In fact, the
last step of value-iteration is to <em>output</em> the optimal policy <span class="math inline">\(\pi^\star\)</span>:</p>
<p><span class="math display">\[
\pi^\star(s) = \argmax_a q(s, a) = \argmax_a R_s^a + \gamma\sum_{s&#39; \in \mathcal{S}} P_{ss&#39;}^a v_{\pi^\star}(s&#39;)
\]</span></p>
<p>To derive the above equation, remind yourself of the Bellman optimality equation
for the state-value function:</p>
<p><span class="math display">\[v_\star(s) = \max_a q_\star(s, a)\]</span></p>
<p>and that for the action-value function:</p>
<p><span class="math display">\[q_\star(s, a) = \mathcal{R}_s^a + \gamma\sum_{s \in \mathcal{S}} \mathcal{P}_{ss&#39;}^a v_\star(s)\]</span></p>
<p>and plug the latter into the former. Basically, at each time step we will update
the value function for a particular state to be the maximum q value.</p>
</div>
<div id="summary-of-dp-algorithms-so-far" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Summary of DP Algorithms (so far)<a href="mod-dp.html#summary-of-dp-algorithms-so-far" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At this point, we know three DP algorithms that solve two different problems. We
know the <em>policy evaluation</em>, <em>policy iteration</em> and <em>value iteration</em>
algorithms that solve the prediction (1) and control (2, 3) problems,
respectively. Let’s briefly summarize each:</p>
<ol style="list-style-type: decimal">
<li><p>The goal of <strong>policy evaluation</strong> is to determine <span class="math inline">\(v_\pi\)</span> given <span class="math inline">\(\pi\)</span>. It does so
by starting with some crude estimate <span class="math inline">\(v(s)\)</span> and iteratively evaluating <span class="math inline">\(\pi\)</span>.
<span class="math inline">\(v(s)\)</span> is updated to finally give <span class="math inline">\(v_\pi\)</span> via the <em>Bellman expectation
equation</em>:
<span class="math display">\[
\begin{align}
  v_{k+1} &amp;= \sum_{a \in \mathcal{A}} \pi(a|s) q_\pi(s, a) \\
  &amp;= \sum_{a \in \mathcal{A}}\pi(a | s)\left(R_s^a + \gamma\sum_{s \in \mathcal{S}} P_{ss&#39;}^a v_k(s&#39;)\right)
\end{align}
\]</span></p></li>
<li><p><strong>Policy iteration</strong> combines policy evaluation and the Bellman expectation
equation with updates to the policy. Instead of just updating the state values,
we also update the policy, setting the chosen action in each state to the one
with the highest q-value:</p>
<ol style="list-style-type: decimal">
<li>Policy Evaluation,</li>
<li><span class="math inline">\(\pi&#39; = \text{greedy}(v_\pi)\)</span>
Policy iteration algorithms have a time-complexity of <span class="math inline">\(O(mn^2)\)</span> for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states.</li>
</ol></li>
<li><p>Lastly, <strong>Value iteration</strong> uses the Bellman <em>optimality</em> equation to
iteratively update the value of each state to the maximum action-value
attainable from that state
<span class="math display">\[
v_{k+1} \gets \max_{a \in \mathcal{A}} q_\pi(s, a) = \max_{a \in \mathcal{A}} \mathcal{R}_s^a + \gamma\sum_{s&#39; \in \mathcal{S}} \mathcal{P}_{ss&#39;}^a v_k(s&#39;)
\]</span>
and finally also outputs the optimal policy: <span class="math inline">\(\pi^\star(s) = \argmax_a q(s, a)\)</span>
Value iteration algorithms have a time-complexity of <span class="math inline">\(O(m^2n^2)\)</span> for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states.</p></li>
</ol>
</div>
<div id="extensions" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Extensions<a href="mod-dp.html#extensions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One extension to dynamic programming as we have discussed it above is
<em>asynchronous</em> DP, where states are updated individually, in any order. This can
significantly improve computation.</p>
<p>The first way to achieve more asynchronous DP is to use <em>in-place DP</em>.
Basically, instead of keeping a copy of the old and new value function in each
value-iteration update, you can just update the value functions in-place. The
thing to note here is that asynchronous updates in other parts of the
state-space will directly be affected by this. However, the point is that this
is not actually a bad idea.</p>
<p>An extension of this is <em>prioritized sweeping</em>. The idea here is to keep track
of how “effective” or “significant” updates to our state-values are. States
where the updates are more significant are likely further away from converging
to the optimal value. As such, we’d like to update them first. For this, we
would compute this significance, called the <em>Bellman error</em>:</p>
<p><span class="math display">\[|v_{k+1}(s) - v_k(s)|\]</span></p>
<p>and keep these values in a priority queue. You can then efficiently pop the top
of it to always get the state you should update next.</p>
<p>An additional improvement is to do <em>prioritize local updates</em>. The idea is that
if your robot is in a particular region of the grid, it is much more important
to update nearby states than faraway ones.</p>
<p>Lastly, in very high dimensional spaces and problems with high branching factor,
it makes sense to sample actions (branches).</p>
</div>
<div id="summary-4" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Summary<a href="mod-dp.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Read Chapter 4.8 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="exercises" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Exercises<a href="mod-dp.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Some of the solutions to each exercise can be seen by pressing the button at each question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="exercise---gamblers-problem" class="section level3 hasAnchor" number="5.9.1">
<h3><span class="header-section-number">5.9.1</span> Exercise - Gambler’s problem<a href="mod-dp.html#exercise---gamblers-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the gambler’s problem in Exercise <a href="#mdp-1-ex-gambler"><strong>??</strong></a>.</p>
<ol style="list-style-type: decimal">
<li>Solve the problem using …</li>
</ol>
</div>
<div id="exercise---car-rental" class="section level3 hasAnchor" number="5.9.2">
<h3><span class="header-section-number">5.9.2</span> Exercise - Car rental<a href="mod-dp.html#exercise---car-rental" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the car rental problem in Exercise <a href="mod-mdp-2.html#mdp-2-ex-car">4.8.2</a> with inventory dynamics:
<span class="math display">\[X = \min(20, \max(0, x&#39; - a - D_1) + H_1))),\]</span>
and
<span class="math display">\[Y = \min(20, \max(0, y&#39; + a - D_2) + H_2))),\]</span>
for Location 1 and 2, respectively. The transition probabilities can be split due to independence: <span class="math display">\[ p((x,y) | (x&#39;,y&#39;), a) = p(x | x&#39;, a) p(y | y&#39;, a) \]</span></p>
<!-- $$ p((x,y) | (x',y'), a) = p(x | x', a) p(y | y', a) = \Pr(x = x' + n_x - a)\Pr(y = y' + n_y + a) = \Pr(n_x = x - x' + a)\Pr(n_y = y - y' - a) = \phi(x - x' + a)\phi(y - y' - a) $$ -->
<!-- For location 1: -->
<!-- $$ -->
<!-- p(x | x', a) = \Pr(x = \min(20, \max(0, x' - a - D_1) + H_1))) = -->
<!-- \begin{cases} -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 = x) & x < 20\\ -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 \geq 20) & x = 20 -->
<!-- \end{cases} -->
<!-- $$ -->
<ol style="list-style-type: decimal">
<li>Solve the problem using …</li>
</ol>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-mdp-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-r-setup.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/05_dp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
