<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 5 Dynamic programming | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 5 Dynamic programming | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 5 Dynamic programming | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-08-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mod-mdp-2.html"/>
<link rel="next" href="mod-mc.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.23/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the course notes</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#purpose-of-the-course"><i class="fa fa-check"></i>Purpose of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-goals-of-the-course"><i class="fa fa-check"></i>Learning goals of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reinforcement-learning-textbook"><i class="fa fa-check"></i>Reinforcement learning textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-organization"><i class="fa fa-check"></i>Course organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programming-software"><i class="fa fa-check"></i>Programming software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ack"><i class="fa fa-check"></i>Acknowledgements and license</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex"><i class="fa fa-check"></i>Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-annotate"><i class="fa fa-check"></i>Exercise - How to annotate</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-templates"><i class="fa fa-check"></i>Exercise - Templates</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL</b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings</a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning</a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics</a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines</a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning</a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream</a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies</a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration</a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play</a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay</a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#summary"><i class="fa fa-check"></i><b>1.11</b> Summary</a></li>
<li class="chapter" data-level="1.12" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.12</b> Exercises</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.12.1</b> Exercise - Self-Play</a></li>
<li class="chapter" data-level="1.12.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-sym"><i class="fa fa-check"></i><b>1.12.2</b> Exercise - Symmetries</a></li>
<li class="chapter" data-level="1.12.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-greedy"><i class="fa fa-check"></i><b>1.12.3</b> Exercise - Greedy Play</a></li>
<li class="chapter" data-level="1.12.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-exploit"><i class="fa fa-check"></i><b>1.12.4</b> Exercise - Learning from Exploration</a></li>
<li class="chapter" data-level="1.12.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-other"><i class="fa fa-check"></i><b>1.12.5</b> Exercise - Other Improvements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods</b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings</a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem</a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action</a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-step-size"><i class="fa fa-check"></i><b>2.5</b> The role of the step-size</a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.6</b> Optimistic initial values</a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#upper-confidence-bound-action-selection"><i class="fa fa-check"></i><b>2.7</b> Upper-Confidence Bound Action Selection</a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
<li class="chapter" data-level="2.9" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-adv"><i class="fa fa-check"></i><b>2.9.1</b> Exercise - Advertising</a></li>
<li class="chapter" data-level="2.9.2" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-coin"><i class="fa fa-check"></i><b>2.9.2</b> Exercise - A coin game</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html"><i class="fa fa-check"></i><b>3</b> Markov decision processes (MDPs)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#learning-outcomes-2"><i class="fa fa-check"></i><b>3.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#textbook-readings-2"><i class="fa fa-check"></i><b>3.2</b> Textbook readings</a></li>
<li class="chapter" data-level="3.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#an-mdp-as-a-model-for-the-agent-environment"><i class="fa fa-check"></i><b>3.3</b> An MDP as a model for the agent-environment</a></li>
<li class="chapter" data-level="3.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#rewards-and-the-objective-function-goal"><i class="fa fa-check"></i><b>3.4</b> Rewards and the objective function (goal)</a></li>
<li class="chapter" data-level="3.5" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#summary-2"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#sec-mdp-1-ex"><i class="fa fa-check"></i><b>3.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-seq"><i class="fa fa-check"></i><b>3.6.1</b> Exercise - Sequential decision problems</a></li>
<li class="chapter" data-level="3.6.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-exp-return"><i class="fa fa-check"></i><b>3.6.2</b> Exercise - Expected return</a></li>
<li class="chapter" data-level="3.6.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-gambler"><i class="fa fa-check"></i><b>3.6.3</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="3.6.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-storage"><i class="fa fa-check"></i><b>3.6.4</b> Exercise - Factory storage</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html"><i class="fa fa-check"></i><b>4</b> Policies and value functions for MDPs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#learning-outcomes-3"><i class="fa fa-check"></i><b>4.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="4.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#textbook-readings-3"><i class="fa fa-check"></i><b>4.2</b> Textbook readings</a></li>
<li class="chapter" data-level="4.3" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#policies-and-value-functions"><i class="fa fa-check"></i><b>4.3</b> Policies and value functions</a></li>
<li class="chapter" data-level="4.4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-opt"><i class="fa fa-check"></i><b>4.4</b> Optimal policies and value functions</a></li>
<li class="chapter" data-level="4.5" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#optimality-vs-approximation"><i class="fa fa-check"></i><b>4.5</b> Optimality vs approximation</a></li>
<li class="chapter" data-level="4.6" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#semi-mdps-non-fixed-time-length"><i class="fa fa-check"></i><b>4.6</b> Semi-MDPs (non-fixed time length)</a></li>
<li class="chapter" data-level="4.7" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#summary-3"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
<li class="chapter" data-level="4.8" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-2-ex"><i class="fa fa-check"></i><b>4.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-policy"><i class="fa fa-check"></i><b>4.8.1</b> Exercise - Optimal policy</a></li>
<li class="chapter" data-level="4.8.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-car"><i class="fa fa-check"></i><b>4.8.2</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mod-dp.html"><a href="mod-dp.html"><i class="fa fa-check"></i><b>5</b> Dynamic programming</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mod-dp.html"><a href="mod-dp.html#learning-outcomes-4"><i class="fa fa-check"></i><b>5.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="5.2" data-path="mod-dp.html"><a href="mod-dp.html#textbook-readings-4"><i class="fa fa-check"></i><b>5.2</b> Textbook readings</a></li>
<li class="chapter" data-level="5.3" data-path="mod-dp.html"><a href="mod-dp.html#sec-dp-pe"><i class="fa fa-check"></i><b>5.3</b> Policy evaluation</a></li>
<li class="chapter" data-level="5.4" data-path="mod-dp.html"><a href="mod-dp.html#policy-improvement"><i class="fa fa-check"></i><b>5.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="5.5" data-path="mod-dp.html"><a href="mod-dp.html#policy-iteration"><i class="fa fa-check"></i><b>5.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="5.6" data-path="mod-dp.html"><a href="mod-dp.html#value-iteration"><i class="fa fa-check"></i><b>5.6</b> Value Iteration</a></li>
<li class="chapter" data-level="5.7" data-path="mod-dp.html"><a href="mod-dp.html#generalized-policy-iteration"><i class="fa fa-check"></i><b>5.7</b> Generalized policy iteration</a></li>
<li class="chapter" data-level="5.8" data-path="mod-dp.html"><a href="mod-dp.html#summary-4"><i class="fa fa-check"></i><b>5.8</b> Summary</a></li>
<li class="chapter" data-level="5.9" data-path="mod-dp.html"><a href="mod-dp.html#exercises"><i class="fa fa-check"></i><b>5.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-gambler"><i class="fa fa-check"></i><b>5.9.1</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="5.9.2" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-rental"><i class="fa fa-check"></i><b>5.9.2</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mod-mc.html"><a href="mod-mc.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo methods for prediction and control</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mod-mc.html"><a href="mod-mc.html#learning-outcomes-5"><i class="fa fa-check"></i><b>6.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="mod-mc.html"><a href="mod-mc.html#textbook-readings-5"><i class="fa fa-check"></i><b>6.2</b> Textbook readings</a></li>
<li class="chapter" data-level="6.3" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-evaluation"><i class="fa fa-check"></i><b>6.3</b> MC prediction (evaluation)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-of-action-values"><i class="fa fa-check"></i><b>6.3.1</b> MC prediction of action-values</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="mod-mc.html"><a href="mod-mc.html#mc-control-improvement"><i class="fa fa-check"></i><b>6.4</b> MC control (improvement)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mod-mc.html"><a href="mod-mc.html#gpi-with-exploring-starts"><i class="fa fa-check"></i><b>6.4.1</b> GPI with exploring starts</a></li>
<li class="chapter" data-level="6.4.2" data-path="mod-mc.html"><a href="mod-mc.html#gpi-using-epsilon-soft-policies"><i class="fa fa-check"></i><b>6.4.2</b> GPI using <span class="math inline">\(\epsilon\)</span>-soft policies</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mod-mc.html"><a href="mod-mc.html#sec-mc-off-policy"><i class="fa fa-check"></i><b>6.5</b> Off-policy MC prediction</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="mod-mc.html"><a href="mod-mc.html#weighted-importance-sampling"><i class="fa fa-check"></i><b>6.5.1</b> Weighted importance sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="mod-mc.html"><a href="mod-mc.html#off-policy-control-improvement"><i class="fa fa-check"></i><b>6.6</b> Off-policy control (improvement)</a></li>
<li class="chapter" data-level="6.7" data-path="mod-mc.html"><a href="mod-mc.html#summary-5"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
<li class="chapter" data-level="6.8" data-path="mod-mc.html"><a href="mod-mc.html#exercises-1"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mod-td-pred.html"><a href="mod-td-pred.html"><i class="fa fa-check"></i><b>7</b> Temporal difference methods for prediction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#learning-outcomes-6"><i class="fa fa-check"></i><b>7.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="7.2" data-path="mod-td-pred.html"><a href="mod-td-pred.html#textbook-readings-6"><i class="fa fa-check"></i><b>7.2</b> Textbook readings</a></li>
<li class="chapter" data-level="7.3" data-path="mod-td-pred.html"><a href="mod-td-pred.html#what-is-td-learning"><i class="fa fa-check"></i><b>7.3</b> What is TD learning?</a></li>
<li class="chapter" data-level="7.4" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction"><i class="fa fa-check"></i><b>7.4</b> TD prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction-for-action-values"><i class="fa fa-check"></i><b>7.4.1</b> TD prediction for action-values</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mod-td-pred.html"><a href="mod-td-pred.html#benefits-of-td-methods"><i class="fa fa-check"></i><b>7.5</b> Benefits of TD methods</a></li>
<li class="chapter" data-level="7.6" data-path="mod-td-pred.html"><a href="mod-td-pred.html#exercises-2"><i class="fa fa-check"></i><b>7.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#ex-td-pred-random"><i class="fa fa-check"></i><b>7.6.1</b> Exercise - A randow walk</a></li>
<li class="chapter" data-level="7.6.2" data-path="mod-td-pred.html"><a href="mod-td-pred.html#ex-td-pred-off-policy"><i class="fa fa-check"></i><b>7.6.2</b> Exercise - Off-policy TD</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mod-td-control.html"><a href="mod-td-control.html"><i class="fa fa-check"></i><b>8</b> Temporal difference methods for control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mod-td-control.html"><a href="mod-td-control.html#learning-outcomes-7"><i class="fa fa-check"></i><b>8.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.2" data-path="mod-td-control.html"><a href="mod-td-control.html#textbook-readings-7"><i class="fa fa-check"></i><b>8.2</b> Textbook readings</a></li>
<li class="chapter" data-level="8.3" data-path="mod-td-control.html"><a href="mod-td-control.html#sarsa---on-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.3</b> SARSA - On-policy GPI using TD</a></li>
<li class="chapter" data-level="8.4" data-path="mod-td-control.html"><a href="mod-td-control.html#q-learning---off-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.4</b> Q-learning - Off-policy GPI using TD</a></li>
<li class="chapter" data-level="8.5" data-path="mod-td-control.html"><a href="mod-td-control.html#expected-sarsa---gpi-using-td"><i class="fa fa-check"></i><b>8.5</b> Expected SARSA - GPI using TD</a></li>
<li class="chapter" data-level="8.6" data-path="mod-td-control.html"><a href="mod-td-control.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="mod-td-control.html"><a href="mod-td-control.html#exercises-3"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R</a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups</a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention</a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes</a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help</a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals</a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon</a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-dp" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Module 5</span> Dynamic programming<a href="mod-dp.html#mod-dp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The term <em>Dynamic Programming</em> (<em>DP</em>) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the <em>principle of optimality</em>. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations.</p>
<p>DP do both <em>policy evaluation</em> (prediction) and <em>control</em>. Policy evaluation give us the value function <span class="math inline">\(v_\pi\)</span> given a policy <span class="math inline">\(\pi\)</span>. Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations.</p>
<p>Two main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions.</p>
<p>Note the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a “program”.</p>
<div id="learning-outcomes-4" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Learning outcomes<a href="mod-dp.html#learning-outcomes-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<ul>
<li>Describe the distinction between policy evaluation and control.</li>
<li>Identify when DP can be applied, as well as its limitations.</li>
<li>Explain and apply iterative policy evaluation for estimating state-values given a policy.</li>
<li>Interpret the policy improvement theorem.</li>
<li>Explain and apply policy iteration for finding an optimal policy.</li>
<li>Explain and apply value iteration for finding an optimal policy.</li>
<li>Describe the ideas behind generalized policy iteration.</li>
<li>Interpret the distinction between synchronous and asynchronous dynamic programming methods.</li>
</ul>
<p>The learning outcomes relate to the <a href="mod-lg-course.html#mod-lg-course">overall learning goals</a> number 2, 4, 6, 7, 8, 10 and 12 of the course.</p>
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings-4" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Textbook readings<a href="mod-dp.html#textbook-readings-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 4-4.7 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module. A summary of the book notation can be seen <a href="https://bss-osca.github.io/rl/sutton-notation.pdf">here</a>.</p>
<div>
Slides for this module can be seen
<a href="https://bss-osca.github.io/rl/slides/05_dp-slides.html" target="_blank">here.</a>
You do not have to look at them before the lecture!
</div>
</div>
<div id="sec-dp-pe" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Policy evaluation<a href="mod-dp.html#sec-dp-pe" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The state-value function can be represented using the Bellman equation <a href="mod-mdp-2.html#eq:bell-state">(4.2)</a>:
<span class="math display" id="eq:bm-pol-eval">\[
v_\pi(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_\pi(s&#39;)\right).            
\tag{5.1}
\]</span></p>
<p>If the dynamics are known perfectly, this becomes a system of <span class="math inline">\(|\mathcal{S}|\)</span> simultaneous linear equations in <span class="math inline">\(|\mathcal{S}|\)</span> unknowns <span class="math inline">\(v_\pi(s), s \in \mathcal{S}\)</span>. This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations <span class="math inline">\(v_0, v_1, v_2, \ldots\)</span>, with initial approximation <span class="math inline">\(v_0\)</span> chosen arbitrarily e.g. <span class="math inline">\(v_0(s) = 0 \: \forall s\)</span> (ensuring terminal state = 0). We can use <em>a sweep</em> with the Bellman equation to update the values:</p>
<p><span class="math display">\[\begin{equation}
v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_k(s&#39;)\right) 
\end{equation}\]</span></p>
<p>We call this update an <em>expected update</em> because it is based on the expectation over all possible next states, rather than a sample of reward from the next state. This update will converge to <span class="math inline">\(v_\pi\)</span> after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold <span class="math inline">\(\theta\)</span> (see Figure <a href="mod-dp.html#fig:policy-eval-alg">5.1</a>). Note the algorithm uses two arrays to maintain the state-value (<span class="math inline">\(v\)</span> and <span class="math inline">\(V\)</span>). Alternatively, a single array could be used that update values in place, i.e. <span class="math inline">\(V\)</span> is used in place of <span class="math inline">\(v\)</span>. Hence, state-values are updated faster.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:policy-eval-alg"></span>
<img src="img/policy-evalution.png" alt="Iterative policy evaluation [@Sutton18]."  />
<p class="caption">
Figure 5.1: Iterative policy evaluation <span class="citation">(<a href="#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</p>
</div>
</div>
<div id="policy-improvement" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Policy Improvement<a href="mod-dp.html#policy-improvement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From the Bellman optimality equation <a href="mod-mdp-2.html#eq:bell-opt-state">(4.4)</a> we have</p>
<p><span class="math display" id="eq:pi-det">\[
\begin{align}
\pi_*(s) &amp;= \arg\max_{a \in \mathcal{A}} q_*(s, a) \\
         &amp;= \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)\right).
\end{align}
\tag{5.2}
\]</span>
That is, a deterministic optimal policy can be found by choosing <em>greedy</em> the best action given the optimal value function. If we apply this greed action selection to the value function for a policy <span class="math inline">\(\pi\)</span> and pick the action with most <span class="math inline">\(q\)</span>:
<span class="math display" id="eq:pi-mark-det">\[
\begin{align}
\pi&#39;(s) &amp;= \arg\max_{a \in \mathcal{A}} q_\pi(s, a) \\
         &amp;= \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_\pi(s&#39;)\right),
\end{align}
\tag{5.3}
\]</span>
then
<span class="math display">\[
q_\pi(s, \pi&#39;(s)) \geq q_\pi(s, \pi(s)) = v_\pi(s) \quad \forall s \in \mathcal{S}.
\]</span>
Note if <span class="math inline">\(\pi&#39;(s) = \pi(s), \forall s\in\mathcal{S}\)</span> then the Bellman optimality equation <a href="mod-mdp-2.html#eq:bell-opt-state">(4.4)</a> holds and <span class="math inline">\(\pi\)</span> must be optimal; Otherwise,
<span class="math display">\[
\begin{align}
  v_\pi(s) &amp;\leq q_\pi(s, \pi&#39;(s)) = \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi&#39;(S_{t+1})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma (R_{t+2} + \gamma^2 v_\pi(S_{t+2})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, \pi&#39;(S_{t+2})) | S_t = s] \\
&amp;\leq \mathbb{E}_{\pi&#39;}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...)) | S_t = s] \\
&amp;= v_{\pi&#39;}(s),
\end{align}
\]</span>
That is, policy <span class="math inline">\(\pi&#39;\)</span> is strictly better than policy <span class="math inline">\(\pi\)</span> since there is at least one state <span class="math inline">\(s\)</span> for which <span class="math inline">\(v_{\pi&#39;}(s) &gt; v_\pi(s)\)</span>. We can formalize the above deductions in a theorem.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-1" class="theorem"><strong>Theorem 5.1  (Policy improvement theorem) </strong></span>Let <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\pi&#39;\)</span> be any pair of deterministic policies, such that
<span class="math display">\[\begin{equation}
    q_\pi(s, \pi&#39;(s)) \geq v_\pi(s) \quad \forall s \in \mathcal{S}.
\end{equation}\]</span>
That is, <span class="math inline">\(\pi&#39;\)</span> is as least as good as <span class="math inline">\(\pi\)</span>.</p>
</div>
</div>
<div id="policy-iteration" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Policy Iteration<a href="mod-dp.html#policy-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given the policy improvement theorem we can now improve policies iteratively until we find an optimal policy:</p>
<ol style="list-style-type: decimal">
<li>Pick an arbitrary initial policy <span class="math inline">\(\pi\)</span>.</li>
<li>Given a policy <span class="math inline">\(\pi\)</span>, estimate <span class="math inline">\(v_\pi(s)\)</span> via the policy evaluation algorithm.</li>
<li>Generate a new, improved policy <span class="math inline">\(\pi&#39; \geq \pi\)</span> by <em>greedily</em> picking <span class="math inline">\(\pi&#39; = \text{greedy}(v_\pi)\)</span> using Eq. <a href="mod-dp.html#eq:pi-mark-det">(5.3)</a>. If <span class="math inline">\(\pi&#39;=\pi\)</span> then stop (<span class="math inline">\(\pi_*\)</span> has been found); otherwise go to Step 2.</li>
</ol>
<p>The algorithm is given in Figure <a href="mod-dp.html#fig:policy-ite-alg">5.2</a>. The sequence of calculations will be:
<span class="math display">\[\pi_0 \xrightarrow[]{E} v_{\pi_0} \xrightarrow[]{I} \pi_1 \xrightarrow[]{E} v_{\pi_1} \xrightarrow[]{I} \pi_2 \xrightarrow[]{E} v_{\pi_2}  \ldots \xrightarrow[]{I} \pi_* \xrightarrow[]{E} v_{*}\]</span>
The number of steps of policy iteration needed to find the optimal policy are often low.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:policy-ite-alg"></span>
<img src="img/policy-iteration.png" alt="Policy iteration [@Sutton18]." width="70%" />
<p class="caption">
Figure 5.2: Policy iteration <span class="citation">(<a href="#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</p>
</div>
</div>
<div id="value-iteration" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Value Iteration<a href="mod-dp.html#value-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Policy iteration requires full policy evaluation at each iteration step. This could be an computationally expensive process which requires may sweeps of the state space. In <em>value iteration</em>, the policy evaluation is stopped after one sweep of the state space. Value iteration is achieved by turning the Bellman optimality equation into an update rule:
<span class="math display">\[
v_{k+1}(s) = \max_a \left(r(s,a) + \gamma\sum_{s&#39;} p(s&#39;|s, a)v_k(s&#39;)\right)
\]</span>
Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement, since it performs a greedy update while also evaluating the current policy. Also, it is important to understand that the value-iteration algorithm does not require a policy to work. No actions have to be chosen. Rather, the state-values are updated and after the last step of value-iteration the optimal policy <span class="math inline">\(\pi_*\)</span> is found:</p>
<span class="math display">\[
\pi_*(s) = \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)\right),
\]</span>
The algorithm is given in Figure <a href="mod-dp.html#fig:value-ite-alg">5.3</a>. Since we do not want an infinite number of iterations we introduce a threshold <span class="math inline">\(\theta\)</span>. The sequence of calculations will be (where G denotes greedy action selection):
<span class="math display">\[v_{0} \xrightarrow[]{EI} v_{1} \xrightarrow[]{EI} v_{2}  \ldots \xrightarrow[]{EI} v_{*} \xrightarrow[]{G} \pi_{*}\]</span>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:value-ite-alg"></span>
<img src="img/value-iteration.png" alt="Value iteration [@Sutton18]." width="70%" />
<p class="caption">
Figure 5.3: Value iteration <span class="citation">(<a href="#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</p>
</div>
</div>
<div id="generalized-policy-iteration" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Generalized policy iteration<a href="mod-dp.html#generalized-policy-iteration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Generalised Policy Iteration (GPI) is the process of letting policy evaluation and policy improvement interact, independent of granularity. For instance, improvement/evaluation can be performed by doing complete sweeps of the state space (policy iteration), or improve the state-value using a single sweep of the state space (value iteration). GPI can also do <em>asynchronous</em> updates of the state-value where states are updated individually, in any order. This can
significantly improve computation. Examples on asynchronous DP are</p>
<ul>
<li><p><em>In-place DP</em> mentioned in Section <a href="mod-dp.html#sec-dp-pe">5.3</a> where instead of keeping a copy of the old and new value function in each value-iteration update, you can just update the value functions in-place. Hence asynchronous updates in other parts of the state-space will directly be affected resulting in faster updates.</p></li>
<li><p><em>Prioritized sweeping</em> where we keep track of how “effective” or “significant” updates to our state-values are. States where the updates are more significant are likely further away from converging to the optimal value. As such, we would like to update them first. For this, we would compute the <em>Bellman error</em>:
<span class="math display">\[|v_{k+1}(s) - v_k(s)|,\]</span>
and keep these values in a priority queue. You can then efficiently pop the top of it to always get the state you should update next.</p></li>
<li><p><em>Prioritize local updates</em> where you update nearby states given the current state, e.g. if your robot is in a particular region of the grid, it is much more important to update nearby states than faraway ones.</p></li>
</ul>
<p>GPI works and will convergence to the optimal policy and optimal value function if the states are visited (in theory) an infinite number of times. That is, you must explore the whole state space for GPI to work.</p>
</div>
<div id="summary-4" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Summary<a href="mod-dp.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Read Chapter 4.8 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="exercises" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Exercises<a href="mod-dp.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="ex-dp-gambler" class="section level3 hasAnchor" number="5.9.1">
<h3><span class="header-section-number">5.9.1</span> Exercise - Gambler’s problem<a href="mod-dp.html#ex-dp-gambler" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the gambler’s problem in Exercise <a href="#ex-mdp-2-gambler"><strong>??</strong></a>.</p>
<ol style="list-style-type: decimal">
<li>Solve the problem using …</li>
</ol>
</div>
<div id="ex-dp-rental" class="section level3 hasAnchor" number="5.9.2">
<h3><span class="header-section-number">5.9.2</span> Exercise - Car rental<a href="mod-dp.html#ex-dp-rental" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the car rental problem in Exercise <a href="mod-mdp-2.html#ex-mdp-2-car">4.8.2</a> with inventory dynamics:
<span class="math display">\[X = \min(20, \max(0, x&#39; - a - D_1) + H_1))),\]</span>
and
<span class="math display">\[Y = \min(20, \max(0, y&#39; + a - D_2) + H_2))),\]</span>
for Location 1 and 2, respectively. The transition probabilities can be split due to independence: <span class="math display">\[ p((x,y) | (x&#39;,y&#39;), a) = p(x | x&#39;, a) p(y | y&#39;, a) \]</span></p>
<!-- $$ p((x,y) | (x',y'), a) = p(x | x', a) p(y | y', a) = \Pr(x = x' + n_x - a)\Pr(y = y' + n_y + a) = \Pr(n_x = x - x' + a)\Pr(n_y = y - y' - a) = \phi(x - x' + a)\phi(y - y' - a) $$ -->
<!-- For location 1: -->
<!-- $$ -->
<!-- p(x | x', a) = \Pr(x = \min(20, \max(0, x' - a - D_1) + H_1))) = -->
<!-- \begin{cases} -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 = x) & x < 20\\ -->
<!-- \Pr(\max(0, x' - a - D_1) + H_1 \geq 20) & x = 20 -->
<!-- \end{cases} -->
<!-- $$ -->
<ol style="list-style-type: decimal">
<li>Solve the problem using …</li>
</ol>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-mdp-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-mc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/05_dp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
