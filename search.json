[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "",
    "text": "1 About the course notes\nThis site contains course notes for the course “Reinforcement Learning for Business” held at Aarhus BSS. It consists of a set of learning modules. The course is an elective course mainly for the Operations and Supply Chain Analytics and Business Intelligence programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated.\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.1 Learning outcomes",
    "text": "1.1 Learning outcomes\nBy the end of this module, you are expected to:\n\nUnderstand the prerequisites and the goals for the course.\nHave downloaded the textbook.\nKnow how the course is organized. \nHave annotated the online notes.\n\nThe learning outcomes relate to the overall learning goals number 3, 5 and 6 of the course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#purpose-of-the-course",
    "href": "index.html#purpose-of-the-course",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.2 Purpose of the course",
    "text": "1.2 Purpose of the course\nThe purpose of this course is to give an introduction and knowledge about reinforcement learning (RL).\nRL may be seen as\n\nAn approach of modelling sequential decision making problems.\nAn approach for learning good decision making under uncertainty from experience.\nMathematical models for learning-based decision making.\nTrying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.\nEstimating and finding near optimal decisions of a stochastic process with sequential decision making.\nA model where, given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.\n\nRL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions.\nA classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge.\nThe course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#sec-lg-course",
    "href": "index.html#sec-lg-course",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.3 Learning goals of the course",
    "text": "1.3 Learning goals of the course\nAfter having participated in the course, you must, in addition to achieving general academic skills, demonstrate:\nKnowledge of\n\nRL for Bandit problems\nMarkov decision processes and ways to optimize them\nthe exploration vs exploitation challenge in RL and approaches for addressing this challenge\nthe role of policy evaluation with stochastic approximation in the context of RL\n\nSkills to\n\ndefine the key features of RL that distinguishes it from other machine learning techniques\ndiscuss fundamental concepts in RL\ndescribe the mathematical framework of Markov decision processes\nformulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives\napply fundamental techniques, results and concepts of RL on selected RL problems.\ngiven an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model)\n\nCompetences to\n\nidentify areas where RL are valuable\nselect and apply the appropriate RL model for a given business problem\ninterpret and communicate the results from RL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#reinforcement-learning-textbook",
    "href": "index.html#reinforcement-learning-textbook",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.4 Reinforcement learning textbook",
    "text": "1.4 Reinforcement learning textbook\nThe course uses the free textbook Reinforcement Learning: An Introduction by Sutton and Barto (2018). The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course.\nSutton and Barto’s book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it’s totally free. Just follow the citation link to download it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#course-organization",
    "href": "index.html#course-organization",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.5 Course organization",
    "text": "1.5 Course organization\nEach week considers a learning module. A learning module is related to either a chapter in the textbook, tutorials, cases or exercises. The learning path would typical be\n\nBefore lectures: Read the chapter in the textbook or tutorial.\nLectures (at campus).\nAfter lectures: Module Exercises (in groups).\n\nLectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials or cases are given and we focus on a specific RL problem.\nLecture notes also contains an appendix with different modules that may be helpful for you.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#programming-software",
    "href": "index.html#programming-software",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.6 Programming software",
    "text": "1.6 Programming software\nWe use Python as programming software and it is assumed that you are familiar with using Python. Python is a programming language and free software environment.\nDuring this course we are going to use Google Colab which is a hosted Jupyter notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. That is, Colab runs in your browser and you do not have to install anything on your computer. With a Jupyter notebook you can weave you code together with text.\nThat said, you may need to run Python on your laptop for larger tasks. You can run Python from a terminal but in general you use an IDE (integrated development environment) such as Positron, PyCharm or VSCode for running Python and to saving your work.\nIt is assumed as a prerequisite that you know how to use Python at a basic level. If you need a brush-up on your Python programming skills then have a look at Module 4 and Appendix A.\nEven though Python will be used in the course, algorithms can also be coded in R if you prefer to it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#ack",
    "href": "index.html#ack",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Acknowledgements and license",
    "text": "Acknowledgements and license\nMaterials are taken from various places:\n\nThe notes are based on Sutton and Barto (2018).\nSome notes are adopted from Scott Jeen, Bryn Elesedy and Peter Goldsborough.\nSome slides are inspired by the RL specialization at Coursera.\nSome exercises are taken from Sutton and Barto (2018) and modified slightly.\nSome code are inspired by Marcin Bogdanski.\n\nI would like to thank all for their inspiration.\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "index.html#sec-intro-ex",
    "href": "index.html#sec-intro-ex",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "1.7 Exercises (do before class)",
    "text": "1.7 Exercises (do before class)\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n1.7.1 Exercise - How to annotate\nThe online course notes can be annotated using hypothes.is. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help indicate spell errors, unclear content etc. in the notes.\n\n\nSign-up at hypothes.is. If you are using Chrome you may also install the Chrome extension.\nGo back to this page and login in the upper right corner (there should be some icons e.g. &lt;).\nSelect some text and try to annotate it using both a private and public annotation (you may delete it again afterwards).\nGo to the slides for this module and try to annotate the page with a private comment.\n\n\n\n1.7.2 Exercise - Colab\nDuring this course we are going to use Google Colab which is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. That is, Colab runs in your browser and you do not have to install anything on your computer.\nTo be familiar with Colab do the following:\n\nIf you do not have a Google account create one. Note if you have a gMail then you already have a Google account.\nOpen and do this tutorial.\n\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the course notes</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html",
    "href": "02_rl-intro.html",
    "title": "2  An introduction to RL",
    "section": "",
    "text": "2.1 Learning outcomes\nThis module gives a short introduction to Reinforcement learning.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 5, 6, 9 and 11 of the course.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#sec-rl-intro-lo",
    "href": "02_rl-intro.html#sec-rl-intro-lo",
    "title": "2  An introduction to RL",
    "section": "",
    "text": "Describe what RL is.\nBe able to identify different sequential decision problems.\nKnow what Business Analytics are and identify RL in that framework.\nMemorise different names for RL and how it fits in a Machine Learning framework.\nFormulate the blocks of a RL model (environment, agent, data, states, actions, rewards and policies).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#textbook-readings",
    "href": "02_rl-intro.html#textbook-readings",
    "title": "2  An introduction to RL",
    "section": "2.2 Textbook readings",
    "text": "2.2 Textbook readings\nFor this lecture, you will need to read Chapter 1-1.5 in Sutton and Barto (2018). Read it before continuing this module.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#what-is-reinforcement-learning",
    "href": "02_rl-intro.html#what-is-reinforcement-learning",
    "title": "2  An introduction to RL",
    "section": "2.3 What is reinforcement learning",
    "text": "2.3 What is reinforcement learning\nRL can be seen as\n\nAn approach of modelling sequential decision making problems.\nAn approach for learning good decision making under uncertainty from experience.\nMathematical models for learning-based decision making.\nTrying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.\nEstimating and finding near optimal decisions of a stochastic process with sequential decision making.\nA model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.\n\nSequential decision problems are problems where you take decisions/actions over time. As an agent, you base your decision on the current state of the system (a state is a function of the information/data available). At the next time-step, the system have moved (stochastically) to the next stage. Here new information may be available and you receive a reward and take a new action. Examples of sequential decision problems are (with possible actions):\n\nPlaying backgammon (how to move the checkers).\nDriving a car (left, right, forward, back, break, stop, …).\nHow to invest/maintain a portfolio of stocks (buy, sell, amount).\n\nControl an inventory (wait, buy, amount).\nVehicle routing (routes).\nMaintain a spare-part (wait, maintain).\nRobot operations (sort, move, …)\nDairy cow treatment/replacement (treat, replace, …)\nRecommender systems e.g. Netflix recommendations (videos)\n\nSince RL involves a scalar reward signal, the goal is to choose actions such that the total reward is maximized. Note actions have an impact on the future and may have long term consequences. As such, you cannot simply choose the action that maximize the current reward. It may, in fact, be better to sacrifice immediate reward to gain more long term reward.\nRL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance:\n\ntotally random trials (in the start),\nsophisticated tactics and superhuman skills (in the end).\n\nThat is, as the agent learn, the reward estimate of a given action becomes better.\nAs humans, we often learn by trial and error too:\n\nLearning to walk (by falling/pain).\nLearning to play (strategy is based on the game rules and what we have experienced works based on previous plays).\n\nThis can also be seen as learning the reward of our actions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#rl-and-business-analytics",
    "href": "02_rl-intro.html#rl-and-business-analytics",
    "title": "2  An introduction to RL",
    "section": "2.4 RL and Business Analytics",
    "text": "2.4 RL and Business Analytics\nBusiness Analytics (BA) (or just Analytics) refers to the scientific process of transforming data into insight for making better decisions in business. BA can both be seen as the complete decision making process for solving a business problem or as a set of methodologies that enable the creation of business value. As a process it can be characterized by descriptive, predictive, and prescriptive model building using “big” data sources.\nDescriptive Analytics: A set of technologies and processes that use data to understand and analyze business performance. Descriptive analytics are the most commonly used and most well understood type of analytics. Descriptive analytics categorizes, characterizes, consolidates, and classifies data. Examples are standard reporting and dashboards (KPIs, what happened or is happening now?) and ad-hoc reporting (how many/often?). Descriptive analytics often serves as a first step in the successful application of predictive or prescriptive analytics.\nPredictive Analytics: The use of data and statistical techniques to make predictions about future outputs/outcomes, identify patterns or opportunities for business performance. Examples of techniques are data mining (what data is correlated with other data?), pattern recognition and alerts (when should I take action to correct/adjust a spare part?), Monte-Carlo simulation (what could happen?), neural networks (which customer group are best?) and forecasting (what if these trends continue?).\nPrescriptive Analytics: The use of optimization and other decision modelling techniques using the results of descriptive and predictive analytics to suggest decision options with the goal of improving business performance. Prescriptive analytics attempt to quantify the effect of future decisions in order to advise on possible outcomes before the decisions are actually made. Prescriptive analytics predicts not only what will happen, but also why it will happen and provides recommendations regarding actions that will take advantage of the predictions. Prescriptive analytics are relatively complex to administer, and most companies are not yet using it in their daily course of business. However, when implemented correctly, it can have a huge impact on business performance and how businesses make decisions. Examples on prescriptive analytics are optimization in production planning and scheduling, inventory management, the supply chain and transportation planning. Since RL focus optimizing decisions it is Prescriptive Analytics also known as sequential decision analytics.\n\n\n\n\n\nBusiness Analytics and competive advantage.\n\n\n\n\nCompanies who use BA focus on fact-based management to drive decision making and treats data and information as a strategic asset that is shared within the company. This enterprise approach generates a companywide respect for applying descriptive, predictive and prescriptive analytics in areas such as supply chain, marketing and human resources. Focusing on BA gives a company a competive advantage (see Figure @ref(fig:analytics)).\nBA and related areas: In the past Business Intelligence traditionally focuses on querying, reporting, online analytical processing, i.e. descriptive analytics. However, a more modern definition of Business Intelligence is the union of descriptive and predictive analytics. Operations Research or Management Science deals with the application of advanced analytical methods to help make better decisions and can hence be seen as prescriptive analytics. However, traditionally it has been taking a more theoretical approach and focusing on problem-driven research while BA takes a more data-driven approach. Logistics is a cross-functional area focusing on the effective and efficient flows of goods and services, and the related flows of information and cash. Supply Chain Management adds a process-oriented and cross-company perspective. Both can be seen as prescriptive analytics with a more problem-driven research focus. Advanced Analytics is often used as a classification of both predictive and prescriptive analytics. Data science is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured and can be seen as Business analytics applied to a wider range of data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#rl-in-different-research-deciplines",
    "href": "02_rl-intro.html#rl-in-different-research-deciplines",
    "title": "2  An introduction to RL",
    "section": "2.5 RL in different research deciplines",
    "text": "2.5 RL in different research deciplines\nRL is used in many research fields using different names:\n\nRL (most used) originated from computer science and AI.\nApproximate dynamic programming (ADP) is mostly used within operations research.\nNeuro-dynamic programming (when states are represented using a neural network).\nRL is closely related to Markov decision processes (a mathematical model for a sequential decision problem).\n\n\n\n\n\n\nAdopted from Silver (2015).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#rl-and-machine-learning",
    "href": "02_rl-intro.html#rl-and-machine-learning",
    "title": "2  An introduction to RL",
    "section": "2.6 RL and machine learning",
    "text": "2.6 RL and machine learning\nDifferent ways of learning:\n\nSupervised learning: Given data \\((x_i, y_i)\\) learn to predict \\(y\\) from \\(x\\), i.e. find \\(y \\approx f(x)\\) (e.g. regression).\nUnsupervised learning: Given data \\((x_i)\\) learn patterns using \\(x\\), i.e. find \\(f(x)\\) (e.g. clustering). \nRL: Given state \\(x\\) you take an action and observe the reward \\(r\\) and the new state \\(x'\\).\n\nThere is no supervisor \\(y\\), only a reward signal \\(r\\).\nYour goal is to find a policy that optimize the total reward function.\n\n\n\n\n\n\n\nAdopted from Silver (2015).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#the-rl-data-stream",
    "href": "02_rl-intro.html#the-rl-data-stream",
    "title": "2  An introduction to RL",
    "section": "2.7 The RL data-stream",
    "text": "2.7 The RL data-stream\nRL considers an agent in an environment:\n\nAgent: The one who takes the action (computer, robot, decision maker).\nEnvironment: The system/world where observations and rewards are found.\n\nData are revealed sequentially as you take actions \\[(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \\ldots).\\] At time \\(t\\) the agent have been taken action \\(A_{t-1}\\) and observed observation \\(O_t\\) and reward \\(R_t\\):\n\n\n\n\n\nAgent-environment representation.\n\n\n\n\nThis gives us the history at time \\(t\\) is the sequence of observations, actions and rewards \\[H_t = (O_0, A_0, R_1, O_1, \\ldots, A_{t-1}, R_t, O_t).\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#states-actions-rewards-and-policies",
    "href": "02_rl-intro.html#states-actions-rewards-and-policies",
    "title": "2  An introduction to RL",
    "section": "2.8 States, actions, rewards and policies",
    "text": "2.8 States, actions, rewards and policies\nThe (agent) state \\(S_t\\) is the information used to take the next action \\(A_t\\):\n\n\n\n\n\nState and action.\n\n\n\n\nA state depends on the history, i.e. a state is a function of the history \\(S_t = f(H_t)\\). Different strategies for defining a state may be considered. Choosing \\(S_t = H_t\\) is bad since the size of a state representation grows very fast. A better strategy is to just store the information needed for taking the next action. Moreover, it is good to have Markov states where given the present state the future is independent of the past. That is, the current state holds just as much information as the history, i.e. it holds all useful information of the history. Symbolically, we call a state \\(S_t\\) Markov iff\n\\[\\Pr[S_{t+1} | S_t] = \\Pr[S_{t+1} | S_1,...,S_t].\\]\nThat is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states. Note that we can always find some Markov state. Though the smaller the state, the more “valuable” it is. In the worst case, \\(H_t\\) is Markov, since it represents all known information about itself.\nThe reward \\(R_t\\) is a number representing the reward at time \\(t\\) (negative if a cost). Examples of rewards are\n\nPlaying backgammon (0 (when play), 1 (when win), -1 (when loose)).\nHow to invest/maintain a portfolio of stocks (the profit).\n\nControl an inventory (inventory cost, lost sales cost).\nVehicle routing (transportation cost).\n\nThe goal is to find a policy that maximize the total future reward. A policy is the agent’s behaviour and is a map from state to action, i.e. a function \\[a = \\pi(s)\\] saying that given the agent is in state \\(s\\) we choose action \\(a\\).\nThe total future reward is currently not defined clearly. Let the value function denote the future reward in state \\(s\\) and define it as the expected discounted future reward: \\[V_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots | S = s).\\] Note the value function is defined using a specific policy and the goal is to find a policy that maximize the total future reward in all possible states \\[\\pi^* = \\arg\\max_{\\pi\\in\\Pi}(V_\\pi(s)).\\]\nThe value of the discount rate is important:\n\nDiscount rate \\(\\gamma=0\\): Only care about present reward.\nDiscount rate \\(\\gamma=1\\): Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite.\nDiscount rate \\(\\gamma&lt;1\\): Rewards near to the present more beneficial. Note \\(V(s)\\) will converge to a number even if the time-horizon is infinite.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#exploitation-vs-exploration",
    "href": "02_rl-intro.html#exploitation-vs-exploration",
    "title": "2  An introduction to RL",
    "section": "2.9 Exploitation vs Exploration",
    "text": "2.9 Exploitation vs Exploration\nA key problem of reinforcement learning (in general) is the difference between exploration and exploitation. Should the agent sacrifice what is currently known as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? Exploitation takes the action assumed to be optimal with respect to the data observed so far. This, gives better predictions of the value function (given the current policy) but prevents the agent from discovering potential better decisions (a better policy). Exploration does not take the action that seems to be optimal. That is, the agent explore to find new states and update the value function for this state.\nExamples in the exploration and exploitation dilemma are for instance movie recommendations: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration) or oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "02_rl-intro.html#summary",
    "href": "02_rl-intro.html#summary",
    "title": "2  An introduction to RL",
    "section": "2.10 Summary",
    "text": "2.10 Summary\nRead Chapter 1.6 in Sutton and Barto (2018).\n\n\n\n\nSilver, David. 2015. “Lectures on Reinforcement Learning.” https://www.davidsilver.uk/teaching/.\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html",
    "href": "03_rl-in-action.html",
    "title": "3  RL in action",
    "section": "",
    "text": "3.1 Learning outcomes\nThis module learn an agent/player to play tic-tac-toe using Reinforcement learning.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 6 and 9-12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#sec-rl-intro-lo",
    "href": "03_rl-in-action.html#sec-rl-intro-lo",
    "title": "3  RL in action",
    "section": "",
    "text": "Code your first RL algorithm\nFormulate the blocks of the RL model (environment, agent, data, states, actions, rewards and policies).\nRun your first RL algorithm and evaluate on its solution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#textbook-readings",
    "href": "03_rl-in-action.html#textbook-readings",
    "title": "3  RL in action",
    "section": "3.2 Textbook readings",
    "text": "3.2 Textbook readings\nFor this lecture, you will need to read Section 1.5 in Sutton and Barto (2018). Read it before continuing this module.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#tic-tac-toe",
    "href": "03_rl-in-action.html#tic-tac-toe",
    "title": "3  RL in action",
    "section": "3.3 Tic-Tac-Toe",
    "text": "3.3 Tic-Tac-Toe\nTic-Tac-Toe is a simple two-player game where players alternately place their marks on a 3x3 grid. That is, the current state of the board is the players’ marks on the grid. The first player to align three of their marks horizontally, vertically, or diagonally wins the game. Reward for a player is 1 for ‘win’, 0.5 for ‘draw’, and 0 for ‘loss’. These values can be seen as the probability of winning.\n\n\n\nExamples of winning, loosing and a draw from player Xs point of view:\n\n\n\n\n\n\n\n\n.\n\n\n.\n\n\nX\n\n\n\n\n.\n\n\nX\n\n\n.\n\n\n\n\nX\n\n\nO\n\n\nO\n\n\n\n\n\n\n\n\n\n\nX\n\n\n.\n\n\nX\n\n\n\n\n.\n\n\nX\n\n\n.\n\n\n\n\nO\n\n\nO\n\n\nO\n\n\n\n\n\n\n\n\n\n\nX\n\n\nX\n\n\nO\n\n\n\n\nO\n\n\nO\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nO\n\n\n\n\n\n\n\n\nWe start with an empty board and have at most 9 moves (a player may win before). If the opponent start and a state denote the board before the opponent makes a move, then a draw game may look as in Fig. 3.1. We start with an empty board state \\(S_0\\), and the opponent makes a move, next we choose a move \\(A_0\\) (among the empty fields) and we end up in state \\(S_1\\). This continues until the game is over.\n\n\n\n\n\n\n\n\nFigure 3.1: A draw.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#colab",
    "href": "03_rl-in-action.html#colab",
    "title": "3  RL in action",
    "section": "3.4 Colab",
    "text": "3.4 Colab\nDuring the lecture for this module we will work with the tutorial where we learn how to play the game using RL. You will both test your coding skills and the dilemma between exploration and exploitation. Should the agent sacrifice what is currently known as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? Exploitation takes the action assumed to be optimal with respect to the data observed so far. Exploration does not take the action that seems to be optimal. That is, the agent explore to find new states that may be better.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nOpen the tutorial:\n\nHave a look at the notebook and run all code cells.\nTry to understand the content.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#summary",
    "href": "03_rl-in-action.html#summary",
    "title": "3  RL in action",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nRead Section 1.6 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "03_rl-in-action.html#sec-rl-in-action-ex",
    "href": "03_rl-in-action.html#sec-rl-in-action-ex",
    "title": "3  RL in action",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\nYou can find all these exercises as a section in this Colab notebook. If you already have your own copy use this.\n\n3.6.1 Exercise - Self-Play\nConsider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?\n\n\n\n\n\n3.6.2 Exercise - Symmetries\nMany tic-tac-toe positions appear different but are really the same because of symmetries.\n\nHow might we amend the reinforcement learning algorithm described above to take advantage of this?\n\n\n\nIn what ways would this improve the algorithm?\n\n\n\nSuppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n\n\n\n\n\n\n3.6.3 Exercise - Greedy Play\nConsider Tic-Tac-Toe and suppose the RL player is only greedy (\\(\\epsilon = 0\\)), that is, always playing the move that that gives the highest probability of winning. Would it learn to play better, or worse, than a non-greedy player? What problems might occur?\n\n\n\n\n\n3.6.4 Exercise - Learning from Exploration\nConsider Tic-Tac-Toe and suppose the RL player is playing against an opponent with a fixed strategy. Suppose learning updates occur after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities.\n\nWhat are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?\n\n\n\nAssuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\n\n\n\n\n3.6.5 Exercise - Other Improvements\nConsider Tic-Tac-Toe. Can you think of other ways to improve the reinforcement learning player?\n\n\n\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>RL in action</span>"
    ]
  },
  {
    "objectID": "04_python.html",
    "href": "04_python.html",
    "title": "4  An introduction to Python",
    "section": "",
    "text": "4.1 Learning outcomes\nThis module gives an introduction to Python and some of the packages we use during the course.\nBy the end of this module, you are expected to:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to Python</span>"
    ]
  },
  {
    "objectID": "04_python.html#sec-rl-intro-lo",
    "href": "04_python.html#sec-rl-intro-lo",
    "title": "4  An introduction to Python",
    "section": "",
    "text": "Identify useful Python workflows.\nDo different programming workflows.\nCode a Python class.\nTransform data.\nPlot data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to Python</span>"
    ]
  },
  {
    "objectID": "04_python.html#textbook-readings",
    "href": "04_python.html#textbook-readings",
    "title": "4  An introduction to Python",
    "section": "4.2 Textbook readings",
    "text": "4.2 Textbook readings\nNo readings and slides.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to Python</span>"
    ]
  },
  {
    "objectID": "04_python.html#python-and-packages",
    "href": "04_python.html#python-and-packages",
    "title": "4  An introduction to Python",
    "section": "4.3 Python and packages",
    "text": "4.3 Python and packages\nAn tutorial is given in this Colab notebook. Open it and get an overview before class.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>An introduction to Python</span>"
    ]
  },
  {
    "objectID": "05_bandit.html",
    "href": "05_bandit.html",
    "title": "5  Multi-armed bandits",
    "section": "",
    "text": "5.1 Learning outcomes\nThis module consider the k-armed bandit problem which is a sequential decision problem with one state and \\(k\\) actions. The problem is used to illustrate different learning methods used in RL.\nThe module is also the first module in the Tabular methods part of the notes. This part describe almost all the core ideas of reinforcement learning algorithms in their simplest forms where the state and action spaces are small enough for the approximate value functions to be represented as arrays or tables.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 1, 3, 6, 9, 10 and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#learning-outcomes",
    "href": "05_bandit.html#learning-outcomes",
    "title": "5  Multi-armed bandits",
    "section": "",
    "text": "Define a k-armed bandit and understand the nature of the problem.\nDefine the reward of a action (action-reward).\nDescribe different methods for estimating the action-reward.\nExplain the differences between exploration and exploitation.\nFormulate an \\(\\epsilon\\)-greedy algorithm for selecting the next action.\nInterpret the sample-average (variable step-size) versus exponential recency-weighted average (constant step-size) action-reward estimation.\nArgue why we might use a constant stepsize in the case of non-stationarity.\nUnderstand the effect of optimistic initial values.\nFormulate an upper confidence bound action selection algorithm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#textbook-readings",
    "href": "05_bandit.html#textbook-readings",
    "title": "5  Multi-armed bandits",
    "section": "5.2 Textbook readings",
    "text": "5.2 Textbook readings\nFor this module, you will need to read Chapter 2 - 2.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#the-k-armed-bandit-problem",
    "href": "05_bandit.html#the-k-armed-bandit-problem",
    "title": "5  Multi-armed bandits",
    "section": "5.3 The k-armed bandit problem",
    "text": "5.3 The k-armed bandit problem\nMulti-armed bandits attempt to find the best option among a collection of alternatives by learning through trial and error. The name derives from “one-armed bandit,” a slang term for a slot machine — which is a perfect analogy for how these algorithms work.\n\n\n\n\n\n\n\n\nFigure 5.1: A 4-armed bandit.\n\n\n\n\n\nImagine you are facing a wall with \\(k\\) slot machines (see Fig. 5.1), and each one pays out at a different rate. A natural way to figure out how to make the most money (rewards) would be to try each at random for a while (exploration), and start playing the higher paying ones once you have gained some experience (exploitation). That is, from an agent/environment point of view the agent considers a single state \\(s\\) at time \\(t\\) and have to choose among \\(k\\) actions given the environment representing the \\(k\\) bandits. Only the rewards from the \\(k\\) bandits are unknown, but the agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. The objective is to find an optimal policy that maximize the total expected reward. Note since the process only have a single state, this is the same as finding an optimal policy \\(\\pi^*(s) = \\pi^* = a^*\\) that chooses the action with the highest expected reward. Due to uncertainty, there is an exploration vs exploitation dilemma. The agent have one action that seems to be most valuable at a time point, but it is highly likely, at least initially, that there are actions yet to explore that are more valuable.\n\n\n\n\n\n\n\nMulti-armed bandits can be used in e.g. digital advertising. Suppose you are an advertiser seeking to optimize which ads (\\(k\\) to choose among) to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time.\n\n\n\n\n\n\n\n\nFigure 5.2: Which ad to choose?\n\n\n\n\n\nIt is reasonable to assume that each of these ads will have different effects, and some will be more engaging than others. That is, each ad has some theoretical — but unknown — click-through-rate (CTR) that is assumed to not change over time. How do we go about solving which ad we should choose (see Fig. 5.2)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#estimating-the-value-of-an-action",
    "href": "05_bandit.html#estimating-the-value-of-an-action",
    "title": "5  Multi-armed bandits",
    "section": "5.4 Estimating the value of an action",
    "text": "5.4 Estimating the value of an action\nHow can the value of an action be estimated, i.e. the expected reward of an action \\(q_*(a) = \\mathbb{E}[R_t | A_t = a].\\) Assume that at time \\(t\\) action \\(a\\) has been chosen \\(N_t(a)\\) times. Then the estimated action value is \\[\\begin{equation}\n    Q_t(a) = \\frac{R_1+R_2+\\cdots+R_{N_t(a)}}{N_t(a)},\n\\end{equation}\\] Storing \\(Q_t(a)\\) this way is cumbersome since memory and computation requirements grow over time. Instead an incremental approach is better. If we assume that \\(N_t(a) = n-1\\) and set \\(Q_t(a) = Q_n\\) then \\(Q_{n+1}\\) becomes: \\[\n\\begin{align}\n  Q_{n+1} &= \\frac{1}{n}\\sum_{i=1}^{n}R_i \\nonumber \\\\\n            &= \\frac{1}{n}\\left( R_{n} + \\sum_{i=1}^{n-1} R_i \\right) \\nonumber \\\\\n            &= \\frac{1}{n}\\left( R_{n} + (n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1} R_i \\right) \\nonumber \\\\\n            &= \\frac{1}{n}\\left( R_{n} + (n-1)Q_n \\right) \\nonumber \\\\\n           &= Q_n + \\frac{1}{n} \\left[R_n - Q_n\\right].\n\\end{align}\n\\tag{5.1}\\] That is, we can update the estimate of the value of \\(a\\) using the previous estimate, the observed reward and how many times the action has occurred (\\(n\\)).\nA greedy approach for selecting the next action is \\[\\begin{equation}\nA_t =\\arg \\max_a Q_t(a).\n\\end{equation}\\] Here \\(\\arg\\max_a\\) means the value of \\(a\\) for which \\(Q_t(a)\\) is maximised. A pure greedy approach do not explore other actions. Instead an \\(\\varepsilon\\)-greedy approach is used in which with probability \\(\\varepsilon\\) we take a random draw from all of the actions (choosing each action with equal probability) and hereby providing some exploration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nDuring the lecture for this module, we will work with this tutorial where we will try to estimate the \\(Q\\) values. We implement the algorithm using an agent and environment class. Open the tutorial:\n\nHave a look at the notebook and run all code cells.\nTry to understand the content.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#sec-bandit-step-size",
    "href": "05_bandit.html#sec-bandit-step-size",
    "title": "5  Multi-armed bandits",
    "section": "5.5 The role of the step-size",
    "text": "5.5 The role of the step-size\nIn general we update the reward estimate of an action using\n\\[\\begin{equation}\n    Q_{n+1} = Q_n +\\alpha_n(a) \\left[R_n - Q_n\\right]\n\\end{equation}\\]\nUntil now we have used the sample average \\(\\alpha_n(a)= 1/n\\); however, other choices of \\(\\alpha_n(a)\\) is possible. In general we will converge to the true expected reward if\n\\[\\begin{equation}\n    \\sum_n \\alpha_n(a) = \\infty \\quad\\quad \\mathsf{and} \\quad\\quad  \\sum_n \\alpha_n(a)^2 &lt; \\infty.\n\\end{equation}\\] {#eq-alpha-convergence}\nMeaning that the coefficients must be large enough to recover from initial fluctuations, but not so large that they do not converge in the long run. However, if the process is non-stationary, i.e. the expected reward of an action change over time, then convergence is undesirable and we may want to use a constant \\(\\alpha_n(a)= \\alpha \\in (0, 1]\\) instead. This results in \\(Q_{n+1}\\) being a weighted average of the past rewards and the initial estimate \\(Q_1\\):\n\\[\\begin{align}\nQ_{n+1} &= Q_n +\\alpha \\left[R_n - Q_n\\right] \\nonumber \\\\\n&= \\alpha R_n + (1 - \\alpha)Q_n \\nonumber \\\\\n&= \\alpha R_n + (1 - \\alpha)[\\alpha R_{n-1} + (1 - \\alpha)Q_{n-1}] \\nonumber \\\\\n&= \\alpha R_n + (1 - \\alpha)\\alpha R_{n-1} + (1 - \\alpha)^2 Q_{n-1}  \\nonumber \\\\\n&= \\vdots \\nonumber \\\\\n&= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^{n-i} R_i \\\\\n\\end{align}\\]\nBecause the weight given to each reward depends on how long ago it was observed, we can see that more recent rewards are given more weight. Note the weights sum to 1 here, ensuring it is indeed a weighted average where more weight is allocated to recent rewards. Since the weight given to each reward decays exponentially into the past. This sometimes called an exponential recency-weighted average.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#optimistic-initial-values",
    "href": "05_bandit.html#optimistic-initial-values",
    "title": "5  Multi-armed bandits",
    "section": "5.6 Optimistic initial values",
    "text": "5.6 Optimistic initial values\nThe methods discussed so far are dependent to some extent on the initial action-value estimate i.e. they are biased by their initial estimates. For methods with constant step-size \\((\\alpha)\\) this bias is permanent.\nWe may set initial value estimates artificially high to encourage exploration in the short run. For instance, if the observed rewards in general are below 5, then by setting initial values of \\(Q\\) to 5 rather than zero, we encourage exploration, even in the greedy case. Here, the agent will almost always be disappointed with its samples because they are less than the initial estimate, and so will explore elsewhere until the values converge.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#upper-confidence-bound-action-selection",
    "href": "05_bandit.html#upper-confidence-bound-action-selection",
    "title": "5  Multi-armed bandits",
    "section": "5.7 Upper-Confidence Bound Action Selection",
    "text": "5.7 Upper-Confidence Bound Action Selection\nAn \\(\\epsilon\\)-greedy algorithm choose the action to explore with equal probability in an exploration step. It would be better to select among non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. One way to do this is to select actions using the upper-confidence bound: \\[\\begin{equation}\n    A_t = \\arg\\max_a \\left(Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\right),\n\\end{equation}\\]\nNote the square root term is a measure of the uncertainty in our estimate (see Fig. 5.3).\n\nIt is proportional to \\(t\\), i.e. how many time-steps have passed and inversely proportional to \\(N_t(a)\\) i.e. how many times that action has been visited.\nThe more time has passed, and the less we have sampled an action, the higher our upper-confidence-bound.\nAs the timesteps increases, the denominator dominates the numerator as the ln term flattens.\nEach time we select an action our uncertainty decreases because \\(N\\) is the denominator of this equation.\nIf \\(N_t(a) = 0\\) then we consider \\(a\\) as a maximal action, i.e. we select first among actions with \\(N_t(a) = 0\\).\nThe parameter \\(c&gt;0\\) controls the degree of exploration. Higher \\(c\\) results in more weight on the uncertainty.\n\nSince upper-confidence bound action selection select actions according to their potential, it is expected to perform better than \\(\\epsilon\\)-greedy methods.\n\n\n\n\n\n\n\n\nFigure 5.3: Square root term for an action using different \\(c\\)-values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#summary",
    "href": "05_bandit.html#summary",
    "title": "5  Multi-armed bandits",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nRead Chapter 2.10 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "05_bandit.html#exercises",
    "href": "05_bandit.html#exercises",
    "title": "5  Multi-armed bandits",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n5.9.1 Exercise - Advertising\nSuppose you are an advertiser seeking to optimize which ads to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time. Assume that:\n\nYou have \\(k=5\\) adds to choose among.\nIf add \\(A\\) is chosen then the user clicks the add with probability \\(p_A\\) which can be seen as the unknown click trough rate CTR (or an average reward).\nThe CTRs are unknown and samples can be picked using the RLAdEnv class and the reward function which returns 1 if click on ad and 0 otherwise.\nIn the class the true CTRs can be observed but in practice this is hidden from the agent (you).\n\nConsider this section in the Colab notebook to see the questions and the RLAdEnv class. Use your own copy if you already have one.\n\n\n5.9.2 Exercise - A coin game\nConsider a game where you choose to flip one of two (possibly unfair) coins. You win 1 if your chosen coin shows heads and lose 1 if it shows tails.\nSee this section in the Colab notebook for questions. Use your own copy if you already have one.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multi-armed bandits</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html",
    "href": "06_mdp-1.html",
    "title": "6  Markov decision processes (MDPs)",
    "section": "",
    "text": "6.1 Learning outcomes\nThis module gives an introduction to Markov decision processes (MDPs) with a finite number of states and actions. This gives us a full model of a sequential decision problem. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also what will be the next state, and hence future rewards. Thus MDPs involve delayed reward and the need to consider the trade-off between immediate and delayed reward. MDPs are a mathematically idealized form of the RL problem where a full model/description is known and the optimal policy can be found. Often in a RL problem some parts of this description is unknown and we hereby have to estimate the best policy by learning. For example, in the bandit problem the rewards was unknown.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 2, 7, 10, and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#learning-outcomes",
    "href": "06_mdp-1.html#learning-outcomes",
    "title": "6  Markov decision processes (MDPs)",
    "section": "",
    "text": "Identify the different elements of a Markov Decision Processes (MDP).\nDescribe how the dynamics of an MDP are defined.\nUnderstand how the agent-environment RL description relates to an MDP.\nInterpret the graphical representation of a Markov Decision Process.\nDescribe how rewards are used to define the objective function (expected return).\nInterpret the discount rate and its effect on the objective function.\nIdentify episodes and how to formulate an MDP by adding an absorbing state.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#textbook-readings",
    "href": "06_mdp-1.html#textbook-readings",
    "title": "6  Markov decision processes (MDPs)",
    "section": "6.2 Textbook readings",
    "text": "6.2 Textbook readings\nRead Chapter 3-3.4 in Sutton and Barto (2018). Read it before continuing this module. You do not have to put to much focus on the examples. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#an-mdp-as-a-model-for-the-agent-environment",
    "href": "06_mdp-1.html#an-mdp-as-a-model-for-the-agent-environment",
    "title": "6  Markov decision processes (MDPs)",
    "section": "6.3 An MDP as a model for the agent-environment",
    "text": "6.3 An MDP as a model for the agent-environment\nLet us recall the RL problem which considers an agent in an environment:\n\nAgent: The one who takes the action (computer, robot, decision maker), i.e. the decision making component of a system. Everything else is the environment. A general rule is that anything that the agent does not have absolute control over forms part of the environment.\nEnvironment: The system/world where observations and rewards are found.\n\nAt time step \\(t\\) the agent is in state \\(S_t\\) and takes action \\(A_{t}\\) and observe the new state \\(S_{t+1}\\) and reward \\(R_{t+1}\\):\n\n\n\n\n\nAgent-environment representation.\n\n\n\n\nNote we here assume that the Markov property is satisfied and the current state holds just as much information as the history of observations. That is, given the present state the future is independent of the past:\n\\[\\Pr(S_{t+1} | S_t, A_t) = \\Pr(S_{t+1} | S_1,...,S_t, A_t).\\] That is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states.\nA Markov decision process (MDP) is a mathematical model that for each time-step \\(t\\) have defined states \\(S_t \\in \\mathcal{S}\\), possible actions \\(A_t \\in \\mathcal{A}(s)\\) given a state and rewards \\(R_t \\in \\mathcal{R} \\subset \\mathbb{R}\\). Consider the example in Fig. 6.1. Each time-step have five states \\(\\mathcal{S} = \\{1,2,3,4,5\\}\\). Assume that the agent start in state \\(s_0 = 3\\) with two actions to choose among \\(\\mathcal{A}(s_0) = \\{a_1, a_2\\}\\). After choosing \\(a_1\\) a transition to \\(s_1 = 2\\) happens with reward \\(R_1 = r_1\\). Next, in state \\(s_1\\) the agent chooses action \\(a_2\\) and a transition to \\(s_2\\) happens with reward \\(r_2\\). This continues as time evolves.\n\n\n\n\n\n\n\n\nFigure 6.1: State-expanded hypergraph\n\n\n\n\n\nIn a finite MDP, the sets of states, actions, and rewards all have a finite number of elements. In this case, the random variables have well defined discrete probability distributions dependent only on the preceding state and action which defines the dynamics of the system: \\[\\begin{equation}\n    p(s', r | s, a) = \\Pr(S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a),\n\\end{equation}\\] which can be used to find the transition probabilities: \\[\\begin{equation}\n    p(s' | s, a) = \\Pr(S_t = s'| S_{t-1} = s, A_{t-1}=A) = \\sum_{r \\in \\mathcal{R}} p(s', r | s, a),\n\\end{equation}\\] and the expected reward: \\[\\begin{equation}\n    r(s, a) = \\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r | s, a).\n\\end{equation}\\]\nThat is, to define an MDP the following are needed:\n\nA finite number of states and actions. That is, we can store values using tabular methods.\nAll states \\(S \\in \\mathcal{S}\\) and actions \\(A \\in \\mathcal{A}(s)\\) are known.\n\nThe transition probabilities \\(p(s' | s, a)\\) and expected rewards \\(r(s, a)\\) are given. Alternatively, \\(p(s', r | s, a)\\).\n\nMoreover, for now a stationary MDP is considered, i.e. at each time-step all states, actions and probabilities are the same and hence the time index can be dropped.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#rewards-and-the-objective-function-goal",
    "href": "06_mdp-1.html#rewards-and-the-objective-function-goal",
    "title": "6  Markov decision processes (MDPs)",
    "section": "6.4 Rewards and the objective function (goal)",
    "text": "6.4 Rewards and the objective function (goal)\nThe reward hypothesis is a central assumption in reinforcement learning:\n\nAll of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nThis assumption can be questioned but in this course we assume it holds. The reward signal is our way of communicating to the agent what we want to achieve not how we want to achieve it.\nThe return \\(G_t\\) can be defined as the sum of future rewards; however, if the time horizon is infinite the return is also infinite. Hence we use a discount rate \\(0 \\leq \\gamma \\leq 1\\) and define the return as\n\\[\\begin{equation}\n    G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\end{equation}\\]\nDiscounting is important since it allows us to work with finite returns because if \\(\\gamma &lt; 1\\) and the reward is bounded by a number \\(B\\) then the return is always finite:\n\\[\\begin{equation}\nG_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\leq B \\sum_{k=0}^{\\infty} \\gamma^k  = B \\frac{1}{1 - \\gamma}\n\\end{equation}\\]\nNote gamma close to one put weight on future rewards while a gamma close to zero put weight on present rewards. Moreover, an infinite time-horizon is assumed.\nAn MDP modelling a problem over a finite time-horizon can be transformed into an infinite time-horizon using an absorbing state with transitions only to itself and a reward of zero. This breaks the agent-environment interaction into episodes (e.g playing a board game). Each episode ends in the absorbing state, possibly with a different reward. Each starts independently of the last, with some distribution of starting states. Sequences of interaction without an absorbing state are called continuing tasks.\nThe objective function is to choose actions such that the expected return is maximized. We will formalize this mathematically in the next module.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#summary",
    "href": "06_mdp-1.html#summary",
    "title": "6  Markov decision processes (MDPs)",
    "section": "6.5 Summary",
    "text": "6.5 Summary\n\nMDPs formalize the problem of an agent interacting with an environment.\nThe agent and environment interact at discrete time steps.\nAt each time, the agent observes the current state of the environment. Then selects an action and the the environment transitions to a new state with a reward.\nAn agent’s choices have long-term consequences (delayed reward).\nSelected actions influences future states and rewards.\nThe objective is to maximize the expected discounted return.\nWith a discount rate less than one, we can guarantee the return remains finite.\nThe value of the discount rate defines how much we care about short-term rewards versus long-term rewards.\nA first step in applying reinforcement learning is to formulate the problem as an MDP.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "06_mdp-1.html#sec-mdp-1-ex",
    "href": "06_mdp-1.html#sec-mdp-1-ex",
    "title": "6  Markov decision processes (MDPs)",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\nAll these exercises can be solved analytically without Python.\n\n6.6.1 Exercise - Sequential decision problems\n\nThink of two sequential decision problems and try to formulate them as MDPs. Describe the states, actions and rewards in words.\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nExamples could be:\n\nLudo - State: position on the board. Actions: Possible movements. Rewards: In a win state e.g. 1, in a loose state -1 and 0 otherwise.\nInventory management - State: inventory level. Actions: Order \\(x\\) units, wait. Rewards: a negative number representing inventory holding cost plus ordering cost.\nInvestment - State: current portfolio, KPI’s from considered companies. Actions: Buy/sell \\(x\\) stocks of company \\(y.\\) Rewards: returns - costs.\n\n\n\n\nHow do the states, actions and rewards look like for the bandit problem? Try drawing the state-expanded hypergraph.\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nFor the k-bandit problem we only have a single state representing before we chose an action. We have \\(k\\) actions and the rewards are the probability distribution from each slot machine. Note the k-bandit problem is trivial if we know the MDP, since then we know the expected reward of each action and hence the action with best expected reward will be optimal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 Exercise - Expected return\n\nSuppose \\(\\gamma=0.8\\) and we observe the following sequence of rewards: \\(R_1 = -3\\), \\(R_2 = 5\\), \\(R_3=2\\), \\(R_4 = 7\\), and \\(R_5 = 1\\) with a finite time-horizon of \\(T=5\\). What is \\(G_0\\)? Hint: work backwards and recall that \\(G_t = R_{t+1} + \\gamma G_{t+1}\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\n\ngam = 0.8\ng = 0\nr = [-3, 5, 2, 7, 1]\nprint(f't = 5 G_5 = {g}')\n#&gt; t = 5 G_5 = 0\nfor i in range(4, -1, -1):\n    g = r[i] + gam * g\n    print(f't = {i} G_{i} = {g}')\n#&gt; t = 4 G_4 = 1.0\n#&gt; t = 3 G_3 = 7.8\n#&gt; t = 2 G_2 = 8.24\n#&gt; t = 1 G_1 = 11.592\n#&gt; t = 0 G_0 = 6.2736\n\n\n\n\nSuppose \\(\\gamma=0.9\\) and we observe rewards: \\(R_1 = 2\\), \\(R_t = 7\\), \\(t&gt;1\\) given a infinite time-horizon. What is \\(G_0\\) and \\(G_1\\)? Hint: recall that \\(\\sum_{k=0}^\\infty x^k = 1/(1-x)\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nNote \\[\\begin{align}G_1 &= R_{t+1} + \\gamma G_{t+1} \\\\\n                        &= 7 + \\gamma G_{t+1} \\\\\n                        &= 7 + \\gamma (7 + \\gamma G_{t+2}) \\\\\n                        &= 7(1 + \\gamma + \\gamma^2 + \\ldots) \\\\\n                        &= 7\\sum_{k=0}^\\infty 0.9^k \\\\\n                        &= \\frac{7}{1-0.9}\\end{align}\\]\n\nG_1 = 7 * 1/(1-0.9)\nprint(G_1)\n#&gt; 70.00000000000001\nG_0 = 2 + 0.9 * G_1\nprint(G_0)\n#&gt; 65.00000000000001\n\n\n\n\n\n\n\n6.6.3 Exercise - Gambler’s problem\nA gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. The coin may be an unequal coin where there is not equal probability \\(p_H\\) for a head (H) and a tail (T). If the coin comes up heads, the gambler wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler reaches his goal of a capital equal 100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP, where we assume that the gambler starts with a capital \\(0 &lt; s_0 &lt; 100\\).\n\nDefine the state space \\(\\mathcal{S}\\). Which states are terminal states?\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nCapital of the gambler: \\[\\mathcal{S} = \\{0, \\ldots, 100 \\}.\\] Terminal states are 0 and 100 (loose or win).\n\n\n\nDefine the action space \\(\\mathcal{A}(s)\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nGiven his capital choose to gamble \\(a\\): \\[\\mathcal{A}(s) = \\{ a\\in \\mathcal{S} | 0 \\leq a \\leq \\min(s, 100-s) \\}.\\]\n\n\n\nLet \\(R_a\\) denote the reward given bet \\(a\\) (a stochastic variable). Calculate the expected rewards. If the state-value for the terminal states is set to zero, what do the state-value of a policy mean?\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nThe expected reward is: \\[r(s,a) = \\mathbb{E}[R_a] = p_H a\\] where \\(p_H\\) denote the probability of head. The state-value denote the expected reward.\n\n\n\nLet \\(R_a\\) be zero for all bets \\(a\\) and set the state-value for the terminal state 0 to zero and for state 100 to one. What do the state-value of a policy mean?\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nSince \\(r(s,a) = 0\\) for all states and actions, the state-value is the probability of winning.\n\n\n\nCalculate the transition probabilities.\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nIf \\(C\\) denote a Bernoulli variable equal 1 if head. Then \\[p(s' | s, a) = \\Pr(s' = s + Ca - (1-C)a).\\] Hence there are two transitions: if \\(s' = s - a\\) then \\(p(s' | s, a) = 1-p_H\\) and if \\(s' = s + a\\) then \\(p(s' | s, a) = p_H\\).\n\n\n\nWrite an alternative MDP model having only a single absorbing state representing “game over” and let the objective be the probability of winning. That is, state 0 and 100 are not absorbing states in this case.\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nStates\nWe add state 101 representing “game over” \\[\\mathcal{S} = \\{0, \\ldots, 100, 101 \\}.\\] Terminal state is now 101 (game over).\nActions\nIf \\(s\\leq 100\\) then \\[\\mathcal{A}(s) = \\{ a\\in \\mathcal{S} | 0 \\leq a \\leq \\min(s, 100-s) \\}.\\] If \\(s = 101\\) then \\(A(101) = \\{d\\}\\) (dummy action).\nRewards\nThe expected reward is \\(r(s,a) = 0 for 0 \\leq s &lt; 100, a\\in A(s)\\), \\(r(100,0) = 1\\) and \\(r(101,d) = 0\\).\nTransition probabilities\nFor \\(0 &lt; s &lt; 100\\) we have that \\[\\begin{align}\np(s' | s, a) = 1-p_H \\qquad &\\text{if } s' = s - a\\\\\np(s' | s, a) = p_H \\qquad &\\text{if } s' = s + a\n\\end{align}\n\\] Moreover, we have transitions to the absorbing state as: \\(p(101 | 0, 0) = p(101 | 100, 0) = 1\\) and \\(p(101 | 101, d)\\). Note that using this model, we do not need to set the state-value of the absorbing state.\n\n\n\n\n\n\n6.6.4 Exercise - Factory storage\nA factory has a storage tank with a capacity of 4 \\(\\mathrm{m}^{3}\\) for temporarily storing waste produced by the factory. Each week the factory produces \\(0,1\\), 2 or 3 \\(\\mathrm{m}^{3}\\) waste with respective probabilities \\[p_{0}=\\displaystyle \\frac{1}{8},\\ p_{1}=\\displaystyle \\frac{1}{2},\\ p_{2}=\\displaystyle \\frac{1}{4} \\text{ and } p_{3}=\\displaystyle \\frac{1}{8}.\\] If the amount of waste produced in one week exceeds the remaining capacity of the tank, the excess is specially removed at a cost of $30 per cubic metre. At the end of each week there is a regular opportunity to remove all waste from the storage tank at a fixed cost of $25 and a variable cost of $5 per cubic metre.\nThe problem can be modelled as a finite MDP where a state denote the amount of waste in the tank at the end of week \\(n\\) just before the regular removal opportunity.\n\nDefine the state space \\(\\mathcal{S}\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\n\\[\\mathcal{S} = \\{ 0,1,2,3,4 \\}\\]\n\n\n\nDefine the action space \\(\\mathcal{A}(s)\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nLet \\(e\\) and \\(k\\) denote empty and keep the waste from the tank. Then the action space is \\[\\mathcal{A}(s) = \\{ e, k \\}.\\]\n\n\n\nCalculate the expected rewards \\(r(s,a)\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nThe expected cost of a given state and action is the cost of empting the container and the expected cost of a special removal during the next week. Hence \\[r(s, e) = -(25 + 5s)\\]and\\[r(s,k) = -30\\sum_{i&gt;4-s} (s+i-4)p_i\\]\n\n\n\nCalculate the transition probabilities \\(p(s'|s,a)\\).\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nThe transition probabilities are: \\[p(s'|s,k) = p_{s'-s}\\text{ if } s\\leq s' \\leq 3\\] \\[p(4|s,k) = \\sum_{i\\geq 4-s} p_i\\] \\[p(s'|s,e) = p_{s'}\\text{ if }  0\\leq s' &lt; 4\\] \\[p(s'|s,a) =  0 \\text{ otherwise.}\\]\n\n\n\n\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markov decision processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html",
    "href": "07_mdp-2.html",
    "title": "7  Policies and value functions for MDPs",
    "section": "",
    "text": "7.1 Learning outcomes\nThis module go deeper in the theory of finite Markov decision processes (MDPs). The concept of a policy and value functions is considered. Once the problem is formulated as an MDP, finding the optimal policy can be found using value functions.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 2, 7, 10, and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#learning-outcomes",
    "href": "07_mdp-2.html#learning-outcomes",
    "title": "7  Policies and value functions for MDPs",
    "section": "",
    "text": "Identify a policy as a distribution over actions for each possible state.\nDefine value functions for a state and action.\nDerive the Bellman equation for a value function.\nUnderstand how Bellman equations relate current and future values.\nDefine an optimal policy.\nDerive the Bellman optimality equation for a value function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#textbook-readings",
    "href": "07_mdp-2.html#textbook-readings",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.2 Textbook readings",
    "text": "7.2 Textbook readings\nRead Chapter 3.5-3.7 in Sutton and Barto (2018). Read it before continuing this module. You do not have to put to much focus on the examples. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#policies-and-value-functions",
    "href": "07_mdp-2.html#policies-and-value-functions",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.3 Policies and value functions",
    "text": "7.3 Policies and value functions\nA policy \\(\\pi\\) is a distribution over actions, given some state:\n\\[\\pi(a | s) = \\Pr(A_t = a | S_t = s).\\] Since the MDP is stationary the policy is time-independent, i.e. given a state, we choose the same action no matter the time-step. If \\(\\pi(a | s) = 1\\) for a single state, i.e. an action is chosen with probability one always then the policy is called deterministic. Otherwise a policy is called stochastic.\nGiven a policy we can define some value functions. The state-value function \\(v_\\pi(s)\\) denote the expected return starting from state \\(s\\) when following the policy \\(\\pi\\):\n\\[\n\\begin{align}\n  v_\\pi(s) &= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n  &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s].\n\\end{align}\n\\] Note the last equal sign comes from \\(G_t = R_{t+1} + \\gamma G_{t+1}\\).\nThe action-value function \\(q_\\pi(s, a)\\), denote the expected return starting from state \\(s\\), taking action \\(a\\) and from thereon following policy \\(\\pi\\):\n\\[\n\\begin{align}\n  q_\\pi(s, a) &= \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\\n  &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a].\n\\end{align}\n\\]\nThis action-value, also known as “q-value”, is very important, as it tells us directly what action to pick in a particular state. Given the definition of q-values, the state-value function is an average over the q-values of all actions we could take in that state:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_\\pi(s, a)\n\\tag{7.1}\\]\nA q-value (action-value) is equal to the expected reward \\(r(s,a)\\) that we get from choosing action \\(a\\) in state \\(s\\), plus a discounted amount of the average state-value of all the future states:\n\\[q_\\pi(s, a) = r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_\\pi(s')\\]\nJoining the equations, the state-value of a particular state \\(s\\) now becomes the sum of weighted state-values of all possible subsequent states \\(s'\\), where the weights are the policy probabilities:\n\\[\n\\begin{align}\n  v_\\pi(s) &= \\sum_{a \\in \\mathcal{A}}\\pi(a | s)q_\\pi(s, a) \\\\\n  &= \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_\\pi(s')\\right),\n\\end{align}\n\\tag{7.2}\\] which is known as the Bellman equation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#sec-mdp-opt",
    "href": "07_mdp-2.html#sec-mdp-opt",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.4 Optimal policies and value functions",
    "text": "7.4 Optimal policies and value functions\nThe objective function of an MDP can now be stated mathematically which is to find an optimal policy \\(\\pi_*\\) with state-value function:\n\\[v_*(s) = \\max_\\pi v_\\pi(s).\\] That is, a policy \\(\\pi'\\) is defined as better than policy \\(\\pi\\) if its expected return is higher for all states. Note the objective function is not a scalar here but if the agent start in state \\(s_0\\) then we may reformulate the objective function maximize the expected return to \\[v_*(s_0) = \\max_\\pi \\mathbb{E}_\\pi[G_0 | S_0 = s_0] = \\max_\\pi v_\\pi(s_0)\\]\nIf the MDP has the right properties (details are not given here), there exists an optimal deterministic policy \\(\\pi_*\\) which is better than or just as good as all other policies. For all such optimal policies (there may be more than one), we only need to find one optimal policy that have the optimal state-value function \\((v_*)\\).\nWe may rewrite \\(v_*(s)\\) using Equation 7.1: \\[\n\\begin{align}\n  v_*(s) &= \\max_\\pi v_\\pi(s) \\\\\n         &= \\max_\\pi \\sum_{a \\in \\mathcal{A}}\\pi(a|s)q_\\pi(s, a) \\\\\n         &= \\max_\\pi \\max_a q_\\pi(s, a)\\qquad \\text{(set $\\pi(a|s) = 1$ where $q_\\pi$ is maximal)} \\\\\n         &= \\max_a \\max_\\pi q_\\pi(s, a) \\\\\n         &= \\max_a q_*(s, a), \\\\\n\\end{align}\n\\] where the optimal q-value/action-value function \\((q_*)\\) is:\n\\[\n\\begin{align}\nq_*(s, a) &= \\max_\\pi q_\\pi(s, a) \\\\\n          &= r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_*(s') \\\\\n          &= r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\max_{a'} q_*(s', a').\n\\end{align}\n\\] This is the the Bellman optimality equation for \\(q_*\\) and the optimal policy is:\n\\[\n\\pi_*(a | s) =\n\\begin{cases}\n1 \\text{ if } a = \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\\n0 \\text { otherwise.}\n\\end{cases}\n\\] Or we may define a deterministic policy as \\[\n\\begin{align}\n\\pi_*(s) &= \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\\n         &= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_*(s')\\right).\n\\end{align}\n\\tag{7.3}\\]\nSimilar we can write the Bellman optimality equation for \\(v_*\\):\n\\[\n\\begin{align}\n  v_*(s) &= \\max_a q_*(s, a) \\\\\n         &= \\max_a r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_*(s')\n\\end{align}\n\\tag{7.4}\\]\nNote the Bellman equations define our state-value and q-value function, while the Bellman optimality equations define how to find the optimal value functions. Using $ v _*$, the optimal expected long-term return is turned into a quantity that is immediately available for each state. On the other hand, if we do not store \\(v_*\\), we can find \\(v_*\\) by a one-step-ahead search using \\(q_*\\), acting greedily.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#optimality-vs-approximation",
    "href": "07_mdp-2.html#optimality-vs-approximation",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.5 Optimality vs approximation",
    "text": "7.5 Optimality vs approximation\nIn Module 7.4 optimal policies and value functions was found; however solving the Bellman optimality equations can be expensive, e.g. if the number of states is huge. Consider a state \\(s = (x_1,\\ldots,x_n)\\) with state variables \\(x_i\\) each taking two possible values, then the number of states is \\(|\\mathcal{S}| = 2^n\\). That is, the state space grows exponentially with the number of state variables also known as the curse of dimensionality.\nLarge state or action spaces may happen in practice; moreover, they may also be continuous. As a result we need to approximate the value functions because calculation of optimality is too expensive. This is indeed what happens in RL where we approximate the expected return. Furthermore, often we focus on states with high encountering probability while allowing the agent to make sub-optimal decisions in states that have a low probability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#semi-mdps-non-fixed-time-length",
    "href": "07_mdp-2.html#semi-mdps-non-fixed-time-length",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.6 Semi-MDPs (non-fixed time length)",
    "text": "7.6 Semi-MDPs (non-fixed time length)\nSo far we have considered MDPs with a fixed length between each time-step. The model can be extended to MDPs with non-fixed time-lengths known as semi-MDPs. Let \\(l(s'|s,a)\\) denote the length of a time-step given that the system is in state \\(s\\), action \\(a\\) is chosen and makes a transition to state \\(s'\\). Then the discount rate over a time-step with length \\(l(s'|s,a)\\) is then\n\\[\\gamma(s'|s,a) = \\gamma^{l(s'|s,a)},\\]\nand the Bellman optimality equations becomes:\n\\[\nv_*(s) = \\max_a r(s,a) + \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\gamma(s'|s,a)  v_*(s'),\n\\]\nand \\[\nq_*(s, a) = r(s,a) + \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\gamma(s'|s,a) \\max_{a'} q_*(s', a').\n\\]\nThat is, the discount rate now is a part of the sum since it depends on the length which depends on the transition.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#summary",
    "href": "07_mdp-2.html#summary",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.7 Summary",
    "text": "7.7 Summary\nRead Chapter 3.8 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "07_mdp-2.html#sec-mdp-2-ex",
    "href": "07_mdp-2.html#sec-mdp-2-ex",
    "title": "7  Policies and value functions for MDPs",
    "section": "7.8 Exercises",
    "text": "7.8 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n7.8.1 Exercise - Optimal policy\n\n\n\n\n\n\n\n\nFigure 7.1: A simple MDP.\n\n\n\n\n\nConsider the transition diagram for an MDP shown in Fig. 7.1 with 3 states (white circles). The only decision to be made is that in the top state \\(s\\), where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies left and right. Which policy is optimal if \\(\\gamma = 0, 0.9\\) and \\(0.5\\)?\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nLet \\(\\pi_L\\) and \\(\\pi_R\\) denote the left and right policy, respectively. Recall the Bellman equation: \\[v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_\\pi(s')\\right).\\] For the left policy this reduces to \\[v_{\\pi_L}(s) = 1 + \\gamma(v_\\pi(s')) = 1 + \\gamma(0 + \\gamma v_{\\pi_L}(s)).\\] Isolating \\(v_{\\pi_L}(s)\\) gives us \\[v_{\\pi_L}(s) = 1/(1-\\gamma^2).\\] Similar for the right policy we get \\[v_{\\pi_R}(s) = 0 + \\gamma(v_\\pi(s')) = 0 + \\gamma(2 + \\gamma v_{\\pi_R}(s)).\\] Isolating \\(v_{\\pi_R}(s)\\) gives us \\[v_{\\pi_R}(s) = 2\\gamma/(1-\\gamma^2).\\] Now\n\nfor \\(\\gamma=0\\) we get \\(v_{\\pi_L}(s) = 1\\) and \\(v_{\\pi_R}(s) = 0\\), i.e. left policy optimal.\nfor \\(\\gamma=0.9\\) we get \\(v_{\\pi_L}(s) = 5.26\\) and \\(v_{\\pi_R}(s) = 9.47\\), i.e. right policy optimal.\nfor \\(\\gamma=0.5\\) we get \\(v_{\\pi_L}(s) = 1.33\\) and \\(v_{\\pi_R}(s) = 1.33\\), i.e. both policies optimal.\n\n\n\n\n\n\n7.8.2 Exercise - Car rental\nConsider a rental company with two locations, each with a capacity of 20 cars. Each day, customers arrive at each location to rent cars. If a car is available, it is rented out with a reward of $10. Otherwise the opportunity is lost. Cars become available for renting the day after they are returned.\nThe number of cars rental requests \\(D_i\\) at Location \\(i=1,2\\) are Poisson distributed with mean 3 and 4. Similar, the number of cars returned \\(H_i\\) at Location \\(i=1,2\\) are Poisson distributed with mean 3 and 2. Cars returned resulting in more cars than the capacity are lost (and thus disappear from the problem).\nTo ensure that cars are available where they are needed, they can be moved between the two locations overnight, at a cost of $2 per car. A maximum of five cars can be moved from one location to the other in one night.\nFormulate the problem as an finite MDP where the time-steps are days.\n\nDefine the state space (with states \\((x,y)\\)) equal the number of cars at each location at the end of the day.\n\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\n\\[\\mathcal{S} = \\{ (x,y) | 0 \\leq x \\leq 20, 0 \\leq y \\leq 20 \\}\\]\n\n\n\n\nDefine the action space equal the net numbers of cars moved from Location 1 to Location 2 overnight, i.e. negative if move from Location 2 to 1.\n\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\n\\[\\mathcal{A}(s) = \\{ a | -\\min(5,y,20-x) \\leq a \\leq min(5,x,20-y) \\}\\]\n\n\n\n\nCalculate the expected reward \\(r(s,a)\\).\n\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nThe reward equals the reward of rentals minus the cost of movements. Note we have \\(\\bar{x} = x - a\\) and \\(\\bar{y} = x + a\\) after movement. Hence \\[r(s,a) = \\mathbb{E}[10(\\min(D_1, \\bar{x}) + \\min(D_2, \\bar{y}) )-2 \\mid a \\mid]\\] where \\[\\mathbb{E}[\\min(D, z)] = \\sum_{i=0}^z i\\Pr(D = i) + (1-\\Pr(D\\leq z))z.\\]\n\n\n\nNote the inventory dynamics (number of cars) at each parking lot is independent of the other given an action \\(a\\). Let us consider Location 1 and assume that we are in state \\(x\\) and chose action \\(a\\). Then the number of cars after movement is \\(x - a\\) and after rental requests \\(x - a - \\min(D_1, x-a)\\). Next, the number of returned cars are added: \\(x - a - \\min(D_1, x-a) +  H_1\\). Finally, note that if this number is above 20 (parking lot capacity), then we only have 20 cars, i.e. the inventory dynamics (number of cars at the end of the day) is \\[X = \\min(20,  x-a - \\min(D_1, x-a) +  H_1))).\\]\n\nGive the inventory dynamics for Location 2.\n\n\n\n\n\n\n\nWarningSolution\n\n\n\n\n\nOnly difference is that cars moved to Location 2 is \\(a\\) (and not \\(-a\\)): \\[Y = \\min(20, y + a - \\min(D_2, y+a) + H_2)).\\]\n\n\n\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Policies and value functions for MDPs</span>"
    ]
  },
  {
    "objectID": "08_dp.html",
    "href": "08_dp.html",
    "title": "8  Dynamic programming",
    "section": "",
    "text": "8.1 Learning outcomes\nThe term Dynamic Programming (DP) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the principle of optimality. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations.\nDP do both policy evaluation (prediction) and control. Policy evaluation give us the value function \\(v_\\pi\\) given a policy \\(\\pi\\). Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations.\nTwo main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions.\nNote the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a “program”.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 2, 4, 6, 7, 8, 10 and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#learning-outcomes",
    "href": "08_dp.html#learning-outcomes",
    "title": "8  Dynamic programming",
    "section": "",
    "text": "Describe the distinction between policy evaluation and control.\nIdentify when DP can be applied, as well as its limitations.\nExplain and apply iterative policy evaluation for estimating state-values given a policy.\nInterpret the policy improvement theorem.\nExplain and apply policy iteration for finding an optimal policy.\nExplain and apply value iteration for finding an optimal policy.\nDescribe the ideas behind generalized policy iteration.\nInterpret the distinction between synchronous and asynchronous dynamic programming methods.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#textbook-readings",
    "href": "08_dp.html#textbook-readings",
    "title": "8  Dynamic programming",
    "section": "8.2 Textbook readings",
    "text": "8.2 Textbook readings\nFor this module, you will need to read Chapter 4-4.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#sec-dp-pe",
    "href": "08_dp.html#sec-dp-pe",
    "title": "8  Dynamic programming",
    "section": "8.3 Policy evaluation",
    "text": "8.3 Policy evaluation\nThe state-value function can be represented using the Bellman equation Equation 7.2: \\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_\\pi(s')\\right).            \n\\tag{8.1}\\]\nIf the dynamics are known perfectly, this becomes a system of \\(|\\mathcal{S}|\\) simultaneous linear equations in \\(|\\mathcal{S}|\\) unknowns \\(v_\\pi(s), s \\in \\mathcal{S}\\). This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations \\(v_0, v_1, v_2, \\ldots\\), with initial approximation \\(v_0\\) chosen arbitrarily e.g. \\(v_0(s) = 0 \\:  \\forall s\\) (ensuring terminal state = 0). We can use a sweep with the Bellman equation to update the values:\n\\[\\begin{equation}\nv_{k+1}(s) = \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_k(s')\\right)\n\\end{equation}\\]\nWe call this update an expected update because it is based on the expectation over all possible next states, rather than a sample of reward from the next state. This update will converge to \\(v_\\pi\\) after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold \\(\\theta\\) (see Fig. 8.1). Note the algorithm uses two arrays to maintain the state-value (\\(v\\) and \\(V\\)). Alternatively, a single array could be used that update values in place, i.e. \\(V\\) is used in place of \\(v\\). Hence, state-values are updated faster.\n\n\n\n\n\n\n\n\nFigure 8.1: Iterative policy evaluation (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#policy-improvement",
    "href": "08_dp.html#policy-improvement",
    "title": "8  Dynamic programming",
    "section": "8.4 Policy Improvement",
    "text": "8.4 Policy Improvement\nFrom the Bellman optimality equation Equation 7.4 we have\n\\[\n\\begin{align}\n\\pi_*(s) &= \\arg\\max_{a \\in \\mathcal{A}} q_*(s, a) \\\\\n         &= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_*(s')\\right).\n\\end{align}\n\\tag{8.2}\\] That is, a deterministic optimal policy can be found by choosing greedy the best action given the optimal value function. If we apply this greed action selection to the value function for a policy \\(\\pi\\) and pick the action with most \\(q\\): \\[\n\\begin{align}\n\\pi'(s) &= \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s, a) \\\\\n         &= \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_\\pi(s')\\right),\n\\end{align}\n\\tag{8.3}\\] then \\[\nq_\\pi(s, \\pi'(s)) \\geq q_\\pi(s, \\pi(s)) = v_\\pi(s) \\quad \\forall s \\in \\mathcal{S}.\n\\] Note if \\(\\pi'(s) = \\pi(s), \\forall s\\in\\mathcal{S}\\) then the Bellman optimality equation Equation 7.4 holds and \\(\\pi\\) must be optimal; Otherwise, \\[\n\\begin{align}\n  v_\\pi(s) &\\leq q_\\pi(s, \\pi'(s)) = \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi'(S_{t+1})) | S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma (R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})) | S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2}, \\pi'(S_{t+2})) | S_t = s] \\\\\n&\\leq \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...)) | S_t = s] \\\\\n&= v_{\\pi'}(s),\n\\end{align}\n\\] That is, policy \\(\\pi'\\) is strictly better than policy \\(\\pi\\) since there is at least one state \\(s\\) for which \\(v_{\\pi'}(s) &gt; v_\\pi(s)\\). We can formalize the above deductions in a theorem.\n\nLet \\(\\pi\\), \\(\\pi'\\) be any pair of deterministic policies, such that \\[\\begin{equation}\n    q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s) \\quad \\forall s \\in \\mathcal{S}.\n\\end{equation}\\] That is, \\(\\pi'\\) is as least as good as \\(\\pi\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#policy-iteration",
    "href": "08_dp.html#policy-iteration",
    "title": "8  Dynamic programming",
    "section": "8.5 Policy Iteration",
    "text": "8.5 Policy Iteration\nGiven the policy improvement theorem we can now improve policies iteratively until we find an optimal policy:\n\nPick an arbitrary initial policy \\(\\pi\\).\nGiven a policy \\(\\pi\\), estimate \\(v_\\pi(s)\\) via the policy evaluation algorithm.\nGenerate a new, improved policy \\(\\pi' \\geq \\pi\\) by greedily picking \\(\\pi' = \\text{greedy}(v_\\pi)\\) using Eq. Equation 8.3. If \\(\\pi'=\\pi\\) then stop (\\(\\pi_*\\) has been found); otherwise go to Step 2.\n\nThe algorithm is given in Fig. 8.2. The sequence of calculations will be: \\[\\pi_0 \\xrightarrow[]{E} v_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} v_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} v_{\\pi_2}  \\ldots \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} v_{*}\\] The number of steps of policy iteration needed to find the optimal policy are often low.\n\n\n\n\n\n\n\n\nFigure 8.2: Policy iteration (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#value-iteration",
    "href": "08_dp.html#value-iteration",
    "title": "8  Dynamic programming",
    "section": "8.6 Value Iteration",
    "text": "8.6 Value Iteration\nPolicy iteration requires full policy evaluation at each iteration step. This could be an computationally expensive process which requires may sweeps of the state space. In value iteration, the policy evaluation is stopped after one sweep of the state space. Value iteration is achieved by turning the Bellman optimality equation into an update rule: \\[\nv_{k+1}(s) = \\max_a \\left(r(s,a) + \\gamma\\sum_{s'} p(s'|s, a)v_k(s')\\right)\n\\] Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement, since it performs a greedy update while also evaluating the current policy. Also, it is important to understand that the value-iteration algorithm does not require a policy to work. No actions have to be chosen. Rather, the state-values are updated and after the last step of value-iteration the optimal policy \\(\\pi_*\\) is found:\n\\[\n\\pi_*(s) = \\arg\\max_{a \\in \\mathcal{A}} \\left(r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) v_*(s')\\right),\n\\] The algorithm is given in Fig. 8.3. Since we do not want an infinite number of iterations we introduce a threshold \\(\\theta\\). The sequence of calculations will be (where G denotes greedy action selection): \\[v_{0} \\xrightarrow[]{EI} v_{1} \\xrightarrow[]{EI} v_{2}  \\ldots \\xrightarrow[]{EI} v_{*} \\xrightarrow[]{G} \\pi_{*}\\]\n\n\n\n\n\n\n\n\nFigure 8.3: Value iteration (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#generalized-policy-iteration",
    "href": "08_dp.html#generalized-policy-iteration",
    "title": "8  Dynamic programming",
    "section": "8.7 Generalized policy iteration",
    "text": "8.7 Generalized policy iteration\nGeneralised Policy Iteration (GPI) is the process of letting policy evaluation and policy improvement interact, independent of granularity. For instance, improvement/evaluation can be performed by doing complete sweeps of the state space (policy iteration), or improve the state-value using a single sweep of the state space (value iteration). GPI can also do asynchronous updates of the state-value where states are updated individually, in any order. This can significantly improve computation. Examples on asynchronous DP are\n\nIn-place DP mentioned in Section Module 8.3 where instead of keeping a copy of the old and new value function in each value-iteration update, you can just update the value functions in-place. Hence asynchronous updates in other parts of the state-space will directly be affected resulting in faster updates.\nPrioritized sweeping where we keep track of how “effective” or “significant” updates to our state-values are. States where the updates are more significant are likely further away from converging to the optimal value. As such, we would like to update them first. For this, we would compute the Bellman error: \\[|v_{k+1}(s) - v_k(s)|,\\] and keep these values in a priority queue. You can then efficiently pop the top of it to always get the state you should update next.\nPrioritize local updates where you update nearby states given the current state, e.g. if your robot is in a particular region of the grid, it is much more important to update nearby states than faraway ones.\n\nGPI works and will convergence to the optimal policy and optimal value function if the states are visited (in theory) an infinite number of times. That is, you must explore the whole state space for GPI to work.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#sec-dp-storage",
    "href": "08_dp.html#sec-dp-storage",
    "title": "8  Dynamic programming",
    "section": "8.8 Example - Factory Storage",
    "text": "8.8 Example - Factory Storage\nLet us consider Exercise 6.6.4 where a factory has a storage tank with a capacity of 4 \\(\\mathrm{m}^{3}\\) for temporarily storing waste produced by the factory.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nDuring the lecture for this module we will work with the tutorial. We implement the DP algorithms. You may have a look at the section with the example before the lecture. Open the section:\n\nHave a look at the section and run all code cells.\nTry to understand the content.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#summary",
    "href": "08_dp.html#summary",
    "title": "8  Dynamic programming",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nRead Chapter 4.8 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "08_dp.html#exercises",
    "href": "08_dp.html#exercises",
    "title": "8  Dynamic programming",
    "section": "8.10 Exercises",
    "text": "8.10 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n8.10.1 Exercise - Gambler’s problem\nWe work with the gambler’s problem you modelled as an MDP in Exercise 6.6.3.\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n8.10.2 Exercise - Maintenance problem\nAt the beginning of each day a piece of equipment is inspected to reveal its actual working condition. The equipment will be found in one of the working conditions \\(s = 1,\\ldots, N\\) where the working condition \\(s\\) is better than the working condition \\(s+1\\).\nThe equipment deteriorates in time. If the present working condition is \\(s\\) and no repair is done, then at the beginning of the next day the equipment has working condition \\(s'\\) with probability \\(q_{ss'}\\). It is assumed that \\(q_{ss'}=0\\) for \\(s'&lt;s\\) and \\(\\sum_{s'\\geq s}q_{ss'}=1\\).\nThe working condition \\(s=N\\) represents a malfunction that requires an enforced repair taking two days. For the intermediate states \\(s\\) with \\(1&lt;s&lt;N\\) there is a choice between preventively repairing the equipment and letting the equipment operate for the present day. A preventive repair takes only one day. A repaired system has the working condition \\(s=1\\).\nThe cost of an enforced repair upon failure is \\(C_{f}\\) and the cost of a preemptive repair in working condition \\(s\\) is \\(C_{p,s}\\). We wish to determine a maintenance rule which minimizes the repair cost.\nThe problem can be formulated as an MDP. Since an enforced repair takes two days and the state of the system has to be defined at the beginning of each day, we need an auxiliary state for the situation in which an enforced repair is in progress already for one day.\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n8.10.3 Exercise - Car rental\nConsider the car rental problem in Exercise 7.8.2. Note the inventory dynamics (number of cars) at each parking lot is independent of the other given an action \\(a\\). Let us consider Location 1 and assume that we are in state \\(x\\) and chose action \\(a\\).\nThe reward equals the reward of rentals minus the cost of movements. Note we have \\(\\bar{x} = x - a\\) and \\(\\bar{y} = x + a\\) after movement. Hence \\[r(s,a) = \\mathbb{E}[10(\\min(D_1, \\bar{x}) + \\min(D_2, \\bar{y}) )-2\\mid a \\mid]\\] where \\[\\mathbb{E}[\\min(D, z)] = \\sum_{i=0}^{z-1} i\\Pr(D = i) + \\Pr(D \\geq z)z.\\]\nLet us have a look at the state transition, the number of cars after rental requests \\(\\bar{x} - \\min(D_1, \\bar{x})\\). Next, the number of returned cars are added: \\(\\bar{x} - \\min(D_1, \\bar{x}) +  H_1\\). Finally, note that if this number is above 20 (parking lot capacity), then we only have 20 cars, i.e. the inventory dynamics (number of cars at the end of the day) is \\[X = \\min(20,  \\bar{x} - \\min(D_1, \\bar{x}) +  H_1))).\\] Similar for Location 2, if \\(\\bar{y}= y+a\\) we have \\[Y = \\min(20, \\bar{y} - \\min(D_2, \\bar{y}) + H_2)).\\]\nSince the dynamics is independent given action a, the transition probabilities can be split: \\[ p((x',y') | (x,y), a) = p(x' | x, a) p(y' | y, a).\\]\nLet us consider Location 1. If \\(x' &lt; 20\\) then \\[\n\\begin{align}\n   p(x' | x, a) &= \\Pr(x' = x-a - \\min(D_1, x-a) +  H_1)\\\\\n                &= \\Pr(x' = \\bar{x} - \\min(D_1, \\bar{x}) +  H_1)\\\\\n                &= \\Pr(H_1 - \\min(D_1, \\bar{x}) = x' - \\bar{x}) \\\\\n                &= \\sum_{i=0}^{\\bar{x}} \\Pr(\\min(D_1, \\bar{x}) = i)\\Pr(H_1 = x' - \\bar{x} + i) \\\\\n                &= \\sum_{i=0}^{\\bar{x}}\\left( (\\mathbf{1}_{(i&lt;\\bar{x})} \\Pr(D_1 = i) + \\mathbf{1}_{(i=\\bar{x})} \\Pr(D_1 \\geq \\bar{x}))\\Pr(H_1 = x' - \\bar{x} + i)\\right) \\\\\n                &= p(x' | \\bar{x}).\n\\end{align}\n\\]\nIf \\(x' = 20\\) then \\[\n\\begin{align}\n   p(x' | x, a) &= \\Pr(20 \\leq \\bar{x} - \\min(D_1, \\bar{x}) +  H_1)\\\\\n                &= \\Pr(H_1 - \\min(D_1, \\bar{x}) \\geq 20 - \\bar{x}) \\\\\n                &= \\sum_{i=0}^{\\bar{x}} \\Pr(\\min(D_1, \\bar{x}) = i)\\Pr(H_1 \\geq 20 - \\bar{x} + i) \\\\\n                &= \\sum_{i=0}^{\\bar{x}}\\left( (\\mathbf{1}_{(i&lt;\\bar{x})} \\Pr(D_1 =i) + \\mathbf{1}_{(i=\\bar{x})} \\Pr(D_1 \\geq \\bar{x}))\\Pr(H_1 \\geq 20 - \\bar{x} + i)\\right)\\\\\n                &= p(x' = 20 | \\bar{x}).\n\\end{align}\n\\]\nSimilar for Location 2. That is we need to calculate and store \\(p(x'| \\bar{x})\\) and \\(p(y'| \\bar{y})\\) to find \\[ p((x',y') | (x,y), a) = p(x' | \\bar{x} = x-a) p(y' | \\bar{y} = y+a).\\]\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Dynamic programming</span>"
    ]
  },
  {
    "objectID": "09_mc.html",
    "href": "09_mc.html",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "",
    "text": "9.1 Learning outcomes\nThe term “Monte Carlo” (MC) is often used for an estimation method which involves a random component. MC methods of RL learn state and action values by sampling and averaging returns. MC do not use dynamics where we estimate the value in the current state using the value in the next state (like in dynamic programming). Instead the MC methods estimate the values by considering different sample-paths (state, action and reward realizations). Compared to a Markov decision process, MC methods are model-free since they not require full knowledge of the transition probabilities and rewards (a model of the environment) instead MC methods learn the value function directly from experience. Often though, the sample-path is generated using simulation, i.e. some knowledge about the environment is given, but it is only used to generate sample transitions. For instance, consider an MDP model for the game Blackjack. Here calculating all the transition probabilities may be tedious and error-prone in terms of coding and numerical precision. Instead we can simulate a game (a sample-path) and use the simulations to evaluate/predict the value function of a policy and then use control to find a good policy. That is, we still use a generalised policy iteration framework, but instead of computing the value function using the MDP model a priori, we learn it from experience.\nMC methods can be used for processes with episodes, i.e. where there is a terminal state. This reduces the length of the sample-path and the value of the states visited on the path can be updated based on the reward received.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 4, 9 and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#learning-outcomes",
    "href": "09_mc.html#learning-outcomes",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "",
    "text": "Identify the difference between model-based and model-free RL.\nIdentify problems that can be solved using Monte-Carlo methods.\nDescribe how MC methods can be used to estimate value functions from sample data.\nDo MC prediction to estimate the value function for a given policy.\nExplain why it is important to maintain exploration in MC algorithms.\nDo policy improvement (control) using MC in a generalized policy improvement algorithm.\nCompare different ways of exploring the state-action space.\nArgue why off-policy learning can help deal with the exploration problem.\nUse importance sampling to estimate the expected value of a target distribution using samples from a different distribution.\nUse importance sampling in off-policy learning to predict the value-function of a target policy.\nExplain how to modify the MC prediction and improvement algorithm for off-policy learning.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#textbook-readings",
    "href": "09_mc.html#textbook-readings",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.2 Textbook readings",
    "text": "9.2 Textbook readings\nFor this module, you will need to read Chapter 5-5.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#mc-prediction-evaluation",
    "href": "09_mc.html#mc-prediction-evaluation",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.3 MC prediction (evaluation)",
    "text": "9.3 MC prediction (evaluation)\nGiven a policy \\(\\pi\\), we want to estimate the state-value function. Recall that the state value function is \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s].\n\\] where the return is \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1}\n\\] Now given policy \\(\\pi\\) and a sample-path (episode) \\(S_0, A_0, R_1, S_1, A_1, \\ldots, S_{T-1}, A_{T-1}, R_T\\) ending in the terminal state at time \\(T\\), we can calculate the realized return for each state in the sample-path. Each time we have a new sample-path a new realized return for the states is given and the average for the returns in a state is an estimate of the state-value. With enough observations, the sample average converges to the true state-value under the policy \\(\\pi\\).\nGiven a policy \\(\\pi\\) and a set of sample-paths, there are two ways to estimate the state values \\(v_\\pi(s)\\):\n\nFirst visit MC: average returns from first visit to state \\(s\\).\nEvery visit MC: average returns following every visit to state \\(s\\).\n\nFirst visit MC generates iid estimates of \\(v_\\pi(s)\\) with finite variance, so the sequence of estimates converges to the expected value by the law of large numbers as the number of observations grow. Every visit MC does not generate independent estimates, but still converges.\nAn algorithm for first visit MC is given in Fig. 9.1. The state-value estimate is stored in a vector \\(V\\) and the returns for each state in a list. Given a sample-path we add the return to the states on the path by scanning the path backwards and updating \\(G\\). Note since the algorithm considers first visit MC, a check of occurrence of the state earlier in the path done. If this check is dropped, we have a every visit MC algorithm instead. Moreover, the computation needed to update the state-value does not depend on the size of the process/MDP but only of the length of the sample-path.\n\n\n\n\n\n\n\n\nFigure 9.1: MC policy prediction (Sutton and Barto 2018).\n\n\n\n\n\nThe algorithm maintains a list of all returns for each state which may require a lot of memory. Instead as incremental update of \\(V\\) can be done. Adapting Equation 5.1, we have that the sample average can be updated using:\n\\[\n  V(s) \\leftarrow V(s) + \\frac{1}{n} \\left[G - V(s)\\right].\n\\] where \\(n\\) denote the number of realized returns found for state \\(s\\) and \\(G\\) the current realized return. The state-value vector must be initialized to zero and a vector counting the number of returns found for each state must be stored.\n\n\n9.3.1 MC prediction of action-values\nWith a model of the environment we only need to estimate the state-value function, since it is easy to determine the policy from the state-values using the Bellman optimality equations Equation 7.3. However, if we do not know the expected reward and transition probabilities state values are not enough. In that case, it is useful to estimate action-values since the optimal policy can be found using \\(q_*\\) (see Equation 7.3). To find \\(q_*\\), we first need to predict action-values for a policy \\(\\pi\\). This is essentially the same as for state-values, only we now talk about state-action pairs being visited, i.e. taking action \\(a\\) in state \\(s\\) instead.\nIf \\(\\pi\\) is deterministic, then we will only estimate the values of actions that \\(\\pi\\) dictates. Therefore some exploration are needed in order to have estimates for all action-values. Two possibilities are:\n\nMake \\(\\pi\\) stochastic, e.g. \\(\\varepsilon\\)-soft that that have non-zero probability of selecting each state-action pair.\nUse exploring starts, which specifies that every state-action pair has a non-zero probability of being selected as the starting state of a sample-path.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#mc-control-improvement",
    "href": "09_mc.html#mc-control-improvement",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.4 MC control (improvement)",
    "text": "9.4 MC control (improvement)\nWe are now ready to formulate a generalized policy iteration (GPI) algorithm using MC to predict the action-values \\(q(s,a)\\). Policy improvement is done by selecting the next policy greedy with respect to the action-value function: \\[\n    \\pi(s) = \\arg\\max_a q(s, a).\n\\] That is, we generate a sequence of policies and action-value functions \\[\\pi_0 \\xrightarrow[]{E} q_{\\pi_0} \\xrightarrow[]{I} \\pi_1 \\xrightarrow[]{E} q_{\\pi_1} \\xrightarrow[]{I} \\pi_2 \\xrightarrow[]{E} q_{\\pi_2} \\xrightarrow[]{I} \\ldots \\xrightarrow[]{I} \\pi_* \\xrightarrow[]{E} q_{*}.\\] Hence the policy improvement theorem applies for all \\(s \\in \\mathcal{S}\\):\n\\[\\begin{align}\n    q_{\\pi_k}(s, a=\\pi_{k+1}(s)) &= q_{\\pi_k}(s, \\arg\\max_a q_{\\pi_k}(s, a)) \\\\\n                    &= \\max_a q_{\\pi_k}(s, a) \\\\\n                    &\\geq q_{\\pi_k}(s, \\pi_k(s))\\\\\n                    &= v_{\\pi_k}(s)\n\\end{align}\\]\nThat is, \\(\\pi_{k+1}\\) is better than \\(\\pi_k\\) or optimal.\nIt is important to understand the major difference between model-based GPI (remember that a model means the transition probability matrix and reward distribution are known) and model-free GPI. We cannot simply use a 100% greedy strategy all the time, since all our action-values are estimates. As such, we now need to introduce an element of exploration into our algorithm to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy:\n\nInfinite exploration: all state-action \\((s,a)\\) pairs should be explored infinitely many times as the number of iterations go to infinity (in the limit), i.e. as the number of iterations \\(k\\) goes to infinity the number of visits \\(n_k\\) does too \\[\\lim_{k\\rightarrow\\infty} n_k(s, a) = \\infty.\\]\nGreedy in the limit: while we maintain infinite exploration, we do eventually need to converge to the optimal policy: \\[\\lim_{k\\rightarrow\\infty} \\pi_k(a|s) = 1 \\text{ for } a = \\arg\\max_a q(s, a).\\]\n\n\n9.4.1 GPI with exploring starts\nAn algorithm using exploring starts and first visit MC is given in Fig. 9.2. It satisfies the convergence properties and and incremental implementation can be used to update \\(Q\\). Note that to predict the action-values for a policy, we in general need a large number of sample-paths. However, much like we did with value iteration, we do not need to fully evaluate the value function for a given policy. Instead we can merely move the value toward the correct value and then switch to policy improvement thereafter. To stop the algorithm from having infinitely many sample-paths we may stop the algorithm once the \\(q_{\\pi_k}\\) stop moving within a certain error.\n\n\n\n\n\n\n\n\nFigure 9.2: GPI using MC policy prediction with exploring starts (Sutton and Barto 2018).\n\n\n\n\n\n\n\n9.4.2 GPI using \\(\\epsilon\\)-soft policies\nNote by using exploring starts in Fig. 9.2, the ‘infinite exploration’ convergence assumption is satisfied. However exploring starts may be hard to use in practice. Another approach to ensure infinite exploration is to use a soft policy, i.e. assign a non-zero probability to each possible action in a state. An on-policy algorithm using \\(\\epsilon\\)-greedy policies is given in Fig. 9.3. Here we put probability \\(1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\) on the maximal action and \\(\\frac{\\varepsilon}{|\\mathcal{A}(s)|}\\) on each of the others. Note using \\(\\epsilon\\)-greedy policy selection will improve the current policy; otherwise we have found best policy amongst the \\(\\epsilon\\)-soft policies. If we want to find the optimal policy we have to ensure the ‘greedy in the limit’ convergence assumption. This can be done by decreasing \\(\\epsilon\\) as the number of iterations increase (e.g. \\(\\epsilon = 1/k\\)).\n\n\n\n\n\n\n\n\nFigure 9.3: On-policy GPI using MC policy prediction (Sutton and Barto 2018).\n\n\n\n\n\nAn incremental approach for updating \\(Q\\) can be used by storing the number of times \\(n_a\\), action \\(a\\) has been visited in state \\(s\\) and then update \\(Q(s,a)\\) using \\[Q_{n+1} = Q_n + \\frac{1}{n_a}(G-Q_n),\\] where \\(Q_n\\) denote the previous value.\nFinally, the algorithm in Fig. 9.3 do not mention how to find the start state of an episode. In general all states that we want to approximate must be used as start state.\n\n\n9.4.3 GPI using upper-confience bound action selection\nGPI using exploring starts or \\(\\epsilon\\)-soft policies may be slow. Often speed-ups can be done by using e.g. upper-confidence bounds (UCB) for action selection. Recall from Module 5 that UCB select actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. That is, the next action \\(a'\\) given a state \\(s\\) is selected using: \\[\n    a' = \\arg\\max_a \\left(Q(s,a) + c\\sqrt{\\frac{\\ln n_s}{n_a}}\\right),\n\\] where \\(n_s\\) denote the number of times state \\(s\\) has been visited and \\(n_a\\) denote the number of times action \\(a\\) has been visited (both numbers must be stored). The parameter \\(c&gt;0\\) controls the degree of exploration. Higher \\(c\\) results in more weight on the uncertainty. However, one problem with UCB is that it is hard to decide on which value of \\(c\\) to use in advance.\n\n\n\n9.4.4 Example - Seasonal inventory and sales planning\nIn the following example, we try to implement an algorithm that uses generalised policy iteration with every-visit estimation using epsilon-greedy action selection.\nWe consider seasonal product such as garden furnitures. Assume that the maximum inventory level is \\(Q\\) items, i.e. we can buy at most \\(Q\\) items at the start of the season for a price of $14. The product can be sold for at most \\(T\\) weeks and at the end of the period (week \\(T\\)), the remaining inventory is sold to an outlet store for $5 per item.\nThe demand depends on the sales price which based on historic observations is assumed in the interval \\([10,25].\\) In general a higher sales price result in a lower demand. Moreover, in the first half part of the season the demand is on average 10% higher given a fixed sales price compared to the last half part of the season. Historic observed demands can be seen in Fig. 9.4.\n\n\n\n\n\n\n\n\nFigure 9.4: Observed demands given price (scaled based on number of observations).\n\n\n\n\n\nLet \\(s = (q,t)\\) denote the state of the system in the start of a week, where \\(q\\) is the inventory and \\(t\\) the week number. Then the state space is \\[\\mathcal{S} = \\{ s = (q,t) | 1 \\leq q \\leq Q, 1 \\leq t \\leq T \\} \\cup \\{ 0 \\},\\] where state \\(s = 0\\) denote the terminal state (inventory empty). Let us limit us to actions \\[\\mathcal{A}(q,t) = \\{ 10,15,20,25 \\}, \\mathcal{A}(0) = \\{ d \\}, \\] where action \\(a\\) denote the price and \\(d\\) denote the dummy action with deterministic transition to state \\(0\\).\nThe inventory dynamics for transitions not to the terminal state are \\[t' = t + 1,\\] \\[q' = q - min(q, D),\\] where \\(D\\) denote the demand. Moreover, if \\(t = T\\) or \\(q' = 0\\), then a transition to the terminal state happens.\nFor \\(t=1\\) the reward of an state \\((q,t)\\) is sales price times the number of sold items minus the purchase cost. For \\(1&lt;t&lt;T\\) the reward is sales price times the number of sold (we assume an inventory cost of zero), while for \\(t=T\\) the reward is the scrap price times the inventory.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nDuring the lecture for example, we will work with the notebook. We implement the MC algorithms. Have a look at the notebook and try to understand the content.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#sec-mc-off-policy",
    "href": "09_mc.html#sec-mc-off-policy",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.5 Off-policy MC prediction",
    "text": "9.5 Off-policy MC prediction\nUntil now we have only considered what is denoted on-policy algorithms for finding the optimal policy. Here we both evaluate or improve the policy that is used to make decisions. To ensure infinite exploration we use for instance exploring starts or \\(\\epsilon\\)-soft policies. Off-policy methods use a different approach by considering two policies: a policy \\(b\\) used to generate the sample-path (behaviour policy) and a policy \\(\\pi\\) that is learned for control (target policy). We update the target policy using the sample-paths from the behaviour policy. The behaviour policy explores the environment for us during training and must ensure infinite exploration. Moreover, the coverage assumption must be satisfied: \\[\\pi(a|s) &gt; 0 \\rightarrow b(a|s) &gt; 0\\] That is, every action in \\(\\pi\\) must also be taken, at least occasionally, by \\(b\\). Put differently, to learn \\(\\pi\\) we must sample paths that occur when using \\(\\pi\\). Note target policy \\(\\pi\\) may be deterministic by using greedy selection with respect to action-value estimates (greedy in the limit satisfied).\nOff-policy learning methods are powerful and more general than on-policy methods (on-policy methods being a special case of off-policy where target and behaviour policies are the same). They can be used to learn from data generated by a conventional non-learning controller or from a human expert.\nBut how do we estimate the expected return using the target policy when we only have sample-paths from the behaviour policy? For this we need to introduce importance sampling, a general technique for estimating expected values under one distribution given samples from another. Let us first explain it using two distributions \\(a\\) and \\(b\\) where we want to estimated the mean of \\(a\\) given data/samples from \\(b\\), then \\[\n\\begin{align}\n  \\mathbb{E}_{a}[X] &= \\sum_{x\\in X} a(x)x \\\\\n  &= \\sum_{x\\in X} a(x)\\frac{b(x)}{b(x)}x \\\\\n  &= \\sum_{x\\in X} b(x)\\frac{a(x)}{b(x)}x \\\\\n  &= \\sum_{x\\in X} b(x)\\rho(x)x \\\\\n  &= \\mathbb{E}_{b}\\left[\\rho(X)X\\right].\n\\end{align}\n\\] Hence to the mean of \\(a\\) can be found by finding the mean of \\(\\rho(X)X\\) where \\(X\\) is has a \\(b\\) distribution and \\(\\rho(x) = a(x)/b(x)\\) denote the importance sampling ratio. Note given samples \\((x_1,\\ldots,x_n)\\) from \\(b\\) we then can calculate the sample average using \\[\n\\begin{align}\n  \\mathbb{E}_{a}[X] &= \\mathbb{E}_{b}\\left[\\rho(X)X\\right] \\\\\n  &\\approx \\frac{1}{n}\\sum_{i = 1}^n \\rho(x_i)x_i \\\\\n\\end{align}\n\\tag{9.1}\\]\n\nNow let us use importance sampling on the target policy \\(\\pi\\) and behaviour policy \\(b\\). Given state \\(S_t\\) and sample path, we want to find \\[v_\\pi(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s] = \\mathbb{E}_{b}[\\rho(G_t)G_t|S_t = s],\\] or since we base our estimates on sample-paths, we are in fact interested in estimating the action-values \\[q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] = \\mathbb{E}_{b}[\\rho(G_t)G_t|S_t = s, A_t = a].\\] For this we need the importance sampling ratio given a certain sample-path \\(S_t, A_t, R_{t+1}, \\ldots, R_T, S_T\\) with return \\(G_t\\): \\[\n\\begin{align}\n    \\rho(G_t) &= \\frac{\\Pr{}(S_t, A_t, \\dots S_T| S_t = s, A_t = a, \\pi)}{\\Pr{}(S_t, A_t, \\dots, S_T)| S_t = s, A_t = a, b)} \\\\\n                 &= \\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)\\Pr{}(S_{k+1}|S_k, A_k)}{\\prod_{k=t}^{T-1}b(A_k|S_k)\\Pr{}(S_{k+1}|S_k, A_k)}\\\\\n                 &=\\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}.\n\\end{align}\n\\tag{9.2}\\] Note the transition probabilities cancel out, i.e. the ratio does not depend on the MDP dynamics by only the policies. Moreover, importance sampling ratios are only non-zero for sample-paths where the target-policy has non-zero probability of acting exactly like the behaviour policy \\(b\\). So, if the behaviour policy takes 10 steps in an sample-path, each of these 10 steps have to have been possible by the target policy, else \\(\\pi(a|s) = 0\\) and \\(\\rho_{t:T-1} = 0\\).\nWe can now approx. \\(q_\\pi(s,a)\\) by rewriting Equation 9.1 for \\(\\pi\\) given returns from \\(b\\) to \\[\n    q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\approx \\frac{1}{n} \\sum_{i = 1}^n \\rho_iG_i,\n\\tag{9.3}\\] where we assume that given the sample-paths (episodes), have \\(n\\) observations of the return \\((G_1, \\ldots, G_n)\\) in state \\(s\\) taking action \\(a\\) with the importance sampling ratio \\(\\rho_i\\) calculated using Eq. Equation 9.2. As a result if we consider the prediction algorithm in Fig. 9.1 it must be modified by:\n\nGenerate an sample-path using policy \\(b\\) instead of \\(\\pi\\).\nAdd a variable W representing the importance sampling ratio which must be set to 1 on line containing \\(G \\leftarrow 0\\).\nModify line \\(G \\leftarrow \\gamma G + R_{t+1}\\) to \\(G \\leftarrow \\gamma WG + R_{t+1}\\) since we now need to multiply with the importance sampling ratio.\nAdd a line after the last with \\(W \\leftarrow W \\pi(A_t|S_t)/b(A_t|S_t)\\), i.e. we update the importance sampling ratio.\nNote if \\(\\pi(A_t|S_t) = 0\\) then we may stop the inner loop earlier (\\(W=0\\) for the remaining \\(t\\)).\nFinally, an incremental update of \\(V\\) can be done having a vector counting the number of of returns found for each state. Then the incremental update is \\[\nV(s) \\leftarrow V(s) + \\frac{1}{n} \\left[WG - V(s)\\right].\n\\tag{9.4}\\] where \\(n\\) denote the number of realized returns found for state \\(s\\) and \\(G\\) the current realized return.\n\n\n9.5.1 Weighted importance sampling\nWhen using a sample average the importance sampling method is called ordinary importance sampling. Ordinary importance sampling may result in a high variance which is not good. As a result we may use other weights and instead of Eq. Equation 9.3 use the estimate (weighted importance sampling): \\[\n    q_\\pi(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\approx \\frac{1}{\\sum_{i = 1}^n \\rho_i} \\sum_{i = 1}^n \\rho_iG_i.\n\\] An incremental update then becomes:\n\\[\n\\begin{align}\n    q_\\pi(s,a) &\\approx V_{n+1} \\\\\n    &= \\frac{1}{\\sum_{i = 1}^n \\rho_i} \\sum_{i = 1}^n \\rho_iG_i \\\\\n    &= \\frac{1}{C_n} \\sum_{i = 1}^n W_iG_i \\\\\n    &= \\frac{1}{C_n} (W_nG_n + C_{n-1}\\frac{1}{C_{n-1}} \\sum_{i = 1}^{n-1} W_iG_i) \\\\\n    &= \\frac{1}{C_n} (W_nG_n + C_{n-1}V_n) \\\\\n    &= \\frac{1}{C_n} (W_nG_n + (C_{n} - W_{n}) V_n) \\\\\n    &= \\frac{1}{C_n} (W_nG_n + C_{n}V_n - W_{n} V_n) \\\\\n    &= V_n + \\frac{W_n}{C_n} (G_n  - V_n),\n\\end{align}\n\\tag{9.5}\\] where \\(C_n = \\sum_{i = 1}^n \\rho_i\\) is the sum of the ratios and and \\(W_n\\) the ratio for the n’th return. Using weighted importance sampling gives a smaller variance and hence faster convergence. An off-policy prediction algorithm using weighted importance sampling and incremental updates is given in Fig. 9.5.\n\n\n\n\n\n\n\n\nFigure 9.5: Off-policy MC prediction (Sutton and Barto 2018).\n\n\n\n\n\nNote both Equation 9.4 and Equation 9.5 follows the general incremental formula: \\[\\begin{equation}\nNew Estimate \\leftarrow Old Estimate + Step Size \\left[Observation - Old Estimate \\right].\n\\end{equation}\\] For ordinary importance sampling the step-size is \\(1/n\\) and for weighted importance sampling the step-size is \\(W_n/C_n\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#off-policy-control-improvement",
    "href": "09_mc.html#off-policy-control-improvement",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.6 Off-policy control (improvement)",
    "text": "9.6 Off-policy control (improvement)\nHaving discussed a framework for off-policy MC prediction, we can now give a GPI algorithm for off-policy MC control that estimate \\(\\pi_*\\) and \\(q_*\\) by using rewards obtained through behaviour policy \\(b\\). We will focus on using weighted importance sampling with incremental updates. The algorithm is given in Fig. 9.6. The target policy \\(\\pi\\) is the greedy policy with respect to \\(Q\\), which is an estimate of \\(q_\\pi\\). This algorithm converges to \\(q_\\pi\\) as long as an infinite number of returns are observed for each state-action pair. This can be achieved by making \\(b\\) \\(\\varepsilon\\)-soft. The policy \\(\\pi\\) converges to \\(\\pi_*\\) at all encountered states even if \\(b\\) changes (to another \\(\\varepsilon\\)-soft policy) between or within sample-paths. Note we exit the inner loop if \\(A_t \\neq \\pi(S_t)\\) which implies \\(W=0\\).\n\n\n\n\n\n\n\n\nFigure 9.6: Off-policy GPI (Sutton and Barto 2018).\n\n\n\n\n\nNotice that this policy only learns from sample-paths in which \\(b\\) selects only greedy actions after some timestep. This can greatly slow learning.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#summary",
    "href": "09_mc.html#summary",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.7 Summary",
    "text": "9.7 Summary\nRead Chapter 5.10 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "09_mc.html#exercises",
    "href": "09_mc.html#exercises",
    "title": "9  Monte Carlo methods for prediction and control",
    "section": "9.8 Exercises",
    "text": "9.8 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n9.8.1 Exercise - Seasonal inventory and sales planning\nConsider the seasonal product in Example 9.4.4 and this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Monte Carlo methods for prediction and control</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html",
    "href": "10_td-pred.html",
    "title": "10  Temporal difference methods for prediction",
    "section": "",
    "text": "10.1 Learning outcomes\nOne of the most fundamental concepts in reinforcement learning is temporal difference (TD) learning. TD learning is a combination of Monte Carlo (MC) and dynamic programming (DP) ideas: Like MC, TD can predict using a model-free environment and learn from experience. Like DP, TD update estimates based on other learned estimates, without waiting for a final outcome (bootstrap). That is, TD can learn on-line and do not need to wait until the whole sample-path is found. TD in general learn more efficiently than MC due to bootstrapping. In this module prediction using TD is considered.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 4, 6, 9, and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#learning-outcomes",
    "href": "10_td-pred.html#learning-outcomes",
    "title": "10  Temporal difference methods for prediction",
    "section": "",
    "text": "Describe what Temporal Difference (TD) learning is.\nFormulate the incremental update formula for TD learning.\nDefine the temporal-difference error.\nInterpret the role of a fixed step-size.\nIdentify key advantages of TD methods over DP and MC methods.\nExplain the TD(0) prediction algorithm.\nUnderstand the benefits of learning online with TD compared to MC methods.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#textbook-readings",
    "href": "10_td-pred.html#textbook-readings",
    "title": "10  Temporal difference methods for prediction",
    "section": "10.2 Textbook readings",
    "text": "10.2 Textbook readings\nFor this module, you will need to read Chapter 6-6.3 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#what-is-td-learning",
    "href": "10_td-pred.html#what-is-td-learning",
    "title": "10  Temporal difference methods for prediction",
    "section": "10.3 What is TD learning?",
    "text": "10.3 What is TD learning?\nGiven a policy \\(\\pi\\), we want to estimate the state-value function. Recall that the state value function is \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s].\n\\] where the return is \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} = R_{t+1} + \\gamma G_{t+1}\n\\]\nLet \\(V\\) denote the state-value estimate. Under MC prediction we used an incremental update formula: \\[\n  V(S_t) \\leftarrow V(S_t) + \\alpha_n\\left[G_t - V(S_t)\\right],\n\\] where \\(n\\) denote the number of observations and \\(\\alpha_n\\) the step-size. Different values of \\(\\alpha_n\\) was discussed in Module 9. Here we assumed a stationary environment (state set, transition probabilities etc. is the same for each stage \\(t\\)) e.g. for the sample average \\(\\alpha_n = 1/n.\\) If the environment is non-stationary (e.g. transition probabilities change over time) then a fixed step-size may be appropriate. Let us for the remaining of this module consider a non-stationary process with fixed step-size: \\[\n  V(S_t) \\leftarrow V(S_t) + \\alpha\\left[G_t - V(S_t)\\right],\n\\]\nNote as pointed out in Module 5.5, a fixed step-size corresponds to a weighted average of the past observed returns and the initial estimate of \\(S_t\\): \\[\n\\begin{align}\nV_{n+1} &= V_n +\\alpha \\left[G_n - V_n\\right] \\nonumber \\\\\n&= \\alpha G_n + (1 - \\alpha)V_n \\nonumber \\\\\n&= \\alpha G_n + (1 - \\alpha)[\\alpha G_{n-1} + (1 - \\alpha)V_{n-1}] \\nonumber \\\\\n&= \\alpha G_n + (1 - \\alpha)\\alpha G_{n-1} + (1 - \\alpha)^2 V_{n-1}  \\nonumber \\\\\n& \\vdots \\nonumber \\\\\n&= (1-\\alpha)^n V_1 + \\sum_{i=1}^{n} \\alpha (1 - \\alpha)^{n-i} G_i \\\\\n\\end{align}\n\\] That is, a larger weight is used for recent observations compared to old observations.\nFor MC prediction we needed the sample path to get the realized return \\(G_t\\). However, since \\[\n\\begin{align}\nv_\\pi(s) &= \\mathbb{E}_\\pi[G_t | S_t = s] \\\\\n         &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n         &= \\mathbb{E}_\\pi[R_{t+1}| S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s] \\\\\n         &= \\mathbb{E}_\\pi[R_{t+1}| S_t = s] + \\gamma v_\\pi(S_{t+1}),\n\\end{align}\n\\] then, given a realized reward \\(R_{t+1}\\), an estimate for the return \\(G_t\\) is \\(R_{t+1} + \\gamma V(S_{t+1})\\) and the incremental update becomes: \\[\n  V(S_t) \\leftarrow V(S_t) + \\alpha\\left[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\right].\n\\tag{10.1}\\] As a result, we do not have to generate a whole sample-path (as for MC) for updating the state-value estimate of \\(s = S_t\\) to \\(V(S_t)\\). Instead we only have to wait until the next state is observed and update the estimate of \\(S_t\\) given the estimate of the next state \\(S_{t+1}\\). As the estimate of \\(S_{t+1}\\) improve the estimate of \\(S_t\\) also improve. The incremental update in Equation 10.1 is called TD(0) or one-step TD because it use a one-step lookahead to update the estimate. Note updating the estimates using TD resembles the way we did for DP: \\[\nV(s = S_t) \\leftarrow \\sum_{a \\in \\mathcal{A}}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}} p(s' | s, a) V(s')\\right)\n\\] Here we updated the value by considering the expectation of all the next states. This was possible since we had a model. Now, by using TD, we do not need a model to estimate the state-value.\nThe term \\[\n\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t),\n\\] is denoted the temporal difference error (TD error) since it is the difference between the current estimate \\(V(S_t)\\) and the better estimate \\(R_{t+1} + \\gamma V(S_{t+1})\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#td-prediction",
    "href": "10_td-pred.html#td-prediction",
    "title": "10  Temporal difference methods for prediction",
    "section": "10.4 TD prediction",
    "text": "10.4 TD prediction\nWe can now formulate a TD(0) algorithm for predicting state-values of a policy (see Fig. 10.1). No stopping criterion is given but could stop when small differences in state-values are observed.\n\n\n\n\n\n\n\n\nFigure 10.1: TD(0) policy prediction (Sutton and Barto 2018).\n\n\n\n\n\nThe algorithm is given for a process with episodes; however, also works for continuing processes. In this case the inner loop runs over an infinite number of time-steps.\n\n10.4.1 TD prediction for action-values\nLater we will use TD to for improving the policy (control). Since we do not have a model we need to estimate action-values instead and the optimal policy can be found using \\(q_*\\) (see Equation 7.3). To find \\(q_*\\), we first need to predict action-values for a policy \\(\\pi\\) and the incremental update Equation 10.1 must be modified to use \\(Q\\) values: \\[\n  Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\left[R_{t+1} + \\gamma Q(S_{t+1}, A_t) - Q(S_t, A_t)\\right].\n\\]\nNote given a policy \\(\\pi\\) you need to know \\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}\\) or short SARSA before you can make an update. This acronym is used to name the SARSA algorithm for control in Module 11. Note to ensure exploration of all action-values we need e.g. an \\(\\epsilon\\)-soft behavioral policy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#benefits-of-td-methods",
    "href": "10_td-pred.html#benefits-of-td-methods",
    "title": "10  Temporal difference methods for prediction",
    "section": "10.5 Benefits of TD methods",
    "text": "10.5 Benefits of TD methods\nLet us try to summarize the benefits of TD prediction\n\nTD methods do not require a model of the environment (compared to DP).\nTD methods can be implemented online, which can speed convergence (compared to MC methods which must wait until the end of the sample-path).\nTD methods learn from all actions, whereas MC methods require the sample-path to have a tail equal to the target policy.\nTD methods do converge on the value function with a sufficiently small step-size parameter, or with a decreasing step-size.\nTD methods generally converge faster than MC methods, although this has not been formally proven.\nTD methods are extremely useful for continuing tasks that cannot be broken down into episodes as required by MC methods.\nTD can be seen as a method for prediction learning where you try to predict what happens next given you current action, get new information and make a new prediction. That is, you do not need a training set (as in supervised learning) instead the reward signal is observed as time goes by.\nTD methods are good for sequential decision problems (multi-step prediction).\nTD methods are scalable in the sense that computations do not grow exponentially with the problem size.\n\nAn example illustrating that TD methods converge faster than MC methods is given in Exercise 10.6.1.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "10_td-pred.html#exercises",
    "href": "10_td-pred.html#exercises",
    "title": "10  Temporal difference methods for prediction",
    "section": "10.6 Exercises",
    "text": "10.6 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n10.6.1 Exercise - A random walk\nConsider a MDP with states 2-6 and two terminal states 1 and 7. Possible transitions are given in Fig. 10.2. All episodes start in the centre state, 4, then proceed either left or right by one state on each step. We assume the stochastic policy \\(\\pi\\) is used where each direction has equal probability. Episodes terminate either on the left (1) or the right (7). When an episode terminates on the right, reward of 1 occurs; all other rewards are zero. If the discount factor equals 1, the state-value of each state is the probability of terminating on the right if starting from that state.\n\n\n\n\n\n\n\n\nFigure 10.2: Possible transitions between states and rewards.\n\n\n\n\n\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporal difference methods for prediction</span>"
    ]
  },
  {
    "objectID": "11_td-control.html",
    "href": "11_td-control.html",
    "title": "11  Temporal difference methods for control",
    "section": "",
    "text": "11.1 Learning outcomes\nIn Module 10 temporal difference (TD) was used to estimate state-values. In this module we focus on improving the policy (control) by applying generalized policy iteration (GPI) using TD methods. GPI repeatedly apply policy evaluation and policy improvement. Since we do not have a model (the transition probability matrix and reward distribution are not known) all our action-values are estimates. Hence an element of exploration are needed to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy:\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 4, 6, 9, and 12 of the course.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#learning-outcomes",
    "href": "11_td-control.html#learning-outcomes",
    "title": "11  Temporal difference methods for control",
    "section": "",
    "text": "Describe how generalized policy iteration (GPI) can be used with TD to find improved policies.\nIdentify the properties that must the satisfied for GPI to converge to the optimal policy.\nDerive and explain SARSA an on-policy GPI algorithm using TD.\nDescribe the relationship between SARSA and the Bellman equations.\nDerive and explain Q-learning an off-policy GPI algorithm using TD.\nArgue how Q-learning can be off-policy without using importance sampling.\nDescribe the relationship between Q-learning and the Bellman optimality equations.\nDerive and explain expected SARSA an on/off-policy GPI algorithm using TD.\nDescribe the relationship between expected SARSA and the Bellman equations.\nExplain how expected SARSA generalizes Q-learning.\nList the differences between Q-learning, SARSA and expected SARSA.\nApply the algorithms to an MDP to find the optimal policy.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#textbook-readings",
    "href": "11_td-control.html#textbook-readings",
    "title": "11  Temporal difference methods for control",
    "section": "11.2 Textbook readings",
    "text": "11.2 Textbook readings\nFor this module, you will need to read Chapter 6.4-6.6 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#sarsa---on-policy-gpi-using-td",
    "href": "11_td-control.html#sarsa---on-policy-gpi-using-td",
    "title": "11  Temporal difference methods for control",
    "section": "11.3 SARSA - On-policy GPI using TD",
    "text": "11.3 SARSA - On-policy GPI using TD\nThe first GPI algorithm we will consider is SARSA. Since we do not have a model we need to estimate action-values so the optimal policy can be found using \\(q_*\\) (see Equation 7.3). Hence to predict action-values for a policy \\(\\pi\\), the incremental update Equation 10.1 must be modified to use \\(Q\\) values: \\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n\\] Note given a policy \\(\\pi\\) you need to know \\(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}\\) or short SARSA before you can make an update. This acronym is used to name the algorithm.\nThe algorithm is given in Fig. 11.1. To ensure infinite exploration of all action-values, we need e.g. an \\(\\epsilon\\)-greedy policy. The algorithm can also be applied for processes with continuing tasks. To ensure greedy in the limit a decreasing epsilon can be used (e.g. \\(\\epsilon = 1/t\\)). No stopping criterion is given but could stop when small differences in action-values are observed.\n\n\n\n\n\n\n\n\nFigure 11.1: SARSA - On-policy GPI using TD (Sutton and Barto 2018).\n\n\n\n\n\nSARSA is a sample based algorithm that do updates based on the Bellman equation for action-values (\\(q\\)): \\[\n\\begin{align}\n  q_\\pi(s, a) &= \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] \\\\\n  &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\\n  &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma v_\\pi(s')\\right) \\\\\n  &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma \\sum_{a'} \\pi(a'|s) q_\\pi(s', a')\\right).\n\\end{align}\n\\] That is, we update the estimate based on samples \\(r\\) and the estimate \\(q_\\pi\\) in \\(s'\\). This is the same approach as policy iteration in DP: we first calculate new estimates of \\(q_\\pi\\) given the current policy \\(\\pi\\) and then improve. Hence SARSA is a sample based version of policy iteration in DP.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#q-learning---off-policy-gpi-using-td",
    "href": "11_td-control.html#q-learning---off-policy-gpi-using-td",
    "title": "11  Temporal difference methods for control",
    "section": "11.4 Q-learning - Off-policy GPI using TD",
    "text": "11.4 Q-learning - Off-policy GPI using TD\nQ-learning resembles SARSA; however, there are some differences. The algorithm is given in Fig. 11.2. Note the incremental update equation is now: \\[\\begin{equation}\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\n\\end{equation}\\] That is, the next action used to update \\(Q\\) is selected greedy. That is, we are no longer following an \\(\\epsilon\\)-greedy policy for our updates.\n\n\n\n\n\n\n\n\nFigure 11.2: Q-learning - Off-policy GPI using TD (Sutton and Barto 2018).\n\n\n\n\n\nSARSA is an on-policy algorithm, meaning that the behavioural and target policy is the same, e.g. an \\(\\epsilon\\)-greedy policy to ensure exploration. That is, for fixed \\(\\epsilon\\) the greedy in the limit assumption is not fulfilled. Q-learning, on the other hand, is an off-policy algorithm where the behavioural policy is an \\(\\epsilon\\)-greedy and the target policy is the (deterministic) greedy policy. That is, Q-learning fulfil both the ‘infinite exploration’ and ‘greedy in the limit’ assumptions.\nNote under MC prediction an off-policy algorithm needed to use importance sampling to estimate the action-value of the target policy (see Section 9.5). This is not necessary for one-step TD, since \\[\n\\begin{align}\nq_\\pi(s,a) &= \\mathbb{E}_{\\pi}[R_t + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\\n           &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma \\sum_{a'} \\pi(a'|s) q_\\pi(s', a')\\right) \\\\\n           &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma \\max_{a'} q_\\pi(s', a')\\right) \\\\\n\\end{align}\n\\tag{11.1}\\]\nThat is, because the target policy is greedy and deterministic, the expectation of \\(G_{t+1}\\) becomes a maximum. Hence we can update the action-value estimates \\(Q\\) for the target policy \\(\\pi\\) even though we sample from an \\(\\epsilon\\)-greedy behavioural policy.\nQ-learning is a sample based algorithm that do updates based on the Bellman optimality equation for action-values (\\(q_*\\)): \\[\n\\begin{align}\n  q_*(s, a) &= \\max_\\pi q_\\pi(s, a) \\\\\n  &= \\max_\\pi \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma v_\\pi(s')\\right) \\\\\n  &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma \\max_\\pi v_\\pi(s')\\right) \\\\\n  &= \\sum_{s',r} p(s', r | s, a) \\left(r + \\gamma \\max_{a'} q_*(s', a')\\right)\n\\end{align}\n\\] That is, we update the estimate based on samples \\(r\\) and the estimate \\(q_*\\) in \\(s'\\). This is the same approach as value iteration in DP: we update the estimates of \\(q_\\pi\\) and improve the policy in one operation. Hence Q-learning is a sample based version of value iteration in DP.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#expected-sarsa---gpi-using-td",
    "href": "11_td-control.html#expected-sarsa---gpi-using-td",
    "title": "11  Temporal difference methods for control",
    "section": "11.5 Expected SARSA - GPI using TD",
    "text": "11.5 Expected SARSA - GPI using TD\nThe expected SARSA, as SARSA, focus on the Bellman equation Equation 11.1. SARSA generate action \\(A_{t+1}\\) from the policy \\(\\pi\\) and use the estimated action-value of \\((S_{t+1},A_{t+1})\\). However, since we know the current policy \\(\\pi\\), we might update based on the expected value instead: \\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[R_{t+1} + \\gamma \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \\right] \\\\\n\\] That is, we use a better estimate of the Bellman equation Equation 11.1 by not sampling \\(A_{t+1}\\) but using the (deterministic) expectation over all actions instead. Doing so reduces the variance induced by selecting random actions according to an \\(\\epsilon\\)-greedy policy. As a result, given the same amount of experiences, expected SARSA generally performs better than SARSA, but has a higher computational cost.\nExpected SARSA is more robust to different step-size values. The incremental update formula can be written as \\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[T_t - Q(S_t, A_t) \\right] = (1-\\alpha)Q(S_t, A_t) + \\alpha T_t,\n\\] with step-size \\(\\alpha\\) and target \\(T_t\\). For SARSA the target is \\[T_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}),\\] and for expected SARSA the target is: \\[T_t = R_{t+1} + \\gamma \\sum_{a} \\pi(a | S_{t+1}) Q(S_{t+1}, a).\\] Now assume that we have run the algorithm over many time-steps so that our estimates \\(Q(S_t, A_t)\\) are close to \\(q_*(S_t, A_t)\\). Since the target in expected SARSA is deterministic (we do not sample \\(A_{t+1}\\)), the target \\(T_t \\approx Q(S_t, A_t)\\) and no matter the step-size \\(Q(S_t, A_t)\\) will be updated to the same value. On the other hand, the target in SARSA uses a sample action \\(A_{t+1}\\) that might have an action-value far from the expectation. This implies that for large step-sizes \\(Q(S_t, A_t)\\) will be updated to the target which is wrong. Hence SARSA is more sensitive to large step-sizes.\nExpected SARSA can be both on-policy and off-policy. If the behavioural policy and the target policy are different it is off-policy. If they are the same it is on-policy. For instance, expected SARSA is off-policy if the target policy is greedy and the behavioural policy \\(\\epsilon\\)-greedy. In which case expected SARSA becomes Q-learning since the expectation of a greedy policy is the maximum value (\\(\\pi(s|a) = 1\\) here). Hence expected SARSA can be seen as a generalisation of Q-learning that improves SARSA.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#summary",
    "href": "11_td-control.html#summary",
    "title": "11  Temporal difference methods for control",
    "section": "11.6 Summary",
    "text": "11.6 Summary\nRead Chapter 6.9 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "11_td-control.html#exercises",
    "href": "11_td-control.html#exercises",
    "title": "11  Temporal difference methods for control",
    "section": "11.7 Exercises",
    "text": "11.7 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n11.7.1 Exercise - Factory Storage\nConsider Example 8.8 where a factory has a storage tank with a capacity of 4 \\(\\mathrm{m}^{3}\\) for temporarily storing waste produced by the factory. Each week the factory produces \\(0,1\\), 2 or 3 \\(\\mathrm{m}^{3}\\) waste with respective probabilities \\[p_{0}=\\displaystyle \\frac{1}{8},\\ p_{1}=\\displaystyle \\frac{1}{2},\\ p_{2}=\\displaystyle \\frac{1}{4} \\text{ and } p_{3}=\\displaystyle \\frac{1}{8}.\\] If the amount of waste produced in one week exceeds the remaining capacity of the tank, the excess is specially removed at a cost of $30 per cubic metre. At the end of each week there is a regular opportunity to remove all waste from the storage tank at a fixed cost of $25 and a variable cost of $5 per cubic metre.\nAn MDP model was formulated in Example 8.8 and solved using policy iteration. Our goal here is to solve the same problem with GPI using TD.\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n11.7.2 Exercise - Car Rental\nConsider the car rental problem in Exercise 7.8.2 and Exercise 8.10.3. An MDP model was formulated in Exercise 8.10.3 and solved using policy iteration. Our goal here is to solve the same problem with GPI using TD.\nConsider this section in the Colab notebook to see the questions. Use your own copy if you already have one.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Temporal difference methods for control</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html",
    "href": "12_approx-pred.html",
    "title": "12  On-policy prediction with approximation",
    "section": "",
    "text": "12.1 Learning outcomes\nTabular methods assume we can store a separate value for each state. In large or continuous state spaces, this is impossible. We therefore approximate the value function using a parametrised model \\(\\hat v(s;\\mathbf w)\\). That is, we approximate the value function using a function with parameters \\(\\mathbf w \\in \\mathbb {R} ^d\\). In this module, a on-policy policy \\(\\pi\\) is assumed (we evaluate the value of the same policy we use to generate experience).\nMost function approximation techniques are examples of supervised learning that require parameters \\(\\textbf{w}\\) to be found using training examples. In our case, we update the value approximation online (our estimate of \\(\\textbf{w}\\)) when new training arrive, i.e. \\(s \\rightarrowtail u\\) where \\(u\\) is the update of the value function we’d like to make at state \\(s\\).\nAfter studying this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#learning-outcomes",
    "href": "12_approx-pred.html#learning-outcomes",
    "title": "12  On-policy prediction with approximation",
    "section": "",
    "text": "Explain why function approximation is needed beyond tabular RL and define the prediction problem.\nWrite the mean-squared value error objective and derive semi-gradient updates.\nImplement Gradient Monte Carlo and semi-gradient TD(0) for value prediction with function approximation.\nCompare and solve algorithms for linear function approximation.\nMotivate and construct feature representations (polynomial/Fourier basis, tile coding) and discuss their trade-offs.\nExplain causes of instability (e.g., step-size sensitivity).\nDescribe other methods for function approximation, such as memory-based and interest/emphasis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#textbook-readings",
    "href": "12_approx-pred.html#textbook-readings",
    "title": "12  On-policy prediction with approximation",
    "section": "12.2 Textbook readings",
    "text": "12.2 Textbook readings\nFor this module, you will need to read Chapter 9-9.11 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#the-prediction-objective",
    "href": "12_approx-pred.html#the-prediction-objective",
    "title": "12  On-policy prediction with approximation",
    "section": "12.3 The prediction objective",
    "text": "12.3 The prediction objective\nThe mean-squared value error (MSVE) is often used as an objective for prediction: \\[\ne(\\mathbf w) = \\overline{VE}(\\mathbf w) = \\sum_s \\mu(s)\\,\\big[v_\\pi(s) - \\hat v(s;\\mathbf w)\\big]^2.\n\\]\nHere \\(\\mu(s)\\) denotes a non-negative weight of state \\(s\\) that indicates how much we care about precision in state \\(s\\). Often, we focus on precision in states visited more often than others. That is, \\(\\mu(s)\\) denote the probability of visiting \\(s\\) (\\(\\sum_s \\mu(s) = 1\\)). If \\(\\mu(s)\\) is unknown we store the number of times \\(s\\) have been visited (including if \\(s\\) have been used as starting state for episodic tasks) and \\(\\mu(s)\\) becomes the fraction of time spent in \\(s\\).\nThe square root \\(\\sqrt{e}\\) of the MSVE gives a rough measure of how much the approximate values differ from the true values and is often used in plots (often denoted RMSVE or just RMS). Note, since \\(|\\mathbf w| \\ll |{\\cal S}|\\), adjusting \\(\\mathbf{w}\\) to reduce error at visited states can increase error elsewhere.\nIt is not completely clear that \\(e\\) is the right performance objective. Remember that our goal is to find a better policy. The best value function for this purpose is not necessarily minimizing \\(e\\). Nevertheless, it is not yet clear what a more useful alternative goal might be.\nGiven objective \\(e\\), the goal is to minimize \\(e\\). That is, to find a global optimum, a weight vector \\(\\mathbf w^*\\) for which \\(e(\\mathbf w^*) \\leq e(\\mathbf w)\\) for all possible \\(\\mathbf w\\). This is, not always possible. Complex function approximations may converge to a local optimum. Although this guarantee is only slightly reassuring, it is typically the best that can be done, and often it is enough.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#stochastic-gradient-and-semi-gradient-methods",
    "href": "12_approx-pred.html#stochastic-gradient-and-semi-gradient-methods",
    "title": "12  On-policy prediction with approximation",
    "section": "12.4 Stochastic-gradient and semi-gradient methods",
    "text": "12.4 Stochastic-gradient and semi-gradient methods\nTo minimise \\(e\\), we use stochastic-gradient descent (SGD) methods. SGD methods are widely used for function approximation and can be used in an online setting. Here we update \\(\\mathbf w\\) each time a new data sample \\(S_t \\mapsto v_\\pi(S_t)\\) arrives (an example). That is, we have a sequence of updates \\(\\mathbf w_t, \\mathbf w_{t+1}, \\mathbf w_{t+2}, \\ldots\\) and we assume that states appear in samples with the same distribution, \\(\\mu\\). A good strategy in this case is to try to minimize error on the observed examples and adjust the weight vector after each sample by a small amount (\\(\\alpha&gt;0\\)): \\[\n\\begin{align}\n\\mathbf w_{t+1} &= \\mathbf w_t - \\frac{1}{2}\\alpha\\nabla_{\\mathbf w}\\big[v_\\pi(S_t) - \\hat v(S_t;\\mathbf w_t)\\big]^2\\\\\n  &= \\mathbf w_t + \\alpha\\,\\big[v_\\pi(S_t) - \\hat v(S_t;\\mathbf w_t)\\big]\\,\\nabla_{\\mathbf w}\\hat v(S_t;\\mathbf w_t).\n\\end{align}\n\\] where \\(\\nabla_{\\mathbf w}\\hat v(S_t;\\mathbf w_t)\\) denote the partial derivative vector (the gradient).\nNote that since we approximate \\(v_\\pi(s)\\), we do not know the exact value of \\(v_\\pi(s)\\). Instead, we have a target output \\(U_t\\) (an estimate of \\(v_\\pi(s)\\)). That is, our sample is now \\(S_t \\mapsto U_t\\) instead and our updates becomes:\n\\[\n\\mathbf w_{t+1} = \\mathbf w_t + \\alpha\\,\\big[U_t - \\hat v(S_t;\\mathbf w_t)\\big]\\,\\nabla_{\\mathbf w}\\hat v(S_t;\\mathbf w_t).\n\\] If \\(U_t\\) is an unbiased estimate, that is, if \\(\\mathbb E[U_t|S_t=s] = v_\\pi(s)\\), for all t, then \\(w_t\\) is guaranteed to converge to a local optimum for decreasing \\(\\alpha\\). In this case it is called a gradient descent method. If \\(U_t\\) is a biased estimate (not independent on \\(\\mathbf w_t\\)) it is not guaranteed to converge to the true local optimum. We call these methods semi-gradient methods. We may choose \\(U_t\\) as\n\nGradient Monte Carlo: \\(U_t = G_t\\) (full return). Unbiased in expectation and requires episode completion. High variance because the full return depends on all future rewards up to the end of the episode. Any randomness in transitions or rewards propagates through the entire sequence.\nSemi-gradient TD(0): \\(U_t = R_{t+1} + \\gamma\\,\\hat v(S_{t+1};\\mathbf w_t)\\). Target depends on \\(\\mathbf w_t\\) (bias via bootstrapping). Lower variance since the target depends only on one reward and the current estimate of the expected reward of the next state value.\n\nAlthough semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case discussed in the next section. Moreover, they offer important advantages e.g significantly faster learning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#linear-methods",
    "href": "12_approx-pred.html#linear-methods",
    "title": "12  On-policy prediction with approximation",
    "section": "12.5 Linear methods",
    "text": "12.5 Linear methods\nOne important function class for function approximation is when the approximate function is a linear function of the weight vector.\nLet \\(\\mathbf x(s)\\in\\mathbb R^d\\) be features and define \\[\n\\hat v(s;\\mathbf w) = \\mathbf w^\\top \\mathbf x(s) = \\sum_{i=1}^d w_ix_i(s).\n\\tag{12.1}\\]\nNote here, the gradient (vector with partial derivatives) is easy to calculate \\[\n\\nabla_{\\mathbf w}\\hat v(s;\\mathbf w) = \\mathbf x(s),\n\\] and our update formula becomes\n\\[\n\\mathbf w_{t+1} = \\mathbf w_t + \\alpha\\,\\big[U_t - \\hat v(S_t;\\mathbf w_t)\\big]\\mathbf x(S_t).\n\\] In the linear case the MSVE \\(e(\\textbf w)\\) is convex and has only one optimum, and thus any method that is guaranteed to converge is guaranteed to converge to the global optimum. That is, under gradient Monte Carlo \\(\\mathbf w_{t}\\) converges to the global optimum if \\(\\alpha\\) is reduced over time according to the usual conditions.\nA semi-gradient method also converges, but not necessarily to the global optimum, but rather a point near the global optimum. The update at each time \\(t\\) given the semi-gradient TD(0) becomes\n\\[\\begin{align}\n\\mathbf w_{t+1}\n      =& \\mathbf w_t + \\alpha\\left( R_{t+1} + \\gamma\\mathbf w_t^\\top \\mathbf x(S_{t+1}) - \\mathbf w_t^\\top \\mathbf x(S_t) \\right) \\mathbf x(S_t) \\\\\n      =& \\mathbf w_t + \\alpha\\left( R_{t+1}\\mathbf x(S_t) - \\mathbf x(S_t) \\left(\\mathbf x(S_t) - \\gamma\\mathbf x(S_{t+1})\\right)^\\top \\mathbf w_t \\right).\n\n\\end{align}\\]\nIn expectation we have \\[\\begin{align}\n\\mathbb E\\left( \\mathbf w_{t+1} | \\mathbf w_{t}\\right) =& \\mathbb E\\left( \\mathbf w_t + \\alpha\\left( R_{t+1}\\mathbf x(S_t) - \\mathbf x(S_t) \\left(\\mathbf x(S_t) - \\gamma\\mathbf x(S_{t+1})\\right)^\\top \\mathbf w_t \\right) \\right) \\\\\n    =& \\mathbf w_t + \\alpha\\left( \\mathbf b - \\mathbf A \\mathbf w_t \\right) \\\\\n\n\\end{align}\\]\nwhere \\[\n\\mathbf A = \\mathbb E\\big[ \\mathbf x(S_t) \\left(\\mathbf x(S_t) - \\gamma\\mathbf x(S_{t+1})\\right)^\\top \\big],\\;\n\\mathbf b = \\mathbb E\\big[R_{t+1}\\mathbf x(S_t)\\big].\n\\] If the system converges, it must converge to the weight vector \\(w^*\\) satisfying \\[\n\\mathbf b − \\mathbf A\\mathbf w^* = \\mathbf 0 \\Leftrightarrow \\mathbf w^* = \\mathbf A^{-1}\\mathbf b\n\\tag{12.2}\\] The estimate \\(w^*\\)is called the TD fixed point. Linear semi-gradient TD(0) converges to this point and the error is at most (in the continuing case) \\[\ne(\\textbf w^*) \\leq \\frac{1}{1-\\gamma}\\min_{\\textbf w} e(\\textbf w).\n\\] Critical to these convergence results is that states are updated according to the on-policy distribution. For other update distributions the approximation may actually diverge to infinity.\nIn the next sections we will focus on different way of selecting features, but first let us present an example to be used.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#a-random-walk",
    "href": "12_approx-pred.html#a-random-walk",
    "title": "12  On-policy prediction with approximation",
    "section": "12.6 A random walk",
    "text": "12.6 A random walk\nConsider a large random walk environment with 1,000 non-terminal states labelled 1 through 1000. Two terminal states lie just outside this range: state 0 on the left, which yields a reward of −1, and state 1001 on the right, which yields a reward of +1. Each episode begins in the center state, 500.\nFrom any non-terminal state \\(s\\), the agent flips a fair coin to choose left or right, then samples a jump size \\(k\\) uniformly from \\(\\{1, 2, \\dots, 100\\}\\). The agent then moves to state \\(s - k\\) or \\(s + k\\). If the resulting state lies outside \\([1,1000]\\), the episode terminates immediately with the corresponding terminal reward. All intermediate transitions give reward 0. The policy is fixed (a random policy as described) and uniform, with left and right chosen equally likely.\nThe goal is to estimate the value function \\(v(s)\\) under this policy using function approximation.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nDuring the lecture for this module, we will work with the tutorial. We implement different functions approximation algorithms. You may have a look at the notebook and the example before the lecture.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#feature-construction-for-linear-models",
    "href": "12_approx-pred.html#feature-construction-for-linear-models",
    "title": "12  On-policy prediction with approximation",
    "section": "12.7 Feature construction for linear models",
    "text": "12.7 Feature construction for linear models\nLinear methods are interesting because of their convergence guarantees, but also because in practice they can be very efficient in terms of both data and computation. This depends on how the states are represented in terms of features which are investigated in this section. Choosing features appropriate to the task is an important way of adding prior domain knowledge about the system. The features should correspond to the aspects of the state space along which generalization may be appropriate. We here consider different ways of constructing features.\n\n12.7.1 State aggregation\nState aggregation is a special case of linear function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. That is, the value of a state is estimated as its group’s value. Let \\(g(s)\\) be a mapping that assigns each state \\(s\\) to a group index. Then the features are \\(\\mathbf x_i(s) = 1\\) if \\(g(s) = i\\) and \\(\\mathbf x_j(s)  = 0\\) for \\(j\\neq i\\) and Equation 12.1 becomes \\[\n\\hat v(s; \\mathbf{w}) = w_{g(s)}.\n\\]\nBecause \\(\\boldsymbol{x}(s)\\) is a unit vector with all entries equal to zero except entry \\(g(s)\\), then \\[\nw_{g(s)} \\leftarrow w_{g(s)} + \\alpha \\,\\big(U_t - w_{g(s)}\\big),\n\\] and all other \\(w_j\\) remain unchanged (the gradient is 1 for group \\(g(s)\\) and 0 for the other components). That is, the approximation is a piecewise constant function that is constant for each group.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nFor an example see the corresponding section in the tutorial.\n\n\n\n\n12.7.2 Polynomials\nPolynomial features approximate \\(v_\\pi(s)\\) by fitting a low-degree polynomial of normalized state variables. Note the model remains linear in parameters \\(\\textbf w\\) even though it is non-linear in \\(s\\).\nPolynomials can capture non-linear relationships in the value function and provide a smoother approximation compared to methods like state aggregation, especially when the true value function is smooth. However, choosing the appropriate degree of the polynomial is important. Too low a degree might not capture the complexity of the value function, while too high a degree can lead to over-fitting and poor generalization. Polynomials can also have difficulty approximating value functions with sharp changes or discontinuities.\nLet us first consider a state \\(s\\) where \\(s\\) is a scalar. Here, the approximation is\n\\[\n\\hat{v} (s;\\mathbf w) = \\mathbf w^\\top \\mathbf x(s) = w_0 + w_1s + w_2s^2 + \\ldots + w_{d}s^d.\n\\]\nThat is, the gradient becomes polynomial of degree \\(d\\) \\[\n\\mathbf x(s) = (1,s,s^2,\\dots,s^d).\n\\]\nSecond, if we have multiple state variables \\(\\mathbf s=(s_1,\\dots,s_K)\\) and want to consider polynomials of degree \\(d\\), then each term/feature in the polynomial becomes\n\\[\n\\prod_{k=1}^K s_k^{i_k},\\quad \\sum_k i_k \\le d.\n\\]\nIf all are included then the number of features are \\(\\binom{K+d}{d}\\). Note the interactions (e.g., \\(s_1s_2^3\\)) capture cross-effects (e.g., inventory × demand, tenure × engagement).\nNumerical instabilities\nA good strategy is always to normalise the state values because if \\(s\\) has a high value, then \\(s^d\\) may be very large, yielding numerical instabilities. If we normalize so state variables are in the interval \\([-1,1]\\) then \\(s^d\\) also lies in this interval. This can be done by:\n\nIf \\(s\\in\\{1,\\dots,N\\}\\): use \\(z = \\dfrac{s}{N+1}\\in(0,1)\\) or \\(u = 2z-1 \\in (-1,1)\\).\nIf \\(s\\in[a,b]\\): use \\(u = \\dfrac{2s-(a+b)}{b-a}\\in(-1,1)\\).\n\nMoreover, using orthogonal polynomials (e.g. the Legendre basis) may improve instabilities (not within the scope of the course).\nTo find the right step-size \\(\\alpha\\), keep track of RMSVE. If it oscillates, then the step-size \\(\\alpha\\) is too high.\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nFor an example see the corresponding section in the tutorial.\n\n\n\n\n12.7.3 Fourier basis\nThe Fourier basis approximates \\(v_\\pi(s)\\) using global sine/cosine waves over a normalized state. The model remains linear in parameters even though it can represent highly non-linear shapes. The Fourier series/transforms are widely used because if a function to be approximated is known, then essentially any function can be approximated as accurately as desired. In reinforcement learning, where the functions to be approximated are unknown, Fourier basis functions are of interest because they are easy to use and can perform well.\nLet us first consider a state \\(s\\) where \\(s\\) is a scalar. Here the Fourier series is a function having period \\(\\tau\\) which is a linear combination of sine and cosine functions that are each periodic with periods that multiplum of \\(1/\\tau\\). Note, we are interested in approximating an aperiodic function defined over an interval. Hence, by setting \\(\\tau\\) to the length of the interval, the function of interest is then just one period of the periodic linear combination of the sine and cosine features. Furthermore, if you set \\(\\tau\\) to twice the length of the interval of interest and restrict attention to the approximation over the half interval [0, \\(\\tau/2\\)], then you can use just the cosine functions (see details in the book). Following this logic and letting \\(\\tau = 2\\), the interval becomes \\(s\\in[0, 1]\\).\nThe one-dimensional order-\\(d\\) Fourier cosine basis consists of the \\(d + 1\\) features. Here the approximation is\n\\[\n\\hat{v} (s;\\mathbf w) = \\mathbf w^\\top \\mathbf x(s) = w_0 + w_1\\cos(\\pi z) + w_2\\cos(2\\pi z) + \\ldots + w_{d}\\cos(d\\pi z),\n\\] where \\(z\\) is the normalised state. That is, the entries in the gradient are \\(x_i(z)=\\cos(i\\pi z), j=0,1,\\dots,n,\\) where \\(j=0\\) is the bias term (\\(\\cos(0)=1\\)).\nIf we have multiple normalized state variables \\(\\mathbf z=(z_1,\\dots,z_K)\\), we choose frequency vectors \\(\\mathbf c=(c_1,\\dots,c_K)\\) with non-negative integers. Combinations can be limited by considering \\(c\\) where \\(\\sum_k c_k\\le n\\), i.e. then number of features are \\(\\binom{K+n}{n}\\). Each feature now becomes\n\\[\nx_{\\mathbf c}(\\mathbf z)=\\prod_{k=1}^K \\cos\\!\\big(\\pi c_k z_k\\big).\n\\]\nNormalization of states\nSince we want the states normalized so \\(s\\in[0,1]\\) we do\n\nIf \\(s\\in\\{1,\\dots,N\\}\\): use \\(z = \\dfrac{s}{N+1}\\in(0,1)\\).\nIf \\(s\\in[a,b]\\): use \\(z = \\dfrac{s-a}{b-a}\\in[0,1]\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nFor an example see the corresponding section in the tutorial.\n\n\n\n\n12.7.4 Coarse coding\nHere, the idea is to cover the state space with many overlapping regions (often called receptive fields or features). A state activates the features whose regions contain it, producing a sparse binary vector. Coarse coding create sparse, localized features so that nearby states share parameters and thus generalize to each other.\nIf \\(s\\) is a normalized scalar we cover the values of s with several intervals. For instance, at \\(s=0.5\\), perhaps three overlapping intervals contain \\(0.5\\), so three features turn on.\nIf \\(s\\) have multiple state variables, regions might be discs/ellipses (2-D), balls/ellipsoids (3D), or any shapes convenient for the problem. Feature \\(i\\) is “on” if \\(s\\) falls inside the region.\nThe Linear approximation is\n\\[\n\\hat v(s;\\mathbf w) \\;=\\; \\mathbf w^\\top \\mathbf x(s) = \\sum_{\\{i | x_i(s) = 1\\}} w_i,\n\\]\nwhere \\(x_i(s)\\in\\{0,1\\}\\) (1 if region \\(i\\) covers \\(s\\)).\nBecause only a few fields cover any given state, \\(\\mathbf x(s)\\) is sparse and updates are cheap.\n\n\n\n12.7.5 Tile coding\nTile coding can be viewed as a structured, grid-based special case of coarse coding that is simple and fast. Here hyper-rectangular receptive fields arranged on offset grids. Overlap comes from having multiple tilings; each tiling itself does not overlap (it is a partition of the state space), but the union of tilings does.\nThat is, first create a tiling that partitions the space into non-overlapping tiles. Next, offset the tiling and hereby creating multiple tilings. Each tiling is offset slightly, so two nearby states are likely to share some tiles even if they fall on different sides of a grid boundary in one tiling. Steps for the scalar case becomes:\n\nChoose an integer number of tilings \\(n\\) (e.g., \\(n=8\\)).\nIn each tiling, split \\([0,1]\\) into \\(m\\) equal tiles (e.g., \\(m=50\\)).\nOffset each tiling by a small shift (e.g., \\(0, \\tfrac{1}{mn}, \\tfrac{2}{mn}, \\dots\\)) or randomly.\nFor a state \\(s\\), exactly one tile is active in each tiling; across \\(n\\) tilings, you get \\(n\\) active features.\n\nIn multiple dimensions (\\(k\\)), the only change is that each tiling is a Cartesian grid with \\(n_1\\times \\cdots \\times n_K\\) tiles.\nNote that offsets matter since a single grid creates discontinuities at tile boundaries. Multiple offset grids ensure that two nearby states share most of their active features, smoothing the representation and reducing boundary artefacts.\nBecause each update touches only \\(n\\) parameters, tile/coarse coding works extremely well with incremental semi-gradient TD: \\[\n\\mathbf w \\leftarrow \\mathbf w + \\alpha\\,\\delta_t\\,\\mathbf x(S_t),\\qquad\n\\delta_t = R_{t+1} + \\gamma\\,\\hat v(S_{t+1}) - \\hat v(S_t).\n\\]\nStep-size scaling. A common heuristic is to scale the per-tiling step size like \\(\\alpha \\approx \\frac{\\alpha_0}{n}\\) (since \\(n\\) weights are updated per step). This keeps the total update magnitude roughly controlled as you add more tilings.\nNote that features that are polynomials or Fourier basis are global. That is, each update affects the entire space. Coarse/tile coding is local. That is, the value function is not globally smooth, has local structure and thus less prone to global oscillations.\nChoosing design parameters\nNumber of tilings (\\(m\\)). More tilings increase overlap and stability but cost more memory/compute.\nTiles per dimension. Finer grids reduce bias (higher resolution) but increase variance and memory. Note that the function is piecewise-constant within each tile (per tiling). Summing across tilings reduces the effective piecewise step size, but if tiles are still large relative to the variation in \\(v_\\pi\\), you will see bias.Smaller tiles and fewer overlapping tilings increase variance (fewer shared samples per parameter). More tilings and/or larger tiles reduce variance by pooling data.\nNormalization. Always normalize each state coordinate to a fixed range (e.g., \\([0,1]\\)) before tiling so that “one tile” has a consistent meaning across features and tasks.\n\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nFor an example see the corresponding section in the tutorial.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#selecting-step-size-parameters-manually",
    "href": "12_approx-pred.html#selecting-step-size-parameters-manually",
    "title": "12  On-policy prediction with approximation",
    "section": "12.8 Selecting step-size parameters manually",
    "text": "12.8 Selecting step-size parameters manually\nThe goal is to pick a step size \\(\\alpha\\) that removes TD error quickly without causing instability.\nIn tabular prediction, a single update with target \\(U_t\\) is \\[V(S_t) \\leftarrow V(S_t) + \\alpha\\,[\\,U_t - V(S_t)\\,].\\] Think of \\(\\alpha\\) as how much you trust the newest target. Big enough to make visible progress, small enough to avoid creating noise. A step size of \\(\\alpha = 1/10\\) would take about 10 experiences to converge approximately to their mean target, and if we wanted to learn in 100 experiences we would use \\(\\alpha = 1/100\\).\nWith general function approximation there is not such a clear notion of number of experiences with a state, as each state may be similar to and dissimilar from all the others to various degrees. Suppose you wanted to learn in about \\(\\tau\\) experiences with substantially the same feature vector. Then, good rule of thumb for setting the step-size parameter to\n\\[\\begin{equation}\n    \\alpha = (\\tau \\mathbb{E}[\\textbf{x}^T\\textbf{x}])^{-1}\n\\end{equation}\\]\nwhere \\(\\textbf{x}\\) is a random feature vector chosen from the same distribution as input vectors will be in the SGD. This method works best if the feature vectors do not vary greatly in length; ideally \\(\\textbf{x}^T\\textbf{x}\\) is a constant.\nFor instance, for tile coding with \\(n\\) tilings we have that \\[\\textbf{x}^T\\textbf{x}=n,\\] and \\[\\alpha = (\\tau n)^{-1}.\\]\nGiven a scalar state and a polynomial of degree \\(d\\), we have that \\[\\textbf{x}^T\\textbf{x}=(1, s, s^2, \\ldots s^d)^T(1, s, s^2, \\ldots s^d) = \\sum_{i=0}^{d} (s^i)^2.\\] If the state is normalised to [-1,1] and we assume a uniform distribution then \\[\\mathbb{E}[(s^i)^2] = 1/(2i + 1),\\] and the step-size becomes \\[\\alpha = (\\tau\\sum_{i=0}^{d} 1/(2i+1))^{-1}.\\]\n\n\nIf features activate unevenly, damp steps by visit counts: \\[\n\\alpha_i \\;=\\; \\frac{\\alpha_0}{1+n_i},\n\\] where \\(n_i\\) is how often feature \\(i\\) has been active. This mimics tabular \\(1/n\\) averaging and reduces variance for frequent features.\nPractical comments\n\nTry a small grid of \\(\\alpha\\) (log-spaced). For each candidate: - run a short training phase, - track an error proxy (e.g., RMSVE), - pick the largest \\(\\alpha\\) that yields a smooth, monotone decline.\nIf curves spike or blow up, reduce \\(\\alpha\\) by a factor of \\(2\\)–\\(10\\); if flat, increase with a factor \\(2\\)–\\(10\\).\nFeature specific\n\nTile coding: with \\(n\\in\\{4,8,16\\}\\), try \\(\\alpha_0\\in[0.1,0.4]\\) and use \\(\\alpha=\\alpha_0/n\\).\nExample: \\(m=8 \\Rightarrow \\alpha\\in[0.0125,0.05]\\) per weight.\nFourier (cos-only), max freq \\(n\\in[4,16]\\): start \\(\\alpha\\in[10^{-4},10^{-3}]\\). Lower \\(n\\) allows larger \\(\\alpha\\).\nPolynomials (degree \\(3\\)–\\(5\\)): start \\(\\alpha\\in[10^{-4},10^{-3}]\\); if you see boundary oscillations, lower \\(\\alpha\\).\n\n\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nFor an example see the corresponding section in the tutorial.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#nonlinear-function-approximation-artificial-neural-networks",
    "href": "12_approx-pred.html#nonlinear-function-approximation-artificial-neural-networks",
    "title": "12  On-policy prediction with approximation",
    "section": "12.9 Nonlinear function approximation: artificial neural networks",
    "text": "12.9 Nonlinear function approximation: artificial neural networks\nArtificial neural networks (ANNs) are widely used nonlinear function approximators capable of representing complex mappings between inputs and outputs. In reinforcement learning, ANNs are employed to approximate value functions, policies, and models of the environment.\nDeep neural networks, composed of multiple hidden layers, can automatically construct hierarchical representations of input data. Each successive layer extracts more abstract features, enabling the network to learn complex relationships without requiring handcrafted features.\nANNs are trained using stochastic gradient methods, where each connection weight is updated in a direction that improves the performance measure or objective function. In supervised learning, this typically involves minimizing the expected prediction error over labeled examples. In reinforcement learning, the objective function may involve minimizing temporal-difference (TD) errors when approximating value functions or maximizing expected reward in policy-gradient methods.\nTo adjust weights, the learning algorithm must estimate how small changes in each weight influence the overall performance. The backpropagation algorithm efficiently computes these partial derivatives for networks with differentiable activation functions. It alternates between a forward pass, computing activations through the network, and a backward pass, propagating errors to compute gradients for each weight. These gradients serve as stochastic estimates of the true gradient used to update the weights.\nAlthough backpropagation can train networks with one or two hidden layers effectively, it struggles with deeper architectures. Increasing the number of layers can lead to poorer performance due to several factors such as overfitting (many parameters makes it difficult to generalize from limited training data).\nEven though there are difficulties, research in deep ANNs is a hot topic, and big advancements have been made since the book was published. Using deep ANNs in RL is denoted deep reinforcement learning, and many RL courses focus on this. Moreover, there are many implementations of deep RL algorithms.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#least-squares-temporal-difference-learning",
    "href": "12_approx-pred.html#least-squares-temporal-difference-learning",
    "title": "12  On-policy prediction with approximation",
    "section": "12.10 Least-squares temporal-difference learning",
    "text": "12.10 Least-squares temporal-difference learning\nConsider Equation 12.2 for calculating the TD fixed point. Least-squares TD (LSTD) estimate the weights using this equation by estimating matrix \\(\\textbf A\\) and vector \\(\\textbf b\\) incrementally from experience.\nThe key advantages of LSTD are:\n\nit eliminates the need for a step-size parameter,\nit converges faster than incremental TD for stationary problems,\nit produces an exact least-squares solution under linear function approximation.\n\nHowever, LSTD also has important limitations:\n\nit lacks a forgetting mechanism, making it unsuitable when the target policy changes (as in control problems),\nit can be sensitive to numerical instabilities,\nit requires storage proportional to \\(d\\^2\\), which can be prohibitive for high-dimensional features.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#memory-based-function-approximation",
    "href": "12_approx-pred.html#memory-based-function-approximation",
    "title": "12  On-policy prediction with approximation",
    "section": "12.11 Memory-based function approximation",
    "text": "12.11 Memory-based function approximation\nMemory-based function approximation have different approach to estimating the state value. Here no set of parameters are adjusted, as in parametric methods. Instead, the agent stores examples of experience pairs \\(s \\mapsto g\\) of states and their estimated returns (the state value). When an estimate of the state value in a query state \\(s'\\) is wanted, the estimates in memory is used directly. That is, there is no parameters and calculation is lazy, since learning is deferred until an estimate is required.\nWhen the value of a query state is requested, the algorithm retrieves from memory a set of stored examples with a short ‘distance’ to the query state. Here, the distance can be defined as a kernel (function) that assigns a number between two states (see Section 9.10 in Sutton and Barto (2018) for further details).\nGiven the kernel, different methods can be used to calculate the state value of the query state. For instance, local search based methods could be nearest neighbour, where the value of the query state is taken to be that of the most similar stored example or weighted average methods, which generalise this by using several neighbours and computing a distance-weighted mean. Another method is locally weighted regression that fits a parametric model to the neighbourhood of the query state.\nThese methods can be advantageous in reinforcement learning because they focus approximation effort on regions of the state space actually encountered by the agent, avoiding the need for a global model. However, their main drawback is computational cost: retrieving and comparing neighbours becomes increasingly expensive as more examples are stored, particularly in high-dimensional spaces.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#on-policy-learning-with-interestemphasis",
    "href": "12_approx-pred.html#on-policy-learning-with-interestemphasis",
    "title": "12  On-policy prediction with approximation",
    "section": "12.12 On-policy learning with interest/emphasis",
    "text": "12.12 On-policy learning with interest/emphasis\nIn traditional on-policy learning methods such as TD(0), all states visited under the policy receive equal attention. Each state contributes equally to the learning process, regardless of how important it may be for accurate value prediction or control. Section 9.11 introduces the ideas of interest and emphasis to refine how learning updates are distributed across states. Here, interest \\(I_t\\) denotes external weighting of states/times) and emphasis \\(M_t\\) the accumulated weighting. As a result updates prioritize these parts of the on-policy distribution.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#summary",
    "href": "12_approx-pred.html#summary",
    "title": "12  On-policy prediction with approximation",
    "section": "12.13 Summary",
    "text": "12.13 Summary\nRead Chapter 9.12 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "12_approx-pred.html#exercises",
    "href": "12_approx-pred.html#exercises",
    "title": "12  On-policy prediction with approximation",
    "section": "12.14 Exercises",
    "text": "12.14 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\nYou may solve the exercises in the corresponding sections in this Colab notebook.\n\n12.14.1 Exercise\nShow that tabular methods, such as those presented in Part I of this book, are a special case of linear function approximation. What would the feature vectors be?\n\n\n12.14.2 Exercise\nSuppose we believe that one of the two state dimensions (let us say \\(x_1\\)) is more likely to have an effect on the value function than is the other (\\(x_2\\))\nWhat kind of tilings could be used to take advantage of this prior knowledge?\n\n\n12.14.3 Exercise\nSuppose you are using tile coding to transform a seven-dimensional continuous state space into binary feature vectors to estimate a state value function. You believe that the dimensions do not interact strongly, so you decide to use eight tilings of each dimension separately. That is, \\(7 \\cdot 8 = 56\\) tilings. In addition, in case there are some pairwise interactions between the dimensions, you also take all pairs of dimensions and tile each pair conjunctively. You make two tilings for each pair of dimensions, making a grand total of 21 + 56 = 98 tilings.\nGiven these feature vectors, you suspect that you still have to average out some noise, so you decide that you want learning to be gradual, taking about 10 presentations with the same feature vector first.\nWhat step-size parameter should you use? Why?\n\n\n12.14.4 Exercise\nConsider the car rental problem described in Module 7.8.2.\nApproximate the state value under the policy \\[\na = \\begin{cases}\n\\lfloor x/5 + 1 \\rfloor & x &gt; 5, y &lt; 5 \\\\\n-\\lfloor y/5 + 1 \\rfloor & y &gt; 5, x &lt; 5 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>On-policy prediction with approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html",
    "href": "13_approx-control.html",
    "title": "13  On-Policy Control with Approximation",
    "section": "",
    "text": "13.1 Learning outcomes\nIn Module 12, the focus was on predicting the state values of a policy using function approximation. Here, the emphasis is on control, specifically finding an optimal policy through function approximation of action values \\(\\hat q(s, a, \\textbf{w})\\). Once again, the focus remains on on-policy methods.\nIn the episodic case, the extension from prediction to control is straightforward, but in the continuing case, discounting is not suitable to find an optimal policy. Surprisingly, once we have a genuine function approximation, we have to give up discounting and switch to an “average-reward” formulation.\nAfter studying this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#learning-outcomes",
    "href": "13_approx-control.html#learning-outcomes",
    "title": "13  On-Policy Control with Approximation",
    "section": "",
    "text": "Describe how to extend semi-gradient prediction to action-value approximation\nImplement episodic one-step semi-gradient SARSA with \\(\\epsilon\\)-greedy improvement.\nBe able to describe how to generalize to \\(n\\)-step semi-gradient SARSA in episodic tasks and explain the bias–variance trade-off as \\(n\\) increases (from TD toward Monte Carlo).\n\nGrasp why in continuing tasks with function approximation the discounted objective lacks a reliable local improvement guarantee, motivating a shift to average-reward.\nDefine and interpret differential returns and differential value functions for the average-reward setting.\nDerive the Bellman equations under the average reward criterion.\nDescribe differential TD errors and corresponding semi-gradient updates (state-value and action-value forms) using a running estimate of the average reward.\nExplain how to update the estimate of the average reward.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#textbook-readings",
    "href": "13_approx-control.html#textbook-readings",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.2 Textbook readings",
    "text": "13.2 Textbook readings\nFor this module, you will need to read Chapter 10-10.5 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen here.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#episodic-semi-gradient-control",
    "href": "13_approx-control.html#episodic-semi-gradient-control",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.3 Episodic Semi-gradient Control",
    "text": "13.3 Episodic Semi-gradient Control\nConsider episodic tasks. To find a good policy, the action-value function is approximated by a differentiable function \\(\\hat q(s,a,\\mathbf{w}) \\approx q^\\pi(s,a)\\) with weight vector \\(\\mathbf{w}\\).\nTraining examples now take the form \\((S_t, A_t) \\mapsto U_t\\), where \\(U_t\\) is any target approximating \\(q^\\pi(S_t,A_t).\\) For instance, for a one-step semi-gradient, we have\n\\[\nU_t = R_{t+1} + \\gamma\\, \\hat q(S_{t+1}, A_{t+1}, \\mathbf{w}_t),\n\\] which bootstraps from the next state–action estimate produced by the same \\(\\varepsilon\\)-greedy policy.\nLearning is done using semi-gradient stochastic gradient descent on the squared error, yielding the generic update \\[\n\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\big[U_t - \\hat q(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_{\\mathbf{w}} \\hat q(S_t,A_t,\\mathbf{w}_t).\n\\] This is the direct action-value analogue of semi-gradient TD for state values.\nTo turn prediction into control, techniques for policy improvement and action selection are needed. If the action set is discrete and not too large, then we can use the techniques already developed in previous chapters. For instance, exploration using an \\(\\varepsilon\\)-greedy policy and update action values using Sarsa-style targets.\nPseudo code for the episodic semi-gradient SARSA is given in Fig. 13.1.\n\n\n\n\n\n\n\n\nFigure 13.1: Episodic semi-gradient SARSA (Sutton and Barto 2018).\n\n\n\n\n\nOne may ask if this algorithm can be modified to use Q-learning? However, this may not work since this is an off-policy algorithm, which may diverge due to the “deadly triad” (off-policy + bootstrapping + approximation). This is true even with linear features and fixed \\(\\epsilon\\)-greedy behavior; there is no general convergence guarantee. But, we may modify the algorithm to use expected SARSA that use the same policy for action selection and estimation (on-policy).\n\n\n\n\n\n\nNoteColab - Before the lecture\n\n\n\nDuring the lecture for this module, we will work with the seasonal inventory and sales planning problem in Module 9.4.4. This is done in the tutorial where we implement different functions approximation algorithms. You may have a look at the notebook and the example before the lecture.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#n-step-sarsa",
    "href": "13_approx-control.html#n-step-sarsa",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.4 \\(n\\)-step SARSA",
    "text": "13.4 \\(n\\)-step SARSA\nTo extend one-step SARSA we may extend our forward view to \\(n\\)-steps where the target accumulates the next \\(n\\) rewards before bootstrapping: \\[\nG_{t:t+n} = U_t = \\sum_{k=1}^{n} \\gamma^{k-1} R_{t+k}\n\\;+\\;\n\\gamma^{n}\\, \\hat q(S_{t+n}, A_{t+n}, \\mathbf{w}).\n\\] Note if the episode terminates before \\(t+n\\), the bootstrap term is omitted and \\(G_{t:t+n}\\) is just the episodic return (\\(G_t\\)). This provides a spectrum between Monte Carlo (large \\(n\\)) and one-step bootstrapping (\\(n=1\\)).\nOut update now becomes \\[\n\\mathbf w \\leftarrow \\mathbf w + \\alpha\\big[G_{t:t+n} - \\hat q(S_t,A_t,\\mathbf w)\\big]\\nabla_w \\hat q(S_t,A_t,\\mathbf w).\n\\]\nExploration could be \\(\\epsilon\\)-greedy with respect to \\(\\hat q\\), forming an on-policy loop that both explores and improves.\nThe choice of look-ahead steps (\\(n\\)) involves a bias-variance trade-off: \\(n=1\\) enables rapid learning but can be shortsighted, while very large \\(n\\) approaches Monte Carlo methods, increasing variance. Practically, small to moderate \\(n\\) values often lead to faster learning and better stability.\nPseudo code for the episodic n-step semi-gradient SARSA is given in Fig. 13.2.\n\n\n\n\n\n\n\n\nFigure 13.2: Episodic \\(n\\)-step semi-gradient SARSA (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#average-reward-a-new-problem-setting-for-continuing-tasks",
    "href": "13_approx-control.html#average-reward-a-new-problem-setting-for-continuing-tasks",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.5 Average Reward: A New Problem Setting for Continuing Tasks",
    "text": "13.5 Average Reward: A New Problem Setting for Continuing Tasks\nThe average-reward formulation treats continuing tasks by optimizing the long-run reward rate instead of discounted returns. The performance of a policy \\(\\pi\\) is defined as the steady-state average \\[\n\\begin{align}\nr(\\pi) &= \\lim_{h\\to\\infty}\\frac{1}{h}\\,\\mathbb{E}_\\pi\\!\\left[\\sum_{t=1}^{h} R_t\\right] \\\\\n  &= \\sum_s \\mu_\\pi(s) \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)r \\\\\n  &= \\sum_s \\mu_\\pi(s) \\sum_a \\pi(a|s) \\sum_{s'} p(s'|s,a)r(s,a),\n\\end{align}\n\\] This holds if the Markov chain (MDP under \\(\\pi\\)) is ergodic, i.e., all states are reached with the same steady-state distribution \\(\\mu_\\pi(s)\\), regardless of the starting state.\nTo measure preferences between states and actions without discounting, we introduce differential returns that subtract the average rate at each step: \\[\nG_t \\doteq \\sum_{k=0}^{\\infty}\\big(R_{t+1+k}-r(\\pi)\\big).\n\\]\nThe differential action-value functions then becomes \\[\n\\begin{align}\nq^\\pi(s,a) &= \\mathbb{E}_\\pi[G_t\\mid S_t=s, A_t=a] \\\\\n  &= \\sum_{s',r} p(s',r\\mid s,a)\\Big(r - r(\\pi) + \\sum_{a'} \\pi(a'\\mid s')\\,q^\\pi(s',a')\\Big).\n\\end{align}\n\\] They satisfy Bellman relations analogous to the discounted case but without \\(\\gamma\\) and with rewards centered by \\(r(\\pi)\\).\nLearning proceeds by replacing discounted TD errors with differential TD errors that incorporate an estimate of the average reward. With function approximation on \\(\\hat q(s,a,\\textbf w)\\) and a running estimate of \\(\\bar R_t \\approx r(\\pi)\\), the errors become \\[\n\\delta_t^{q} \\doteq R_{t+1}-\\bar R_t + \\hat q(S_{t+1},A_{t+1},\\textbf w_t) - \\hat q(S_t,A_t,\\textbf  w_t).\n\\] The Semi-gradient update is \\[\nw_{t+1} \\leftarrow \\textbf w_t + \\alpha\\,\\delta_t^{q}\\,\\nabla_w \\hat q(S_t,A_t,\\textbf w_t).\n\\]\nLeft is how to update the average-reward estimate? This can be done incrementally with a small step size to ensure stability, e.g. \\[\n\\bar R_{t+1} \\leftarrow \\bar R_t + \\beta\\delta_t^q.\n\\]\nControl replaces policy terms with maximization as usual, defining optimal differential values \\(v^*\\) and \\(q^*\\) and coupling learning with \\(\\epsilon\\)-greedy improvement over \\(\\hat q\\).\nPseudo code for the continuing (differential) semi-gradient SARSA is given in Fig. 13.3.\n\n\n\n\n\n\n\n\nFigure 13.3: Continuing semi-gradient SARSA (Sutton and Barto 2018).\n\n\n\n\n\nConceptually, this formulation aligns the objective with what matters in truly continuing problems (long-run reward rate), avoids artifacts from discounting, and remains compatible with multi-step, eligibility-trace, and function-approximation methods by the simple substitution of average-reward-centered TD errors.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#why-avoidig-discounted-reward",
    "href": "13_approx-control.html#why-avoidig-discounted-reward",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.6 Why avoidig discounted reward?",
    "text": "13.6 Why avoidig discounted reward?\nWhy are we moving from the discounted reward criterion to the avearage reward criterion?\nUsing the discounted reward criterion is ill-suited for truly continuing tasks once function approximation enters the picture. Note, function approximation creates bias among states. Hence, with approximation, your value function may not represent the true value function accurately.\nThe policy improvement theorem does not apply with function approximation in the discounted setting. In fact, the approximation errors can be amplified by discounting, and greedy improvement is not guaranteed to improve the policy. This loss of a policy improvement theorem means discounted control lacks a firm local-improvement foundation under approximation.\nHence, the best solution is to replace discounted control in continuing tasks with the average-reward formulation and its differential value functions, which align the objective with the steady-state reward rate and integrate smoothly with semi-gradient methods.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#summary",
    "href": "13_approx-control.html#summary",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.7 Summary",
    "text": "13.7 Summary\nRead Chapter 10.6 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "13_approx-control.html#exercises",
    "href": "13_approx-control.html#exercises",
    "title": "13  On-Policy Control with Approximation",
    "section": "13.8 Exercises",
    "text": "13.8 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\nYou may solve the exercises in the corresponding sections in this Colab notebook. Use your own copy if you already have one.\n\n13.8.1 Exercise (seasonal inventory)\nSolve the seasonal inventory and sales planning problem in Module 9.4.4 using a Fourier basis as function approximation to estimate the action-values.\n\n\n13.8.2 Exercise (car rental)\nConsider the Car rental problem in Exercise 7.8.2. Our goad is to use function approximation to estimate the optimal state value function.\nConsider the Colab notebook to see the questions.\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>On-Policy Control with Approximation</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html",
    "href": "14_policy-gradient.html",
    "title": "14  Policy Gradient Methods",
    "section": "",
    "text": "14.1 Learning outcomes\nUp to this point, most methods in reinforcement learning have been based on estimating value functions, i.e. learning the expected return for each state–action pair. Next, the best policy can be found by selecting the action with the highest estimate. That is, the policy used is derived from the estimates and hence dependent on the estimates.\nHere, we consider a new approach and focus on directly learning a parameterized policy that can select actions without referring to a value function. Although a value function may still be employed to assist in learning the policy parameters, it is no longer required for decision making.\nLet the policy be represented as \\[\\pi(a|s, \\theta) = \\Pr(A_t = a|S_t = s, \\theta_t = \\theta),\\] where \\(\\theta \\in \\mathbb{R}^{d'}\\) is a vector of policy parameters. That is, \\(\\pi(a|s, \\theta)\\) is the probability that action \\(a\\) is taken in state \\(s\\) when the policy parameters have value \\(\\theta\\).\nThe objective is to learn the policy parameters by following the gradient of a scalar performance measure \\(J(\\theta)\\) with respect to \\(\\theta\\). Because the goal is to maximize performance, the parameter updates follow a stochastic gradient-ascent rule: \\[\n\\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\theta_t)\n\\tag{14.1}\\] where \\(\\nabla J(\\theta_t)\\) is an estimate of the gradient of the performance measure with respect to \\(\\theta_t\\).\nAny method that follows this structure is known as a policy gradient method. When such a method also learns a value function approximation, it is referred to as an actor-critic method. In this terminology, the actor is the agent that acts. It outputs an action given the current state, according to a policy. The critic is the one who criticises or evaluates the actor’s performance by estimating the value function. The critic’s evaluation is used to improve the actor.\nFirst, we consider the episodic setting, where performance is defined as the value of the start state under the parameterised policy. Next, the continuing case is considered, where performance is defined in terms of the long-run average reward.\nAfter studying this chapter, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#textbook-readings",
    "href": "14_policy-gradient.html#textbook-readings",
    "title": "14  Policy Gradient Methods",
    "section": "14.2 Textbook readings",
    "text": "14.2 Textbook readings\nFor this module, you will need to read Chapter 13-13.7 in Sutton and Barto (2018). Read it before continuing this module. A summary of the book notation can be seen \\[here\\]\\[sutton-notation\\].\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#policy-approximation-and-its-advantages",
    "href": "14_policy-gradient.html#policy-approximation-and-its-advantages",
    "title": "14  Policy Gradient Methods",
    "section": "14.3 Policy Approximation and its Advantages",
    "text": "14.3 Policy Approximation and its Advantages\nPolicy gradient methods optimize a parameterized policy directly rather than relying on value functions for action selection. The policy, denoted \\(\\pi(a|s,\\theta)\\), depends on a vector of parameters \\(\\theta\\) and must be differentiable with respect to these parameters. In practice, to ensure exploration we generally require that the policy never becomes deterministic, i.e., that \\(\\pi(a|s,\\theta) \\in (0, 1)\\) for all \\(s, a\\).\nFor discrete action spaces, a common approach is to assign a numerical preference \\(h(s, a, \\theta)\\) to each action in each state. These preferences are then transformed into action probabilities using the softmax function:\n\\[\n\\pi(a|s,\\theta) = \\frac{e^{h(s,a,\\theta)}}{\\sum_b e^{h(s,b,\\theta)}}\n\\]\nThis ensures that \\(\\pi(a|s,\\theta) \\in (0,1)\\) and that the probabilities across all actions in a state sum to one. The softmax structure guarantees continual exploration since no action ever receives zero probability. We call this policy parameterisation soft-max in action preferences.\nThe action preferences \\(h(s, a,\\theta)\\) can be parameterised arbitrarily. For example, they might be computed by a neural network, or the preferences could be linear in features, as in Chapter 9.\nCompared to value-based methods, policy approximation offers several advantages.\n\nOne advantage of parameterizing policies with a softmax over action preferences is that the resulting stochastic policy can approach a deterministic one. As the differences between action preferences grow, the softmax distribution becomes increasingly peaked, and in the limit it becomes deterministic.\nA second advantage of parameterising policies according to the softmax in action preferences is that it enables the selection of actions with arbitrary probabilities. In problems with significant function approximation, the best approximate policy may be stochastic.\nThe policy may be a simpler function to approximate. Problems vary in the complexity of their policies and action-value functions. For some, the action-value function is simpler and thus easier to approximate. For others, the policy is simpler.\nThe choice of policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. This is often the most important reason for using a policy-based learning method.\nWith continuous policy parameterization the action probabilities change smoothly as a function of the learned parameter, whereas in \\(\\epsilon\\)-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a different action having the maximal value. This gives us stronger convergence guarantees.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#the-policy-gradient-theorem",
    "href": "14_policy-gradient.html#the-policy-gradient-theorem",
    "title": "14  Policy Gradient Methods",
    "section": "14.4 The Policy Gradient Theorem",
    "text": "14.4 The Policy Gradient Theorem\nThe policy gradient theorem provides a fundamental result showing that the gradient of the performance measure with respect to the policy parameters can be expressed without involving the derivative of the state distribution.\nTo do stochastic gradient-ascent in Equation 14.1, we need to find the gradient of the performance measure \\(J(\\theta)\\) with respect to the policy parameters \\(\\theta\\). In the episodic case, the performance is defined as the expected return starting from the initial state \\(s_0\\): \\[\nJ(\\theta) = v_{\\pi_\\theta}(s_0)\n\\] where \\(\\pi_\\theta\\) is the parametrized policy.\nGiven a state, the effect of the policy parameter on the actions, and thus on reward, can be computed in a relatively straightforward way from knowledge of the parameterization. But the effect of the policy on the state distribution is a function of the environment and is typically unknown. How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution? This can be done using the policy gradient theorem, the gradient of \\(J(\\theta)\\) can be written as \\[\n\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s,a) \\nabla \\pi(a|s,\\theta)\n\\] where \\(\\mu(s)\\) is the on-policy distribution over states under \\(\\pi\\). In Module 14.4.1 is a step-by-step proof of this result in the episodic case.\nA more convenient form of the gradient is obtained by expressing it in terms of the gradient of the logarithm of the policy: \\[\n\\nabla J(\\theta) \\propto \\mathbb{E}_\\pi \\big[ q_\\pi(S_t, A_t) \\nabla \\ln \\pi(A_t|S_t, \\theta) \\big]\n\\] This expectation is taken with respect to the trajectory distribution generated by the current policy. It states that the policy parameters should be adjusted in proportion to the product of the action-value \\(q_\\pi(S_t, A_t)\\) and the gradient of the log-probability of the action taken.\nThis relationship gives a practical way to compute the gradient using samples. The term \\(\\nabla \\ln \\pi(A_t|S_t, \\theta)\\) acts as an eligibility vector, pointing in the direction that makes the selected action more probable, while \\(q_\\pi(S_t, A_t)\\) measures how good that action was. Averaging over experience yields an unbiased estimate of the true gradient.\nThe theorem applies both to episodic and continuing tasks. In the continuing case, the average reward per time step \\(r(\\pi)\\) is used as the performance measure, and the same result holds with appropriate definitions of values and gradients.\nThe significance of the policy gradient theorem lies in providing a clean and general foundation for all policy-gradient methods. It guarantees that by following the gradient of expected performance, the learning algorithm improves the policy without needing to differentiate the complex dynamics of the state distribution. This makes it the theoretical basis for methods such as REINFORCE and actor–critic algorithms.\n\n14.4.1 Proff (episodic case)\nWe assume:\n\nfinite state and action sets,\nan episodic MDP that always terminates in finite time with probability 1,\nthe transition dynamics \\(p(s', r \\mid s, a)\\) do not depend on \\(\\theta\\),\nthe policy \\(\\pi(a \\mid s, \\theta)\\) is differentiable in \\(\\theta\\).\n\nWe write \\(v_\\pi(s)\\) and \\(q_\\pi(s,a)\\) for the value and action-value functions under policy \\(\\pi\\).\n\nExpress the state-value function in terms of the action-value function\nFor any fixed policy \\(\\pi\\), the state-value function can be written as \\[\nv_\\pi(s) = \\sum_a \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a).\n\\]\nDifferentiate the state-value function\nTake the gradient with respect to \\(\\theta\\): \\[\n\\nabla v_\\pi(s)\n= \\nabla \\left( \\sum_a \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a) \\right)\n= \\sum_a \\left[ \\nabla \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a) + \\pi(a \\mid s, \\theta)\\,\\nabla q_\\pi(s,a) \\right].\n\\]\nDefine \\[\ng(s) \\doteq \\sum_a \\nabla \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a),\n\\] so that \\[\n\\nabla v_\\pi(s) = g(s) + \\sum_a \\pi(a \\mid s, \\theta)\\,\\nabla q_\\pi(s,a).\n\\]\nUse the Bellman equation for \\(q_\\pi\\)\nThe Bellman equation for the action-value function is \\[\nq_\\pi(s,a)\n= \\sum_{s', r} p(s', r \\mid s,a)\\Bigl[r + v_\\pi(s')\\Bigr].\n\\] The transition probabilities and rewards do not depend on \\(\\theta\\), so \\[\n\\nabla q_\\pi(s,a)\n= \\sum_{s', r} p(s', r \\mid s,a)\\,\\nabla v_\\pi(s').\n\\]\nSubstitute this into the previous expression: \\[\n\\nabla v_\\pi(s)\n= g(s) + \\sum_a \\pi(a \\mid s, \\theta) \\sum_{s', r} p(s', r \\mid s,a)\\,\\nabla v_\\pi(s').\n\\]\nIntroduce the one-step transition matrix under the policy\nDefine the one-step state transition probabilities under policy \\(\\pi\\): \\[\nP^\\pi(s' \\mid s) \\doteq \\sum_a \\pi(a \\mid s, \\theta)\\,p(s' \\mid s,a),\n\\] where \\(p(s' \\mid s,a) = \\sum_r p(s', r \\mid s,a)\\).\nThen the previous expression becomes \\[\n\\nabla v_\\pi(s)\n= g(s) + \\sum_{s'} P^\\pi(s' \\mid s)\\,\\nabla v_\\pi(s').\n\\]\nThis is a recursive equation relating \\(\\nabla v_\\pi(s)\\) at one state to gradients at successor states.\nUnroll the recursion along trajectories\nWe can repeatedly substitute the expression for \\(\\nabla v_\\pi(\\cdot)\\) on the right-hand side into itself.\nDefine \\(P^\\pi_k(s \\to x)\\) as the probability of being in state \\(x\\) after exactly \\(k\\) steps when starting from state \\(s\\) and following policy \\(\\pi\\). This is the \\(k\\)-step transition probability under \\(P^\\pi\\).\nBy repeatedly expanding the recursion, we obtain \\[\n\\nabla v_\\pi(s)\n= \\sum_x \\sum_{k=0}^\\infty P^\\pi_k(s \\to x)\\,g(x).\n\\]\nIntuitively: at each state \\(x\\), the local “source term” \\(g(x)\\) contributes to the gradient at \\(s\\), weighted by how likely and how often we reach \\(x\\) from \\(s\\) under policy \\(\\pi\\).\nSpecialize to the performance measure \\(J(\\theta) = v_\\pi(s_0)\\)\nIn the episodic case, the performance measure is defined as \\[\nJ(\\theta) \\doteq v_\\pi(s_0),\n\\] where \\(s_0\\) is the (deterministic) start state.\nHence, \\[\n\\nabla J(\\theta) = \\nabla v_\\pi(s_0)\n= \\sum_x \\sum_{k=0}^\\infty P^\\pi_k(s_0 \\to x)\\,g(x).\n\\]\nDefine \\[\n\\eta(x) \\doteq \\sum_{k=0}^\\infty P^\\pi_k(s_0 \\to x).\n\\]\nIn an episodic finite-horizon or absorbing setting, \\(\\eta(x)\\) is finite and can be interpreted as the expected number of visits to state \\(x\\) per episode under policy \\(\\pi\\).\nThus, \\[\n\\nabla J(\\theta) = \\sum_x \\eta(x)\\,g(x)\n= \\sum_x \\eta(x) \\sum_a \\nabla \\pi(a \\mid x, \\theta)\\,q_\\pi(x,a).\n\\]\nIntroduce the on-policy state distribution \\(\\mu(s)\\)\nLet the normalized on-policy state distribution \\(\\mu(s)\\) be \\[\n\\mu(s) \\doteq \\frac{\\eta(s)}{\\sum_x \\eta(x)}.\n\\]\nIn an episodic setting, \\(\\sum_x \\eta(x)\\) is the expected episode length under policy \\(\\pi\\); call this constant \\(C &gt; 0\\). Then \\[\n\\eta(s) = C\\,\\mu(s),\n\\] and the gradient becomes \\[\n\\nabla J(\\theta)\n= \\sum_s C\\,\\mu(s) \\sum_a \\nabla \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a)\n= C \\sum_s \\mu(s) \\sum_a q_\\pi(s,a)\\,\\nabla \\pi(a \\mid s, \\theta).\n\\]\nSince \\(C\\) is a positive constant independent of \\(\\theta\\), we have \\[\n\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a)\\,\\nabla \\pi(a \\mid s, \\theta),\n\\] which is exactly the policy gradient theorem in the episodic case.\nOptional log-policy form\nUsing the identity \\(\\nabla ln(x) = \\nabla x / x\\), we get \\[\n\\nabla \\pi(a \\mid s, \\theta) = \\pi(a \\mid s, \\theta)\\,\\nabla \\ln \\pi(a \\mid s, \\theta),\n\\] we can rewrite the inner sum as \\[\n\\sum_a q_\\pi(s,a)\\,\\nabla \\pi(a \\mid s, \\theta)\n= \\sum_a q_\\pi(s,a)\\,\\pi(a \\mid s, \\theta)\\,\\nabla \\ln \\pi(a \\mid s, \\theta),\n\\] and thus \\[\n\\nabla J(\\theta) \\propto\n\\sum_s \\mu(s) \\sum_a \\pi(a \\mid s, \\theta)\\,q_\\pi(s,a)\\,\\nabla \\ln \\pi(a \\mid s, \\theta),\n\\] which is the expectation form used to derive REINFORCE: \\[\n\\nabla J(\\theta) \\propto \\mathbb{E}_\\pi\\!\\left[ q_\\pi(S_t, A_t)\\,\\nabla \\ln \\pi(A_t \\mid S_t, \\theta) \\right].\n\\]\n\nThis completes the proof of the policy gradient theorem in the episodic case.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#reinforce-monte-carlo-policy-gradient",
    "href": "14_policy-gradient.html#reinforce-monte-carlo-policy-gradient",
    "title": "14  Policy Gradient Methods",
    "section": "14.5 REINFORCE: Monte Carlo Policy Gradient",
    "text": "14.5 REINFORCE: Monte Carlo Policy Gradient\nREINFORCE is the simplest policy-gradient algorithm. It performs stochastic gradient ascent on the expected return by using Monte Carlo returns to estimate the gradient direction. That is, it is applicable to episodic tasks because it relies on full-episode returns.\nOur objective is maximize the performance objective: \\[\nJ(\\theta) = v_{\\pi_\\theta}(s_0)\n\\]\nUsing gradient ascent: \\[\n\\theta_{t+1} = \\theta_t + \\alpha\\,\\widehat{\\nabla J(\\theta_t)}\n\\]\nThe Policy Gradient Theorem gives:\n\\[\n\\begin{alignat}{3}\n    \\nabla J(\\theta) &\\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s,a) \\nabla \\pi(a|s,\\theta) \\\\\n    &= \\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a)\\nabla\\,\\pi(a \\mid S_t, \\theta)\\right]\n        &&\\qquad\\text{(mean given $\\mu(s)$, $S_t$ now stochastic)} \\\\\n    &= \\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a) \\pi(a \\mid S_t, \\theta)\\,\\nabla \\ln \\pi(a \\mid S_t, \\theta)\\right]\n        &&\\qquad\\text{(use $\\nabla \\pi(a \\mid S_t, \\theta) = \\pi(a \\mid s, \\theta)\\,\\nabla \\ln \\pi(a \\mid s, \\theta)$)} \\\\\n    &= \\mathbb{E}_\\pi\\left[q_\\pi(S_t, A_t)\\,\\nabla \\ln \\pi(A_t|S_t, \\theta)\\right]\n        &&\\qquad\\text{(replace $a$ with stochastic $A_t\\sim\\pi$)} \\\\\n    &= \\mathbb{E}_\\pi\\left[G_t\\,\\nabla \\ln \\pi(A_t|S_t, \\theta)\\right]\n        &&\\qquad\\text{($\\mathbb{E}_\\pi[G_t|A_t, S_t] = q_\\pi(S_t, A_t)$)} \\\\\n\\end{alignat}\n\\]\nNote, since \\(q_\\pi(S_t, A_t)\\) cannot be computed exactly, we use the Monte Carlo estimate \\(G_t\\). The update now becomes: \\[\n\\theta_{t+1}\n= \\theta_t + \\alpha\\,G_t \\nabla \\ln \\pi(A_t|S_t, \\theta_t)\n\\]\nObservations:\n\nThe gradient \\[\n  \\nabla \\ln \\pi(A_t|S_t, \\theta) = \\frac{\\nabla \\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t,\\theta)}.\\] Hence \\(\\nabla \\ln \\pi(A_t|S_t,\\theta)\\) is the direction that increases the probability of action \\(A_t\\) divided by the probability of taking that action. If the action had low probability, the update is amplified and if the action had high probability, the update is weaker.\nThe return \\(G_t\\) is used for adjustment. Good returns push probability up, bad returns push it down. Good outcomes imply increased probability of the actions that led to them. Bad outcomes imply decreased probability.\n\nWe do Monte Carlo, ie. no bootstrapping (high variance but unbiased).\nThis is direct policy optimization with no value function.\n\nThe vector \\(\\nabla \\ln \\pi(A_t|S_t, \\theta_t)\\) is called the eligibility vector. Note this is the only place where the policy parametrization appears.\nKey Formulas\nPolicy gradient estimate: \\[\n\\widehat{\\nabla J(\\theta)} = G_t\\,\\nabla \\ln \\pi(A_t|S_t,\\theta)\n\\]\nREINFORCE update: \\[\n\\theta \\leftarrow \\theta + \\alpha G_t \\nabla \\ln \\pi(A_t|S_t,\\theta)\n\\]\nReturn definition (episodic): \\[\nG_t = \\sum_{k=t+1}^T \\gamma^{k-t-1} R_k\n\\]\nLog-policy derivative identity: \\[\n\\nabla \\ln \\pi(a|s,\\theta) = \\frac{\\nabla \\pi(a|s,\\theta)}{\\pi(a|s,\\theta)}\n\\]\n\nPseudo code for REINFORCE is given in Fig. 14.1.\n\n\n\n\n\n\n\n\nFigure 14.1: REINFORCE: Monte Carlo Policy Gradient Control (episodic) (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#reinforce-with-baseline",
    "href": "14_policy-gradient.html#reinforce-with-baseline",
    "title": "14  Policy Gradient Methods",
    "section": "14.6 REINFORCE with Baseline",
    "text": "14.6 REINFORCE with Baseline\nThe original REINFORCE algorithm updates the policy parameters using the full Monte Carlo return: \\[\n\\theta_{t+1} = \\theta_t + \\alpha\\,G_t\\,\\nabla \\ln \\pi(A_t|S_t,\\theta_t).\n\\] This update is unbiased but typically has very high variance. To reduce this variance, a baseline function \\(b(s)\\) can be subtracted from the return. This does not change the expected value of the gradient but can greatly improve learning stability.\nThe key idea is to replace the return \\(G_t\\) with the advantage-like term \\(G_t - b(S_t)\\). The new update rule becomes: \\[\n\\theta_{t+1}\n= \\theta_t + \\alpha\\,(G_t - b(S_t))\\,\\nabla \\ln \\pi(A_t|S_t,\\theta_t).\n\\] The baseline may depend on the state but must not depend on the action. If it did depend on the action, it would bias the estimate of the gradient. The reason it does not introduce bias is: \\[\n\\sum_a b(s)\\,\\nabla \\pi(a|s,\\theta) = b(s)\\,\\nabla \\sum_a \\pi(a|s,\\theta) = b(s)\\,\\nabla 1 = 0.\n\\] Thus, subtracting \\(b(s)\\) alters only variance, not the expectation.\nA natural and effective choice for the baseline is the state-value function: \\[\nb(s) = \\hat v(s, w),\n\\] where the parameter vector \\(w\\) is learned from data. The value-function parameters are updated by a Monte Carlo regression method: \\[\nw \\leftarrow w + \\alpha_w\\,(G_t - \\hat v(S_t,w))\\,\\nabla \\hat v(S_t,w).\n\\] This produces a critic that approximates how good each state is on average. The policy update (the actor) then adjusts the probabilities in proportion to how much better or worse the return was compared to what is expected for the state: \\[\n\\theta \\leftarrow \\theta + \\alpha_\\theta\\,(G_t - \\hat v(S_t,w))\\,\\nabla \\ln \\pi(A_t|S_t,\\theta).\n\\]\nREINFORCE with baseline remains a Monte Carlo method: it requires full-episode returns and performs updates only after the episode ends. It still provides unbiased estimates of the policy gradient. The improvement is purely variance reduction, which can significantly accelerate learning. Empirically, adding a learned baseline commonly leads to much faster convergence, especially when episode returns vary widely.\nThese ideas capture the essence of REINFORCE with baseline: the gradient direction is preserved, variance is reduced, and learning becomes more efficient.\nKey formulas\n\\[\n\\nabla J(\\theta) \\propto \\mathbb{E}_\\pi[(G_t - b(S_t))\\,\\nabla \\ln \\pi(A_t|S_t,\\theta)],\n\\] and the baseline restriction: \\[\nb(s)\\text{ must not depend on }a.\n\\] Using a value-function baseline: \\[\nb(s) = \\hat v(s,w),\n\\] leads to a combined learning rule for actor and critic: \\[\n\\begin{aligned}\nw &\\leftarrow w + \\alpha_w\\,(G_t - \\hat v(S_t,w))\\,\\nabla \\hat v(S_t,w), \\\\\n\\theta &\\leftarrow \\theta + \\alpha_\\theta\\,(G_t - \\hat v(S_t,w))\\,\\nabla \\ln \\pi(A_t|S_t,\\theta).\n\\end{aligned}\n\\]\nPseudo code for REINFORCE with baseline algorithm is given in Fig. 14.2.\n\n\n\n\n\n\n\n\nFigure 14.2: REINFORCE with baseline: Monte Carlo Policy Gradient Control (episodic) (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#actor-critic-methods",
    "href": "14_policy-gradient.html#actor-critic-methods",
    "title": "14  Policy Gradient Methods",
    "section": "14.7 Actor-Critic Methods",
    "text": "14.7 Actor-Critic Methods\nActor-critic methods extend the REINFORCE with baseline algorithm by replacing the full Monte Carlo return with a bootstrapped estimate. In these methods, the policy is the actor and the value function is the critic. The critic evaluates how good the current state is, and the actor uses this evaluation to adjust the policy parameters.\nThe key observation motivating actor-critic methods is that in REINFORCE with baseline, the critic (the state-value function) is used only as a baseline for variance reduction and is evaluated using Monte Carlo returns. Because Monte Carlo returns can have large variance, learning becomes slow. A natural improvement is to let the critic use temporal-difference style updates, producing a more immediate and less variable assessment of action quality.\nTo achieve this, actor-critic methods use the one-step return: \\[\nG_{t:t+1} = R_{t+1} + \\gamma \\hat v(S_{t+1}, w),\n\\] which leads to the temporal-difference error: \\[\n\\delta_t = R_{t+1} + \\gamma \\hat v(S_{t+1}, w) - \\hat v(S_t, w).\n\\] This error serves two roles: it updates the critic by TD learning and it provides the advantage signal for the actor.\nThe critic update becomes: \\[\nw \\leftarrow w + \\alpha_w \\,\\delta_t\\, \\nabla \\hat v(S_t, w).\n\\] The actor update becomes: \\[\n\\theta \\leftarrow \\theta + \\alpha_\\theta\\,\\delta_t\\,\\nabla \\ln \\pi(A_t|S_t, \\theta).\n\\]\nThis replaces the Monte Carlo return \\(G_t\\) used in REINFORCE with the more immediate \\(\\delta_t\\). Although \\(\\delta_t\\) introduces bias (because \\(\\hat v\\) is only an approximation), the resulting variance reduction often greatly improves learning efficiency.\nActor-critic methods can be seen as the policy-gradient analogue of SARSA, in the sense that they learn online, incrementally, and from single-step bootstrapped targets. Multi-step versions and methods with eligibility traces can also be built in direct analogy with the \\(n\\)-step TD algorithms.\nThe introduction of bootstrapping means that actor-critic methods are no longer pure Monte Carlo. They combine the benefits of policy gradient methods (direct optimization of the policy) with the benefits of TD learning (low variance and online updates). This blend makes them practical for long-horizon tasks where Monte Carlo variance would be problematic.\n\nActor-critic methods form the foundation for many modern reinforcement learning algorithms, including those used in deep RL. The version introduced here is the simplest: one-step, on-policy, with a tabular or approximate state-value critic.\nPseudo code for the one-step Actor-Critic algorithm is given in Fig. 14.3.\n\n\n\n\n\n\n\n\nFigure 14.3: One-step Actor-Critic (episodic) (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#policy-gradient-for-continuing-problems",
    "href": "14_policy-gradient.html#policy-gradient-for-continuing-problems",
    "title": "14  Policy Gradient Methods",
    "section": "14.8 Policy Gradient for Continuing Problems",
    "text": "14.8 Policy Gradient for Continuing Problems\nIn the continuing setting, there are no episodes, and returns do not naturally terminate. Because of this, the performance measure used in episodic policy gradients \\(J(\\theta) = v_{\\pi}(s_0)\\) no longer applies. Instead, policy gradient methods for continuing tasks optimize the average reward: \\[\nr(\\pi) = \\sum_s \\mu(s)\\sum_a \\pi(a|s)\\sum_{s',r} p(s',r|s,a)\\, r.\n\\] Here, the distribution \\(\\mu(s)\\) is the stationary on-policy distribution over states under the current policy. This describes how often the agent visits each state in the long-run average sense.\nThe policy gradient theorem still holds in the continuing case, but with an important difference: the constant of proportionality becomes exactly 1. Thus, \\[\n\\nabla r(\\pi) = \\sum_s \\mu(s) \\sum_a q_\\pi(s,a)\\,\\nabla \\pi(a|s,\\theta).\n\\] This means the gradient depends only on the action-value function and the stationary distribution, not on the derivative of the distribution with respect to the policy parameters. This is what makes policy gradient methods tractable even in continuing tasks.\nThe relevant action-value function becomes the differential action-value: \\[\nq_\\pi(s,a) = \\mathbb{E}_\\pi\\bigl[\\,G_t \\mid S_t=s, A_t=a\\,\\bigr],\n\\] where the return is defined as: \\[\nG_t = R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + \\cdots.\n\\] This return subtracts the average reward so that long-term returns are finite and meaningful in a continuing environment. The corresponding value function is the differential value: \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t=s].\n\\]\nTo construct a Monte Carlo-style policy gradient, one replaces \\(q_\\pi\\) with sampled differential returns. The resulting gradient estimate is: \\[\n\\nabla r(\\pi) \\approx \\mathbb{E}\\left[(G_t)\\,\\nabla \\ln \\pi(A_t|S_t,\\theta)\\right].\n\\] As in the episodic setting, baselines can be used to reduce variance. The baseline must not depend on the action, and a natural choice is the differential value function \\(v_\\pi(s)\\). Subtracting it leads to: \\[\n\\nabla r(\\pi) \\approx \\mathbb{E}\\left[(G_t - v_\\pi(S_t))\\,\\nabla \\ln \\pi(A_t|S_t,\\theta)\\right].\n\\] This produces an unbiased gradient estimate while helping to control variance in ongoing tasks.\nA major difference from the episodic case is that the average reward \\(r(\\pi)\\) must be estimated during learning. This can be done incrementally, often using a running average of observed rewards. Differential actor-critic methods incorporate this into the critic: they estimate both the differential value function and the average reward, then use the temporal-difference error: \\[\n\\delta_t = R_{t+1} - \\hat r + \\hat v(S_{t+1}) - \\hat v(S_t),\n\\] which is the natural TD error for average-reward problems. This error then drives both critic and actor updates.\nThe essential message is that policy gradient methods extend naturally to the continuing case, but the formulation shifts from episodic returns to average reward and differential values. The policy gradient theorem still applies with almost the same structure, enabling the creation of both Monte Carlo and actor-critic algorithms for continuing tasks.\nPseudo code for the Actor-Critic with eligibility traces algorithm is given in Fig. 14.4. Note that we have not considered eligibility traces in this course. However, think of it as the one-step actor-critic is TD(0), i.e. \\(\\lambda^\\textbf w = \\lambda^\\theta  = 0\\) and if increase these weights you come closer to monte carlo (\\(\\lambda^\\textbf w = \\lambda^\\theta  = 1\\)).\n\n\n\n\n\n\n\n\nFigure 14.4: Actor-critic with eligibility traces (continuing) (Sutton and Barto 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#policy-parameterisation-for-continuous-actions",
    "href": "14_policy-gradient.html#policy-parameterisation-for-continuous-actions",
    "title": "14  Policy Gradient Methods",
    "section": "14.9 Policy Parameterisation for Continuous Actions",
    "text": "14.9 Policy Parameterisation for Continuous Actions\nUntil now, we have assumed a discrete action space so the softmax function could be used to approximate the policy. If continuous action spaces, meaning actions are real-valued (or vector-valued), discrete softmax policies are no longer suitable. Instead, policies are represented as parameterised probability density functions over continuous actions.\nThe main idea is to model the policy as: \\[\n\\pi(a \\mid s, \\theta) = \\text{a differentiable density over } a,\n\\] so that gradients concerning the parameters can be calculated and employed in policy gradient updates.\nA common parameterisation is the univariate Gaussian or Normal distribution: \\[\n\\pi(a \\mid s, \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2(s, \\theta)}} \\exp\\left( -\\frac{(a - \\mu(s, \\theta))^2}{2\\sigma^2(s, \\theta)} \\right),\n\\] where both the mean \\(\\mu(s)\\) and standard deviation \\(\\sigma(s)\\) may depend on the state and are parameterised by separate sets of weights \\(\\theta = (\\theta_\\mu, \\theta_\\sigma)\\). Note we slightly misuse the notation here. The \\(\\pi\\) on the left is notation for the policy, while the \\(\\pi\\) on the right simply denotes the number \\(\\pi\\).\nUsually, the mean is expressed as a linear function of features \\[\\mu(s, \\theta) = {\\theta_\\mu}^\\top \\textbf x_\\mu(s).\\]\nThe variance must be kept strictly positive (e.g., via taking the exponential) \\[\\sigma^2(s, \\theta) = \\exp({\\theta_\\sigma}^\\top \\textbf x_\\sigma(s)).\\]\nRemaining is to calculate the eligibility vector \\(\\nabla \\ln \\pi(A_t|S_t, \\theta_t)\\) for the algorithm:\n\\[\n\\begin{align}\n\\nabla \\ln \\pi(a|s, \\theta_\\mu)\n  &= \\frac{a-\\mu(s, \\theta_\\mu)}{\\sigma(s, \\theta_\\sigma)^2}\\, \\nabla \\mu(s, \\theta_\\mu) \\\\\n\\nabla \\ln \\pi(a|s, \\theta_\\mu)\n  &= \\left(\\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2} - 1\\right)\n\\nabla\\ln\\sigma(s, \\theta_\\sigma).\n\\end{align}\n\\]\nThese two equations can be found by first note that \\[\n\\ln \\pi(a \\mid s, \\theta)\n=\n-\\ln(\\sqrt{2\\pi})\n-\\ln \\sigma(s, \\theta_\\sigma)\n-\\frac{(a - \\mu(s, \\theta_\\mu))^2}{2\\sigma(s, \\theta_\\sigma)^2}.\n\\]\nTaking the derivative w.r.t. \\(\\theta_\\mu\\): \\[\n\\begin{align}\n\\nabla \\ln \\pi(a|s, \\theta_\\mu)\n  &= -\\frac{1}{2\\sigma(s, \\theta_\\sigma)^2}2(a-\\mu(s, \\theta_\\mu))(-1)\\nabla \\mu(s, \\theta_\\mu) \\\\\n  &= \\frac{a-\\mu(s, \\theta_\\mu)}{\\sigma(s, \\theta_\\sigma)^2}\\, \\nabla \\mu(s, \\theta_\\mu).\n\\end{align}\n\\]\nTaking the derivative w.r.t. \\(\\theta_\\sigma\\): \\[\n\\begin{align}\n\\nabla \\ln \\pi(a|s, \\theta_\\mu)\n  &= -\\nabla\\ln\\sigma(s, \\theta_\\sigma)  \n  - \\frac{(-2)(a-\\mu(s, \\theta_\\mu))^2}{2\\sigma(s, \\theta_\\sigma)^3}\\nabla\\sigma(s, \\theta_\\sigma)  \\\\\n  &= -\\nabla\\ln\\sigma(s, \\theta_\\sigma)  \n  + \\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2}\n  \\frac{\\nabla\\sigma(s, \\theta_\\sigma)}{\\sigma(s, \\theta_\\sigma)} \\\\\n  &= -\\nabla\\ln\\sigma(s, \\theta_\\sigma)  \n  + \\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2}\n  \\nabla\\ln\\sigma(s, \\theta_\\sigma) \\\\\n  &= \\left(\\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2} - 1\\right)\\nabla\\ln\\sigma(s, \\theta_\\sigma).\n\\end{align}\n\\]\nHence \\[\n\\nabla \\ln \\pi(a|s, \\theta)\n  = \\frac{a-\\mu(s, \\theta_\\mu)}{\\sigma(s, \\theta_\\sigma)^2}\\, \\nabla \\mu(s, \\theta_\\mu) +\n  \\left(\\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2} - 1\\right)\n\\nabla\\ln\\sigma(s, \\theta_\\sigma).\n\\]\nThe first term shifts the mean towards actions that led to better returns; the second term adjusts the variance depending on how surprising the sampled action was relative to the current policy.\nUsing linear features we get \\[\n\\nabla \\ln \\pi(a|s, \\theta)\n  = \\frac{a-\\mu(s, \\theta_\\mu)}{\\sigma(s, \\theta_\\sigma)^2}\\, \\textbf x(s, \\theta_\\mu) +\n  \\left(\\frac{(a-\\mu(s, \\theta_\\mu))^2}{\\sigma(s, \\theta_\\sigma)^2} - 1\\right)\n\\textbf x(s, \\theta_\\sigma).\n\\]\n\nThe choice of parameterization has important effects. If the variance is too small, exploration collapses; if too large, gradient estimates become noisy. Learning both mean and variance enables adaptive exploration: the variance shrinks in well-understood regions and grows where uncertainty is higher.\nOnce a differentiable density is available, all previous machinery for policy gradients applies unchanged. The policy gradient theorem still holds, as it does not depend on action space cardinality. Actor-critic methods remain preferable because they reduce variance, even more critical in continuous action settings where gradients may be noisier.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#mixed-action-spaces",
    "href": "14_policy-gradient.html#mixed-action-spaces",
    "title": "14  Policy Gradient Methods",
    "section": "14.10 Mixed Action Spaces",
    "text": "14.10 Mixed Action Spaces\nWhen an action includes both continuous and discrete components, the policy must represent a joint distribution over this mixed action space. Policy gradient methods handle this naturally as long as the policy is differentiable.\nSuppose an action is: \\[\na = (a^{\\text{disc}},\\, a^{\\text{cont}}).\n\\]\nA standard and convenient factorization is: \\[\n\\pi(a \\mid s)\n= \\pi(a^{\\text{disc}} \\mid s)\\,\n  \\pi(a^{\\text{cont}} \\mid s, a^{\\text{disc}}).\n\\]\nThis means:\n\nFirst choose the discrete action component.\nThen choose the continuous parameters conditioned on the discrete choice.\n\nThis factorization is compatible with the policy gradient theorem. The log-policy splits naturally: \\[\n\\ln \\pi(a \\mid s)\n=\n\\ln \\pi(a^{\\text{disc}} \\mid s)\n+\n\\ln \\pi(a^{\\text{cont}} \\mid s, a^{\\text{disc}}).\n\\]\nThus the gradient becomes: \\[\n\\nabla_\\theta \\ln \\pi(a \\mid s)\n=\n\\nabla_\\theta \\ln \\pi(a^{\\text{disc}} \\mid s)\n+\n\\nabla_\\theta \\ln \\pi(a^{\\text{cont}} \\mid s, a^{\\text{disc}}).\n\\]\nA REINFORCE-style policy gradient update takes the form: \\[\n\\theta \\leftarrow \\theta\n+ \\alpha\\, G_t\\,\n\\big(\n\\nabla \\ln \\pi(a^{\\text{disc}}|s)\n+\n\\nabla \\ln \\pi(a^{\\text{cont}}|s,a^{\\text{disc}})\n\\big).\n\\]\nBoth the discrete and continuous components contribute independently to the gradient.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#summary",
    "href": "14_policy-gradient.html#summary",
    "title": "14  Policy Gradient Methods",
    "section": "14.11 Summary",
    "text": "14.11 Summary\nRead Chapter 13.8 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "14_policy-gradient.html#exercises",
    "href": "14_policy-gradient.html#exercises",
    "title": "14  Policy Gradient Methods",
    "section": "14.12 Exercises",
    "text": "14.12 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\nYou may solve the exercises in the corresponding sections in this \\[Colab notebook\\]\\[colab-14-policy-gradient\\].\n\n14.12.1 Exercise\n???\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Silver, David. 2015. “Lectures on Reinforcement Learning.”\nhttps://www.davidsilver.uk/teaching/.\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An\nIntroduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "99_1_appdx.html",
    "href": "99_1_appdx.html",
    "title": "Appendix A — Python",
    "section": "",
    "text": "A.1 Colab\nPython is a popular and beginner-friendly programming language known for its simplicity and readability. It’s widely used in web development, data science, automation, artificial intelligence, and more. Whether you’re analyzing data, building a website, or creating software, Python is a powerful and versatile tool to learn.\nDuring this course we are going to use Google Colab which is a hosted Jupyter notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. That is, Colab runs in your browser and you do not have to install anything on your computer. With a Jupyter notebook you can weave you code together with text.\nTo be familiar with Colab do the following:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#colab",
    "href": "99_1_appdx.html#colab",
    "title": "Appendix A — Python",
    "section": "",
    "text": "If you do not have a Google account create one. Note if you have a gMail then you already have a Google account.\nOpen and do this tutorial.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#laptop-setup",
    "href": "99_1_appdx.html#laptop-setup",
    "title": "Appendix A — Python",
    "section": "A.2 Laptop setup",
    "text": "A.2 Laptop setup\nIf you prefer to have Python installed on your laptop, then do the following:\n\nGo to the official Python website: https://www.python.org/downloads\nClick the download button for your operating system (Windows, macOS, or Linux).\nRun the installer.\nImportant: Make sure to check the box that says “Add Python to PATH” during installation.\nFollow the prompts to complete the installation.\n\nTo verify the installation, open a terminal or command prompt and type:\npython --version\nTo install an IDE for working with your code, you may install Visual Studio Code (VSCode):\n\nGo to the VSCode website: https://code.visualstudio.com\nDownload and install the version for your operating system.\nOpen VSCode after installation.\nInstall the Python extension in VSCode:\n\nGo to the Extensions tab (or press Ctrl+Shift+X)\nSearch for “Python” and install the one published by Microsoft\n\n\nYou’re now ready to start coding in Python!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#learning-python",
    "href": "99_1_appdx.html#learning-python",
    "title": "Appendix A — Python",
    "section": "A.3 Learning Python",
    "text": "A.3 Learning Python\nIf you are new to Python you may have a look at some DataCamp Courses. First you HAVE TO SIGNUP using this link. Afterwards have a look at these courses:\n\nIntroduction to Python - Learn the basics of Python programming, including variables, data types, and control flow.\nIntermediate Python - Build on your basic knowledge with functions, loops, and working with Python libraries.\nData Manipulation with pandas - pandas is the world’s most popular Python library, used for everything from data manipulation to data analysis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#sec-coding-convention",
    "href": "99_1_appdx.html#sec-coding-convention",
    "title": "Appendix A — Python",
    "section": "A.4 Coding/naming convention",
    "text": "A.4 Coding/naming convention\nThe main reason for using a consistent set of coding conventions is to standardize the structure and coding style of an application so that you and others can easily read and understand the code. Good coding conventions result in precise, readable, and unambiguous source code that is consistent with other language conventions and as intuitive as possible.\nDifferent ways of naming you variables exists. You are advised to adopt a naming convention. PEP 8 is the official style guide for Python code. It provides conventions for writing readable, consistent, and maintainable Python programs. Adhering to PEP 8 helps teams collaborate more effectively and improves code quality across projects. An overview is given below.\nNaming Conventions\n\n\n\n\n\n\n\n\nType\nConvention\nExample\n\n\n\n\nVariable\nlower_case_with_underscores\ntotal_count\n\n\nFunction\nlower_case_with_underscores\nprocess_data()\n\n\nClass\nCapitalizedWords\nDataProcessor\n\n\nConstant\nALL_CAPS_WITH_UNDERSCORES\nMAX_SIZE\n\n\nMethod (in class)\nlower_case_with_underscores\ncompute_value()\n\n\nModule/Package\nlowercase or lower_case_with_underscores\nutils, data_loader\n\n\nPrivate/Internal\n_single_leading_underscore\n_helper_function()\n\n\nStrong “private”\n__double_leading_underscore\n__internal_method()\n\n\nMagic method (dunder)\n__double_leading_and_trailing__\n__init__, __str__\n\n\n\nIndentation\n\nUse 4 spaces per indentation level.\nDo not use tabs.\nUse hanging indents for long statements, and align with the opening delimiter.\n\nLine Length\n\nLimit lines to 79 characters.\nFor docstrings and comments, limit to 72 characters.\n\nWhitespace\n\nAvoid extra spaces:\n\nNo space inside parentheses/brackets/braces: func(a, b)\nNo space before commas, colons, or semicolons.\nOne space after commas and around binary operators: x = a + b\n\nUse blank lines to separate functions and classes.\n\nComments\n\nUse comments to explain why, not just what.\nBlock comments:\n\nUse full sentences.\nStart with a capital letter and end with a period.\n\nInline comments:\n\nUse sparingly.\nLeave two spaces before the #, and one space after.\n\n\nDocstrings\n\nUse triple double quotes: \"\"\"This is a docstring.\"\"\"\nAll public modules, functions, classes, and methods should have docstrings.\nFirst line should be a short summary.\nLeave a blank line after the summary if more detail follows.\nExample:\ndef add(a, b):\n    \"\"\"\n    Return the sum of two numbers.\n\n    Parameters:\n    a (int or float): The first number.\n    b (int or float): The second number.\n\n    Returns:\n    int or float: The sum of a and b.\n    \"\"\"\n    return a + b",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#debugging",
    "href": "99_1_appdx.html#debugging",
    "title": "Appendix A — Python",
    "section": "A.5 Debugging",
    "text": "A.5 Debugging\nCode development and data analysis always require a bit of trial and error. This Colab notebook briefly cover some options.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_1_appdx.html#r-and-python-packages",
    "href": "99_1_appdx.html#r-and-python-packages",
    "title": "Appendix A — Python",
    "section": "A.6 R and Python packages",
    "text": "A.6 R and Python packages\nIf you are used to do data transformation in R then this table may be useful.\n\n\n\n\n\n\n\n\n\nR Package\nPurpose\nPython Equivalent(s)\nNotes\n\n\n\n\ndplyr\nData manipulation (filter, mutate, etc.)\npandas, dfply, siuba\npandas is standard; dfply and siuba mimic dplyr syntax with pipes and tidy verbs\n\n\nggplot2\nData visualization (Grammar of Graphics)\nplotnine\nClosest match in syntax and philosophy; uses + for layers like ggplot2\n\n\ntidyr\nData tidying (pivoting, reshaping)\npandas, pyjanitor\npandas handles pivot, melt; pyjanitor adds more tidy-like helpers\n\n\nreadr\nRead/write CSV\npandas\nUse read_csv(), to_csv()\n\n\njsonlite\nRead/write JSON\njson (standard), pandas\njson for raw files; pandas.read_json() for tabular JSON\n\n\nreadxl / writexl\nRead/write Excel\npandas, openpyxl, xlsxwriter\npandas integrates with Excel libraries for reading/writing .xlsx\n\n\ngooglesheets4\nGoogle Sheets I/O\ngspread, gspread-pandas\nPython requires Google API setup; gspread-pandas integrates with DataFrames",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "99_2_appdx.html",
    "href": "99_2_appdx.html",
    "title": "Appendix B — Getting help",
    "section": "",
    "text": "We all get stuck sometimes and need some help. Below are some advises on how to help yourself and ask for help:\nIf you have a question:\n\nAsk it during the lecture or in the breaks\nUse generative AI (Gemini, ChatGPT, Copilot, …) as your mentor. GAI is in general good at giving hints for programming tasks. Use it as a mentor and not to give you the solution. For instance, ask “Given me an example on a Hello world procedure in Python. Explain careful the code since I am new and want to learn.”\nGoogle is your friend. This is always the next step. Try searches like “python dfply mask”, “python pandas”, etc.\nAbout programming errors. First try to understand the error message and solve the problem. Then you may\n\nTry to debug your code by inserting print statements in your code or run single lines using cmd+shift+enter.\nUse a debugger. Further details about debugging can be seen here.\n\n\nIf you can’t find an answer then it is time to ask on-line. I recommend asking a question at stackoverflow. To make your question effective, the idea is to make things as easy as possible for someone to answer. The process of providing a good minimal reproducible example (reprex) often causes you to answer your own question! See also Stack Exchange’s ‘How to ask’.\n\nNote: help using mail correspondence is not supported!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Getting help</span>"
    ]
  }
]