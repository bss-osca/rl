[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "",
    "text": "About the course notes\nThis site contains course notes for the course “Reinforcement Learning for Business” held at Aarhus BSS. It consists of a set of learning modules. The course is an elective course mainly for the Operations and Supply Chain Analytics and Business Intelligence programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of this module, you are expected to:\n\nUnderstand the prerequisites and the goals for the course.\nHave downloaded the textbook.\nKnow how the course is organized.\nInstalled R and RStudio.\nAnnotated the online notes.\n\nThe learning outcomes relate to the overall learning goals number 3, 5 and 6 of the course.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#purpose-of-the-course",
    "href": "index.html#purpose-of-the-course",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Purpose of the course",
    "text": "Purpose of the course\nThe purpose of this course is to give an introduction and knowledge about reinforcement learning (RL).\nRL may be seen as\n\nAn approach of modelling sequential decision making problems.\nAn approach for learning good decision making under uncertainty from experience.\nMathematical models for learning-based decision making.\nTrying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.\nEstimating and finding near optimal decisions of a stochastic process with sequential decision making.\nA model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.\n\nRL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions.\nA classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge.\nThe course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#learning-goals-of-the-course",
    "href": "index.html#learning-goals-of-the-course",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Learning goals of the course",
    "text": "Learning goals of the course\nAfter having participated in the course, you must, in addition to achieving general academic skills, demonstrate:\nKnowledge of\n\nRL for Bandit problems\nMarkov decision processes and ways to optimize them\nthe exploration vs exploitation challenge in RL and approaches for addressing this challenge\nthe role of policy evaluation with stochastic approximation in the context of RL\n\nSkills to\n\ndefine the key features of RL that distinguishes it from other machine learning techniques\ndiscuss fundamental concepts in RL\ndescribe the mathematical framework of Markov decision processes\nformulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives\napply fundamental techniques, results and concepts of RL on selected RL problems.\ngiven an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model)\n\nCompetences to\n\nidentify areas where RL are valuable\nselect and apply the appropriate RL model for a given business problem\ninterpret and communicate the results from RL",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#reinforcement-learning-textbook",
    "href": "index.html#reinforcement-learning-textbook",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Reinforcement learning textbook",
    "text": "Reinforcement learning textbook\nThe course uses the free textbook Reinforcement Learning: An Introduction by Sutton and Barto (2018). The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course.\nSutton and Barto’s book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it’s totally free. Just follow the citation link to download it.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#course-organization",
    "href": "index.html#course-organization",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Course organization",
    "text": "Course organization\nEach week considers a learning module. A learning module is related to a chapter in the textbook. The learning path in a typical week are\n\nBefore lectures: Read the chapter in the textbook and consider the extra module material.\nLectures (at campus).\nAfter lectures: Module Exercises (in groups).\n\nLectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials are given and we focus on a specific RL problem.\nThis module gives a short introduction to the course. Next, the site consists of different parts each containing teaching modules about specific topics:\n\nPart I gives you a general introduction to RL and the bandit problem.\nPart II consider RL sequential decision problems where the state and action spaces are small enough so values can be represented as arrays, or tables. We start by considering bandit problems (Module @ref(mod-bandit)) a RL problem in which there is only a single state. Next, Markov decision processes (the full model known) are considered as a general modelling framework (Module @ref(mod-mdp-1)) and the concept of policies and value functions are discussed (Module @ref(mod-mdp-2)). Model-based algorithms for finding the optimal policy (dynamic programming) are given in Module @ref(mod-dp). The next modules consider model-free methods for finding the optimal policy, i.e. methods that do not require full knowledge of the transition probabilities and rewards of the process. Monte Carlo sampling methods are presented in Module @ref(mod-mc) and …\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n   \nThe appendix contains different modules that may be helpful for you including hints on how to work in groups, how to get help if you are stuck and how to annotate the course notes.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#programming-software",
    "href": "index.html#programming-software",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Programming software",
    "text": "Programming software\nWe use R as programming software and it is assumed that you are familiar with using R. R is a programming language and free software environment. R can be run from a terminal but in general you use an IDE (integrated development environment) RStudio for running R and to saving your work. R and RStudio can either be run from your laptop or using RStudio Cloud which run R in the cloud using your browser.\nIt is assumed as a prerequisite that you know how to use R. If you need a brush-up on your R programming skills then have a look at Module @ref(mod-r-setup) in the appendix.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "index.html#ack",
    "href": "index.html#ack",
    "title": "Reinforcement Learning for Business (RL)",
    "section": "Acknowledgements and license",
    "text": "Acknowledgements and license\nMaterials are taken from various places:\n\nThe notes are based on Sutton and Barto (2018).\nThe bookdown skeleton and some notes are based on the Tools for Analytics course.\nSome notes are adopted from Scott Jeen, Bryn Elesedy and Peter Goldsborough.\nSome slides are inspired by the RL specialization at Coursera.\nSome exercises are taken from Sutton and Barto (2018) and modified slightly.\n\nI would like to thank all for their inspiration.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0.\n## Exercises {#sec-intro-ex -}\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\nExercise - How to annotate\nThe online course notes can be annotated using hypothes.is. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help indicate spell errors, unclear content etc. in the notes.\n\n\nSign-up at hypothes.is. If you are using Chrome you may also install the Chrome extension.\nGo back to this page and login in the upper right corner (there should be some icons e.g. &lt;).\nSelect some text and try to annotate it using both a private and public annotation (you may delete it again afterwards).\nGo to the slides for this module and try to annotate the page with a private comment.\n\n\n\nExercise - Templates\nA template in RMarkdown of the course notes and exercises are available at GitHub. You can download the repository and keep your own notes during the course by having an R project with it.\n\nOpen R studio and do: File &gt; New Project … &gt; Version Control &gt; Git. Add https://github.com/bss-osca/rl-student as repository url and create the project.\nRun renv::restore() from the R command line to install needed packages (this may take some time). If you experience errors then try to install the packages one at a time using install.packages(\"pkg name\").\nOpen e.g. the file 01_rl-intro.Rmd and try to knit it using the Knit button in the upper left corner. A html file with the output will be made. You should be able to add your own notes and solve the exercises using the Rmd file for each module.\n\n\n\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "About the course notes"
    ]
  },
  {
    "objectID": "01_rl-intro.html",
    "href": "01_rl-intro.html",
    "title": "1  An introduction to RL",
    "section": "",
    "text": "1.1 Learning outcomes\nThis module gives a short introduction to Reinforcement learning.\nBy the end of this module, you are expected to:\nThe learning outcomes relate to the overall learning goals number 3, 5, 6, 9 and 11 of the course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#mod-rl-intro-lo",
    "href": "01_rl-intro.html#mod-rl-intro-lo",
    "title": "1  An introduction to RL",
    "section": "",
    "text": "Describe what RL is.\nBe able to identify different sequential decision problems.\nKnow what Business Analytics are and identify RL in that framework.\nMemorise different names for RL and how it fits in a Machine Learning framework.\nFormulate the blocks of a RL model (environment, agent, data, states, actions, rewards and policies).\nRun your first RL algorithm and evaluate on its solution.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#textbook-readings",
    "href": "01_rl-intro.html#textbook-readings",
    "title": "1  An introduction to RL",
    "section": "1.2 Textbook readings",
    "text": "1.2 Textbook readings\nFor this week, you will need to read Chapter 1-1.5 in Sutton and Barto (2018). Read it before continuing this module.\n\n\n\nSlides for this module can be seen\nhere.\nYou do not have to look at them before the lecture!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#what-is-reinforcement-learning",
    "href": "01_rl-intro.html#what-is-reinforcement-learning",
    "title": "1  An introduction to RL",
    "section": "1.3 What is reinforcement learning",
    "text": "1.3 What is reinforcement learning\nRL can be seen as\n\nAn approach of modelling sequential decision making problems.\nAn approach for learning good decision making under uncertainty from experience.\nMathematical models for learning-based decision making.\nTrying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.\nEstimating and finding near optimal decisions of a stochastic process with sequential decision making.\nA model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.\n\nSequential decision problems are problems where you take decisions/actions over time. As an agent, you base your decision on the current state of the system (a state is a function of the information/data available). At the next time-step, the system have moved (stochastically) to the next stage. Here new information may be available and you receive a reward and take a new action. Examples of sequential decision problems are (with possible actions):\n\nPlaying backgammon (how to move the checkers).\nDriving a car (left, right, forward, back, break, stop, …).\nHow to invest/maintain a portfolio of stocks (buy, sell, amount).\n\nControl an inventory (wait, buy, amount).\nVehicle routing (routes).\nMaintain a spare-part (wait, maintain).\nRobot operations (sort, move, …)\nDairy cow treatment/replacement (treat, replace, …)\nRecommender systems e.g. Netflix recommendations (videos)\n\nSince RL involves a scalar reward signal, the goal is to choose actions such that the total reward is maximized. Note actions have an impact on the future and may have long term consequences. As such, you cannot simply choose the action that maximize the current reward. It may, in fact, be better to sacrifice immediate reward to gain more long term reward.\nRL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance:\n\ntotally random trials (in the start),\nsophisticated tactics and superhuman skills (in the end).\n\nThat is, as the agent learn, the reward estimate of a given action becomes better.\nAs humans, we often learn by trial and error too:\n\nLearning to walk (by falling/pain).\nLearning to play (strategy is based on the game rules and what we have experienced works based on previous plays).\n\nThis can also be seen as learning the reward of our actions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#rl-and-business-analytics",
    "href": "01_rl-intro.html#rl-and-business-analytics",
    "title": "1  An introduction to RL",
    "section": "1.4 RL and Business Analytics",
    "text": "1.4 RL and Business Analytics\nBusiness Analytics (BA) (or just Analytics) refers to the scientific process of transforming data into insight for making better decisions in business. BA can both be seen as the complete decision making process for solving a business problem or as a set of methodologies that enable the creation of business value. As a process it can be characterized by descriptive, predictive, and prescriptive model building using “big” data sources.\nDescriptive Analytics: A set of technologies and processes that use data to understand and analyze business performance. Descriptive analytics are the most commonly used and most well understood type of analytics. Descriptive analytics categorizes, characterizes, consolidates, and classifies data. Examples are standard reporting and dashboards (KPIs, what happened or is happening now?) and ad-hoc reporting (how many/often?). Descriptive analytics often serves as a first step in the successful application of predictive or prescriptive analytics.\nPredictive Analytics: The use of data and statistical techniques to make predictions about future outputs/outcomes, identify patterns or opportunities for business performance. Examples of techniques are data mining (what data is correlated with other data?), pattern recognition and alerts (when should I take action to correct/adjust a spare part?), Monte-Carlo simulation (what could happen?), neural networks (which customer group are best?) and forecasting (what if these trends continue?).\nPrescriptive Analytics: The use of optimization and other decision modelling techniques using the results of descriptive and predictive analytics to suggest decision options with the goal of improving business performance. Prescriptive analytics attempt to quantify the effect of future decisions in order to advise on possible outcomes before the decisions are actually made. Prescriptive analytics predicts not only what will happen, but also why it will happen and provides recommendations regarding actions that will take advantage of the predictions. Prescriptive analytics are relatively complex to administer, and most companies are not yet using it in their daily course of business. However, when implemented correctly, it can have a huge impact on business performance and how businesses make decisions. Examples on prescriptive analytics are optimization in production planning and scheduling, inventory management, the supply chain and transportation planning. Since RL focus optimizing decisions it is Prescriptive Analytics also known as sequential decision analytics.\n\n\n\n\n\nBusiness Analytics and competive advantage.\n\n\n\n\nCompanies who use BA focus on fact-based management to drive decision making and treats data and information as a strategic asset that is shared within the company. This enterprise approach generates a companywide respect for applying descriptive, predictive and prescriptive analytics in areas such as supply chain, marketing and human resources. Focusing on BA gives a company a competive advantage (see Figure @ref(fig:analytics)).\nBA and related areas: In the past Business Intelligence traditionally focuses on querying, reporting, online analytical processing, i.e. descriptive analytics. However, a more modern definition of Business Intelligence is the union of descriptive and predictive analytics. Operations Research or Management Science deals with the application of advanced analytical methods to help make better decisions and can hence be seen as prescriptive analytics. However, traditionally it has been taking a more theoretical approach and focusing on problem-driven research while BA takes a more data-driven approach. Logistics is a cross-functional area focusing on the effective and efficient flows of goods and services, and the related flows of information and cash. Supply Chain Management adds a process-oriented and cross-company perspective. Both can be seen as prescriptive analytics with a more problem-driven research focus. Advanced Analytics is often used as a classification of both predictive and prescriptive analytics. Data science is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured and can be seen as Business analytics applied to a wider range of data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#rl-in-different-research-deciplines",
    "href": "01_rl-intro.html#rl-in-different-research-deciplines",
    "title": "1  An introduction to RL",
    "section": "1.5 RL in different research deciplines",
    "text": "1.5 RL in different research deciplines\nRL is used in many research fields using different names:\n\nRL (most used) originated from computer science and AI.\nApproximate dynamic programming (ADP) is mostly used within operations research.\nNeuro-dynamic programming (when states are represented using a neural network).\nRL is closely related to Markov decision processes (a mathematical model for a sequential decision problem).\n\n\n\n\n\n\nAdopted from Silver (2015).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#rl-and-machine-learning",
    "href": "01_rl-intro.html#rl-and-machine-learning",
    "title": "1  An introduction to RL",
    "section": "1.6 RL and machine learning",
    "text": "1.6 RL and machine learning\nDifferent ways of learning:\n\nSupervised learning: Given data \\((x_i, y_i)\\) learn to predict \\(y\\) from \\(x\\), i.e. find \\(y \\approx f(x)\\) (e.g. regression).\nUnsupervised learning: Given data \\((x_i)\\) learn patterns using \\(x\\), i.e. find \\(f(x)\\) (e.g. clustering). \nRL: Given state \\(x\\) you take an action and observe the reward \\(r\\) and the new state \\(x'\\).\n\nThere is no supervisor \\(y\\), only a reward signal \\(r\\).\nYour goal is to find a policy that optimize the total reward function.\n\n\n\n\n\n\n\nAdopted from Silver (2015).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#the-rl-data-stream",
    "href": "01_rl-intro.html#the-rl-data-stream",
    "title": "1  An introduction to RL",
    "section": "1.7 The RL data-stream",
    "text": "1.7 The RL data-stream\nRL considers an agent in an environment:\n\nAgent: The one who takes the action (computer, robot, decision maker).\nEnvironment: The system/world where observations and rewards are found.\n\nData are revealed sequentially as you take actions \\[(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \\ldots).\\] At time \\(t\\) the agent have been taken action \\(A_{t-1}\\) and observed observation \\(O_t\\) and reward \\(R_t\\):\n\n\n\n\n\nAgent-environment representation.\n\n\n\n\nThis gives us the history at time \\(t\\) is the sequence of observations, actions and rewards \\[H_t = (O_0, A_0, R_1, O_1, \\ldots, A_{t-1}, R_t, O_t).\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#states-actions-rewards-and-policies",
    "href": "01_rl-intro.html#states-actions-rewards-and-policies",
    "title": "1  An introduction to RL",
    "section": "1.8 States, actions, rewards and policies",
    "text": "1.8 States, actions, rewards and policies\nThe (agent) state \\(S_t\\) is the information used to take the next action \\(A_t\\):\n\n\n\n\n\nState and action.\n\n\n\n\nA state depends on the history, i.e. a state is a function of the history \\(S_t = f(H_t)\\). Different strategies for defining a state may be considered. Choosing \\(S_t = H_t\\) is bad since the size of a state representation grows very fast. A better strategy is to just store the information needed for taking the next action. Moreover, it is good to have Markov states where given the present state the future is independent of the past. That is, the current state holds just as much information as the history, i.e. it holds all useful information of the history. Symbolically, we call a state \\(S_t\\) Markov iff\n\\[\\Pr[S_{t+1} | S_t] = \\Pr[S_{t+1} | S_1,...,S_t].\\]\nThat is, the probability of seeing some next state \\(S_{t+1}\\) given the current state is exactly equal to the probability of that next state given the entire history of states. Note that we can always find some Markov state. Though the smaller the state, the more “valuable” it is. In the worst case, \\(H_t\\) is Markov, since it represents all known information about itself.\nThe reward \\(R_t\\) is a number representing the reward at time \\(t\\) (negative if a cost). Examples of rewards are\n\nPlaying backgammon (0 (when play), 1 (when win), -1 (when loose)).\nHow to invest/maintain a portfolio of stocks (the profit).\n\nControl an inventory (inventory cost, lost sales cost).\nVehicle routing (transportation cost).\n\nThe goal is to find a policy that maximize the total future reward. A policy is the agent’s behaviour and is a map from state to action, i.e. a function \\[a = \\pi(s)\\] saying that given the agent is in state \\(s\\) we choose action \\(a\\).\nThe total future reward is currently not defined clearly. Let the value function denote the future reward in state \\(s\\) and define it as the expected discounted future reward: \\[V_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots | S = s).\\] Note the value function is defined using a specific policy and the goal is to find a policy that maximize the total future reward in all possible states \\[\\pi^* = \\arg\\max_{\\pi\\in\\Pi}(V_\\pi(s)).\\]\nThe value of the discount rate is important:\n\nDiscount rate \\(\\gamma=0\\): Only care about present reward.\nDiscount rate \\(\\gamma=1\\): Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite.\nDiscount rate \\(\\gamma&lt;1\\): Rewards near to the present more beneficial. Note \\(V(s)\\) will converge to a number even if the time-horizon is infinite.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#exploitation-vs-exploration",
    "href": "01_rl-intro.html#exploitation-vs-exploration",
    "title": "1  An introduction to RL",
    "section": "1.9 Exploitation vs Exploration",
    "text": "1.9 Exploitation vs Exploration\nA key problem of reinforcement learning (in general) is the difference between exploration and exploitation. Should the agent sacrifice what is currently known as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? Exploitation takes the action assumed to be optimal with respect to the data observed so far. This, gives better predictions of the value function (given the current policy) but prevents the agent from discovering potential better decisions (a better policy). Exploration does not take the action that seems to be optimal. That is, the agent explore to find new states and update the value function for this state.\nExamples in the exploration and exploitation dilemma are for instance movie recommendations: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration) or oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#rl-in-action-tic-tac-toe",
    "href": "01_rl-intro.html#rl-in-action-tic-tac-toe",
    "title": "1  An introduction to RL",
    "section": "1.10 RL in action (Tic-Tac-Toe)",
    "text": "1.10 RL in action (Tic-Tac-Toe)\nThe current state of the board is represented by a row-wise concatenation of the players’ marks in a 3x3 grid. For example, the 9 character long string \"......X.O\" denotes a board state in which player X has placed a mark in the third row and first column whereas player O has placed a mark in the third row and the third column:\n\n\n\n\n\n\n\n\n.\n.\n.\n\n\n.\n.\n.\n\n\nX\n.\nO\n\n\n\n\n\nThat is, we index the fields row-wise:\n\n\n\n\n\n1\n2\n3\n\n\n4\n5\n6\n\n\n7\n8\n9\n\n\n\n\n\nThe game is continued until all fields are filled or the game is over (win or loose).\n\nThe player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. Reward for a player is 1 for ‘win’, 0.5 for ‘draw’, and 0 for ‘loss’. These values can be seen as the probabilities of winning.\nExamples of winning, loosing and a draw from player Xs point of view:\n\n\n\n\n\n\n\n\n.\n\n\n.\n\n\nX\n\n\n\n\n.\n\n\nX\n\n\n.\n\n\n\n\nX\n\n\nO\n\n\nO\n\n\n\n\n\n\n\n\n\n\nX\n\n\n.\n\n\nX\n\n\n\n\n.\n\n\nX\n\n\n.\n\n\n\n\nO\n\n\nO\n\n\nO\n\n\n\n\n\n\n\n\n\n\nX\n\n\nX\n\n\nO\n\n\n\n\nO\n\n\nO\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\nO\n\n\n\n\n\n\n\n\nNote a state can also be represented using a state vector of length 9:\n\nstateStr &lt;- function(sV) {\n   str &lt;- str_c(sV, collapse = \"\")\n   return(str)\n}\nstateVec &lt;- function(s) {\n   sV &lt;- str_split(s, \"\")[[1]]\n   return(sV)\n}\nsV &lt;- stateVec(\"X.X.X.OOO\")\nsV\n#&gt; [1] \"X\" \".\" \"X\" \".\" \"X\" \".\" \"O\" \"O\" \"O\"\n\nGiven a state vector, we can check if we win or loose:\n\n#' Check board state\n#'\n#' @param pfx Player prefix (the char used on the board).\n#' @param sV Board state vector.\n#' @return A number 1 (win), 0 (loose) or 0.5 (draw/unknown)\nwin &lt;- function(pfx, sV) {\n   idx &lt;- which(sV == pfx)\n   mineV &lt;- rep(0, 9)\n   mineV[idx] &lt;- 1\n   mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE)\n   if (any(rowSums(mineM) == 3) ||  # win\n      any(colSums(mineM) == 3) ||\n      sum(diag(mineM)) == 3 ||\n      sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 3) return(1)\n   idx &lt;- which(sV == \".\")\n   mineV[idx] &lt;- 1\n   mineM &lt;- matrix(mineV, 3, 3, byrow = TRUE)\n   if (any(rowSums(mineM) == 0) ||  # loose\n      any(colSums(mineM) == 0) ||\n      sum(diag(mineM)) == 0 ||\n      sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 0) return(0)\n   return(0.5)  # draw\n}\nwin(\"O\", sV)\n#&gt; [1] 1\nwin(\"X\", sV)\n#&gt; [1] 0\n\nWe start with an empty board and have at most 9 moves (a player may win before). If the opponent start and a state denote the board before the opponent makes a move, then a draw game may look as in Figure @ref(fig:hgf). We start with an empty board state \\(S_0\\), and the opponent makes a move, next we choose a move \\(A_0\\) (among the empty fields) and we end up in state \\(S_1\\). This continues until the game is over.\n\n\n\n\n\nA draw.\n\n\n\n\n\n1.10.1 Players and learning to play\nAssume that we initially define a value \\(V(S)\\) of each state \\(S\\) to be 1 if we win, 0 if we loose and 0.5 otherwise. Most of the time we exploit our knowledge, i.e. choose the action which gives us the highest estimated reward (probability of winning). However, some times (with probability \\(\\epsilon\\)) we explore and choose another action/move than what seems optimal. These moves make us experience states we may otherwise never see. If we exploit we update the value of a state using \\[V(S_t) = V(S_t) + \\alpha(V(S_{t+1})-V(S_t))\\] where \\(\\alpha\\) is the step-size parameter which influences the rate of learning.\nLet us implement a RL player using a R6 class and store the values using a hash list. We keep the hash list minimal by dynamically adding only states which has been explored or needed for calculations. Note using R6 is an object oriented approach and objects are modified by reference. The internal method move takes the previous state (from our point of view) and the current state (before we make a move) and returns the next state (after our move) and update the value function (if exploit). The player explore with probability epsilon if there is not a next state that makes us win.\n\nPlayerRL &lt;- R6Class(\"PlayerRL\",\n   public = list(\n      pfx = \"\",  # player prefix\n      hV = NA,   # empty hash list (states are stored using a string key)\n      control = list(epsilon = 0.2, alpha = 0.3), \n      clearLearning = function() clear(self$hV),\n      initialize = function(pfx = \"\", control = list(epsilon = 0.2, alpha = 0.3)) {\n         self$pfx &lt;- pfx\n         self$control &lt;- control\n         self$hV &lt;- hash()\n      },\n      finalize = function() {\n         # cat(\"FIN\\n\")\n         clear(self$hV)\n      },\n      move = function(sP, sV) { # previous state (before opponent move) and current state (before we move)\n         idx &lt;- which(sV == \".\")  # possible places to place our move\n         state &lt;- stateStr(sP)    # state as a string\n         if (!has.key(state, self$hV)) self$hV[[state]] &lt;- 0.5 # if the state hasn't a value then set it to 0.5 (default)\n         # find possible moves and add missing states\n         keys &lt;- c()\n         keysV &lt;- NULL\n         for (i in idx) {  # find possible moves\n            sV[i] &lt;- self$pfx\n            str &lt;- str_c(sV, collapse = \"\")\n            keys &lt;- c(keys, str)\n            keysV &lt;- rbind(keysV, sV)\n            sV[i] &lt;- \".\"   # set the value back to default\n         }\n         # add missing states of next sP\n         idx &lt;- which(!has.key(keys, self$hV))\n         if (length(idx) &gt; 0) {\n            for (i in 1:nrow(keysV)) {\n               self$hV[keys[i]] &lt;- win(self$pfx, keysV[i,])\n            }\n         }\n         # cat(\"Player\", pfx, \"\\n\")\n         # print(self$hV)\n         # update and find next state\n         val &lt;- values(self$hV[keys])\n         # cat(\"Moves:\"); print(val)\n         m &lt;- max(val)\n         if (rbinom(1,1, self$control$epsilon) &gt; 0 & any(val &lt; m) & m &lt; 1) { # explore\n            idx &lt;- which(val &lt; m)\n            idx &lt;- idx[sample(length(idx), 1)]\n            nextS &lt;- names(val)[idx] \n            # cat(\"Explore - \")\n         } else { # exploit\n            idx &lt;- which(val == m)\n            idx &lt;- idx[sample(length(idx), 1)]\n            nextS &lt;- names(val)[idx] # pick one\n            self$hV[[state]] &lt;- self$hV[[state]] + self$control$alpha * (m - self$hV[[state]])\n            # cat(\"Exploit - \")\n         }\n         # cat(\"Next:\", nextS, \"\\n\")\n         return(str_split(nextS, \"\")[[1]])\n      }\n   )\n)\n\nWe then can define a player using:\n\nplayerA &lt;- PlayerRL$new(pfx = \"A\", control = list(epsilon = 0.5, alpha = 0.1))   \n\nOther players may be defined similarly, e.g. a player which moves randomly (if can not win in the next move):\n\nPlayerRandom &lt;- R6Class(\"PlayerRandom\",\n   public = list(\n      pfx = NA,\n      initialize = function(pfx) {\n         self$pfx &lt;- pfx\n      },\n      move = function(sP, sV) {  # previous state (before opponent move) and current state (before we move)\n         idx &lt;- which(sV == \".\")\n         state &lt;- stateStr(sV)\n         keys &lt;- c()\n         keysV &lt;- NULL\n         for (i in idx) {  # find possible moves\n            sV[i] &lt;- self$pfx\n            str &lt;- str_c(sV, collapse = \"\")\n            keys &lt;- c(keys, str)\n            keysV &lt;- rbind(keysV, sV)\n            sV[i] &lt;- \".\"\n         }\n         # check if can win in one move\n         for (i in 1:nrow(keysV)) {\n            if (win(self$pfx, keysV[i,]) == 1) {\n               return(keysV[i,])  # next state is the win state\n            }\n         }\n         # else pick one random\n         return(keysV[sample(nrow(keysV), 1),])\n      }\n   )\n)\n\nA player which always place at the lowest field index:\n\nPlayerFirst &lt;- R6Class(\"PlayerFirst\",\n   public = list(\n      pfx = NA,\n      initialize = function(pfx) {\n         self$pfx &lt;- pfx\n      },\n      move = function(sP, sV) { # previous state (before opponent move) and current state (before we move)\n         idx &lt;- which(sV == \".\")\n         sV[idx[1]] &lt;- self$pfx\n         return(sV)\n      }\n   )\n)\n\n\n\n1.10.2 Gameplay\nWe define a game which returns the prefix of the winner (NA if a draw):\n\n#' @param player1 A player R6 object. This player starts the game\n#' @param player2 A player R6 object.\n#' @param verbose Print gameplay.\n#' @return The winners prefix or NA if a tie.\nplayGame &lt;- function(player1, player2, verbose = FALSE) {\n   sP2 &lt;- rep(\".\", 9)  # start state / game state\n   sP1 &lt;- sP2          # state from player 1s viewpoint\n   for (i in 1:5) { # at most 4.5 rounds\n      ## player 1\n      if (verbose) cat(\"Player \", player1$pfx, \":\\n\", sep=\"\")\n      sP1 &lt;- player1$move(sP1, sP2)  # new state from player 1s viewpoint\n      # states &lt;- c(states, stateChr(sV))\n      # cat(stateStr(sV), \" | \", sep = \"\")\n      if (verbose) plot_board_state_cat(stateStr(sP1))\n      if (win(player1$pfx, sP1) == 1) {\n         return(player1$pfx)\n         break\n      }\n      if (i == 5) break  # a draw\n      ## player 2\n      if (verbose) cat(\"Player \", player2$pfx, \":\\n\", sep=\"\")\n      sP2 &lt;- player2$move(sP2, sP1)\n      # states &lt;- c(states, stateChr(sV))\n      # cat(stateStr(sV), \" | \", sep = \"\")\n      if (verbose) plot_board_state_cat(stateStr(sP2))\n      if (win(player2$pfx, sP2) == 1) {\n         return(player2$pfx)\n         break\n      }\n   }\n   return(NA)\n}\n\nLet us play a game between playerA and playerR:\n\nplayerR &lt;- PlayerRandom$new(pfx = \"R\")\nplayGame(playerA, playerR, verbose = T)\n#&gt; Player A:\n#&gt; |------------------|\n#&gt; |  .  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  A  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  .  |  .   |  .  |\n#&gt; |------------------|\n#&gt; Player R:\n#&gt; |------------------|\n#&gt; |  R  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  A  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  .  |  .   |  .  |\n#&gt; |------------------|\n#&gt; Player A:\n#&gt; |------------------|\n#&gt; |  R  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  A  |  A   |  .  |\n#&gt; |------------------|\n#&gt; |  .  |  .   |  .  |\n#&gt; |------------------|\n#&gt; Player R:\n#&gt; |------------------|\n#&gt; |  R  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  A  |  A   |  .  |\n#&gt; |------------------|\n#&gt; |  .  |  R   |  .  |\n#&gt; |------------------|\n#&gt; Player A:\n#&gt; |------------------|\n#&gt; |  R  |  .   |  .  |\n#&gt; |------------------|\n#&gt; |  A  |  A   |  A  |\n#&gt; |------------------|\n#&gt; |  .  |  R   |  .  |\n#&gt; |------------------|\n#&gt; [1] \"A\"\n\nNote playerA has been learning when playing the game. The current estimates that are stored in the hash list are:\n\nplayerA$hV\n#&gt; &lt;hash&gt; containing 22 key-value pair(s).\n#&gt;   ......... : 0.5\n#&gt;   ........A : 0.5\n#&gt;   .......A. : 0.5\n#&gt;   ......A.. : 0.5\n#&gt;   .....A... : 0.5\n#&gt;   ....A.... : 0.5\n#&gt;   ...A..... : 0.5\n#&gt;   ..A...... : 0.5\n#&gt;   .A....... : 0.5\n#&gt;   A........ : 0.5\n#&gt;   R..A....A : 0.5\n#&gt;   R..A...A. : 0.5\n#&gt;   R..A..A.. : 0.5\n#&gt;   R..A.A... : 0.5\n#&gt;   R..AA.... : 0.55\n#&gt;   R..AA..RA : 0.5\n#&gt;   R..AA.AR. : 0.5\n#&gt;   R..AAA.R. : 1\n#&gt;   R.AA..... : 0.5\n#&gt;   R.AAA..R. : 0.5\n#&gt;   RA.A..... : 0.5\n#&gt;   RA.AA..R. : 0.5\n\n\n\n1.10.3 Learning by a sequence of games\nWith a single game only a few states are explored and estimates are not good. Let us instead play a sequence of games and learn along the way:\n\n#' @param playerA Player A (R6 object).\n#' @param playerB Player B (R6 object).\n#' @param games Number of games\n#' @param prA Probability of `playerA` starts.\n#' @return A list with results (a data frame and a plot).  \nplayGames &lt;- function(playerA, playerB, games, prA = 0.5) {\n   winSeq &lt;- rep(NA, games)\n   for (g in 1:games) {\n      # find start player\n      if (sample(0:1, 1, prob = c(prA, 1-prA)) == 0) {\n         player1 &lt;- playerA\n         player2 &lt;- playerB\n      } else {\n         player2 &lt;- playerA\n         player1 &lt;- playerB\n      }\n      winSeq[g] &lt;- playGame(player1, player2)\n   }\n   # process the data\n   dat &lt;- tibble(game = 1:length(winSeq), winner = winSeq) %&gt;% \n      mutate(\n         players = str_c(playerA$pfx, playerB$pfx),\n         winA := case_when(\n            winner == playerA$pfx ~ 1,\n            winner == playerB$pfx ~ 0,\n            TRUE ~ 0.5\n         ),\n         winsA_r = rollapply(winA, ceiling(games/10), mean, align = \"right\", fill = NA)  #, fill = 0, partial = T\n      )\n   # make a plot\n   pt &lt;- dat %&gt;% \n      ggplot(aes(x = game, y = winA)) +\n      geom_line(aes(y = winsA_r), size = 0.2) +\n      geom_smooth(se = F) +\n      labs(y = str_c(\"Avg. wins player \", playerA$pfx),\n           title = str_c(\"Wins \", playerA$pfx, \" = \", round(mean(dat$winA), 2), \" \", playerB$pfx, \" = \", round(1-mean(dat$winA), 2)))\n   return(list(dat = dat, plot = pt))\n}\n\nLet us now play games against a player who moves randomly using \\(\\epsilon = 0.1\\) (explore probability) and \\(\\alpha = 0.1\\) (step size).\n\nplayerA &lt;- PlayerRL$new(pfx = \"A\", control = list(epsilon = 0.1, alpha = 0.1)) \nplayerR &lt;- PlayerRandom$new(pfx = \"R\")\nres &lt;- playGames(playerA, playerR, games = 2000)\nres$plot\n\n\n\n\n\n\n\n\nThe black curve is the moving average of winning with a trend line. Note the values of the parameters have an effect on our learning:\n\n\n\n\n\n\n\n\n\nIn general we do not need to explore (\\(\\epsilon = 0\\)) (the other player explore enough for us) and a high explore probability (\\(\\epsilon = 0.9\\)) make us loose. Moreover, using a high step size seems to work best.\nOther players may give different results. If the RL player play against a player which always move to first free field index:\n\n\n\n\n\n\n\n\n\nHere a high step size and a low exploration probability is good and the RL player will soon figure out how to win all the time.\nThis is different if the RL player A play against another clever (RL) player B.\n\nplayerA &lt;- PlayerRL$new(pfx = \"A\", control = list(epsilon = 0, alpha = 0.1))\nplayerB &lt;- PlayerRL$new(pfx = \"B\", control = list(epsilon = 0, alpha = 0.1))\n\nIf both players play using the same control parameters, one would expect that they after learning should win/loose with probability 0.5. However if there is no exploration (\\(\\epsilon = 0\\)) this is not always true:\n\n\n\n\n\n\n\n\n\nDepending on how the game starts a player may learn a better strategy and win/loose more. That is, exploration is important. Finally let us play against a player B with fixed control parameters.\n\n\n\n\n\n\n\n\n\nIn general it is best to explore using the same probability otherwise you loose more and a higher step size than your opponent will make you win.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#summary",
    "href": "01_rl-intro.html#summary",
    "title": "1  An introduction to RL",
    "section": "1.11 Summary",
    "text": "1.11 Summary\nRead Chapter 1.6 in Sutton and Barto (2018).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "01_rl-intro.html#sec-rl-intro-ex",
    "href": "01_rl-intro.html#sec-rl-intro-ex",
    "title": "1  An introduction to RL",
    "section": "1.12 Exercises",
    "text": "1.12 Exercises\nBelow you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the help page. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!\n\n1.12.1 Exercise - Self-Play\nConsider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the exploration parameter is non-zero, the algorithm will continue to adapt until it reaches an equilibrium (either fixed or cyclical). You may try to code it.\n\n\n\n\n\n1.12.2 Exercise - Symmetries\nMany tic-tac-toe positions appear different but are really the same because of symmetries.\n\nHow might we amend the reinforcement learning algorithm described above to take advantage of this?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt is possible to use 4 axis of symmetry to essentially fold the board down to a quarter of the size.\n\n\n\nIn what ways would this improve the algorithm?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA smaller state space would increase the speed of learning and reduce the memory required.\n\n\n\nSuppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the opponent did not use symmetries then it could result in a worse learning. For example, if the opponent always played correct except for 1 corner, then using symmetries would mean you never take advantage of that information. That is, we should not use symmetries too since symmetrically equivalent positions do not always hold the same value in such a game.\n\n\n\n\n\n\n1.12.3 Exercise - Greedy Play\nConsider Tic-Tac-Toe and suppose the RL player is only greedy (\\(\\epsilon = 0\\)), that is, always playing the move that that gives the highest probability of winning. Would it learn to play better, or worse, than a non-greedy player? What problems might occur?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs seen in Section @ref(rl-intro-tic-learn) using \\(\\epsilon = 0\\) may be okay for this game if the opponent use a simple strategy (e.g. random or first index). However, in general the RL player would play worse. The chance the optimal action is the one with the current best estimate of winning is low and depending on the gameplay the RL player might win or loose. The RL player would also be unable to adapt to an opponent that slowly alter behavior over time. required.\n\n\n\n\n\n1.12.4 Exercise - Learning from Exploration\nConsider Tic-Tac-Toe and suppose the RL player is playing against an opponent with a fixed strategy. Suppose learning updates occur after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities.\n\nWhat are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability set \\(V(s)\\) found by applying no learning from exploration is the probability of winning when using the optimal policy. The probability set \\(V(s)\\) found by applying learning from exploration is the probability of winning including the active exploration policy.\n\n\n\nAssuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability set found by applying no learning from exploration would result in more wins. The probability set found by applying learning from exploration is better to learn, as it reduces variance from sub-optimal future states.\n\n\n\n\n\n\n1.12.5 Exercise - Other Improvements\nConsider Tic-Tac-Toe. Can you think of other ways to improve the reinforcement learning player?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAltering the exploration rate/learning based on the variance in the opponent’s actions. If the opponent is always making the same moves and you are winning from it then using a non-zero exploration rate will make you lose you games. If the agent is able to learn how the opponent may react to certain moves, it will be easier for it to win as it can influence the opponent to make moves that leads it to a better state.\n\n\n\n\n\n\n\nSilver, David. 2015. “Lectures on Reinforcement Learning.” https://www.davidsilver.uk/teaching/.\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An Introduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An introduction to RL</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Silver, David. 2015. “Lectures on Reinforcement Learning.”\nhttps://www.davidsilver.uk/teaching/.\n\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning: An\nIntroduction. Second Edition. MIT Press. http://incompleteideas.net/book/the-book.html.",
    "crumbs": [
      "References"
    ]
  }
]