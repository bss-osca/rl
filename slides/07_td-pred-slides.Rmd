---
title: "Temporal Difference (TD) methods for prediction"
author: "Lars Relund Nielsen"
output:
  xaringan::moon_reader:
    css: "./libs/slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
      slideNumberFormat: ""
editor_options: 
  chunk_output_type: console
---

```{r, child = "setup.Rmd", echo=FALSE}
```

```{r include=FALSE, eval=FALSE}
# to preview
xaringan::inf_mr(cast_from = ".", quiet = T)
```

layout: true
  
```{r, echo=FALSE}
module_name <- "td-pred"
module_number <- "07"
here::i_am(str_c("slides/", module_number, "_", module_name, "-slides.Rmd"))
library(htmltools)
footerHtml <- withTags({
   div(class="my-footer",
      span(
         a(href=str_c("https://bss-osca.github.io/rl/mod-", module_name, ".html"), target="_blank", "Notes"), 
         " | ",
         a(href=str_c("https://bss-osca.github.io/rl/slides/", module_number, "_", module_name, "-slides.html"), target="_blank", "Slides"),    
         " | ",
         a(href=str_c("https://github.com/bss-osca/rl/blob/master/slides/", module_number, "_", module_name, "-slides.Rmd"), target="_blank", "Source"),  
      )
   )
})
footerHtml
knitr::opts_chunk$set(fig.path=str_c("img/", module_name, "-"))
```



<!-- Templates -->
<!-- .pull-left[] .pull-right[] -->
<!-- knitr::include_graphics("img/bandit.png") -->
<!-- .left-column-wide[]  .right-column-small[] -->

---

## Learning outcomes

* Describe what Temporal Difference (TD) learning is.
* Formulate the incremental update formula for TD learning.
* Define the temporal-difference error.
* Interpret the role of a fixed step-size.
* Identify key advantages of TD methods over DP and MC methods.
* Explain the TD(0) prediction algorithm.
* Understand the benefits of learning online with TD compared to MC methods.

---

## What is TD learning?

* TD learning is a combination of Monte Carlo (MC) and dynamic programming (DP) ideas
* Like MC, TD can predict using a model-free environment and learn from experience. 
* Like DP, TD update estimates based on other learned estimates, without waiting for a final outcome (bootstrapping). 
* TD can learn on-line and do not need to wait until the whole sample-path is found. 
* Given a policy $\pi$, we want to estimate the state-value function:
$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s].$$
where the return is 
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}$$
<!-- TD in general learn more efficiently than MC due to bootstrapping. In this module prediction using TD is considered.  -->

---

## Estimating the state-value

How do we find estimates $V$ of $v_\pi$ and the return $G_t$?

   * For MC we generated sample-paths and calculated returns $G_t$ and updated the estimate by taking the average over all the realized returns.
   * For TD we use 

\begin{align}
v_\pi(s) &= \mathbb{E}_\pi[G_t | S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1}| S_t = s] + \gamma \mathbb{E}_\pi[G_{t+1} | S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1}| S_t = s] + \gamma v_\pi(S_{t+1}),
\end{align}

   * TD: Given a realized reward $R_{t+1}$, an estimate for the return $G_t$ is $R_{t+1} + \gamma V(S_{t+1})$ 
   * TD update the estimate based on the estimate of the next state (bootstrapping). 

---


## Incremental update

.pull-left[
**Monte Carlo**

* Generate sample-path and calculate $G_t$ along the path.
* Unbiased estimate.
* Update the estimate using
$$
  V(S_t) \leftarrow V(S_t) + \alpha_n\left[G_t - V(S_t)\right].
$$
* Can update after the full sample-path known.
] 

--

.pull-right[
**Temporal Difference**

* Replace $G_t$ with the TD estimate $R_{t+1} + \gamma V(S_{t+1}).$
* Update now becomes 
.midi[
$$V(S_t) \leftarrow V(S_t) + \alpha\left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right].$$
] 
* Can update when the next state $S_{t+1}$ is observed.
* As the estimate of $S_{t+1}$ improve the estimate of $S_t$ also improve. 
* The incremental update is called *TD(0)* or one-step TD because it use a one-step lookahead.
]

---

## A note about fixed step-size

If the environment is non-stationary (e.g. transition probabilities change over time) then a fixed step-size $\alpha\in(0,1]$ may be appropriate. A fixed step-size corresponds to a weighted average of the past observed returns and the initial estimate of $S_t$:
$$
\begin{align}
V_{n+1} &= V_n +\alpha \left[G_n - V_n\right] \nonumber \\
&= \alpha G_n + (1 - \alpha)V_n \nonumber \\
&= \alpha G_n + (1 - \alpha)[\alpha G_{n-1} + (1 - \alpha)V_{n-1}] \nonumber \\
&= \alpha G_n + (1 - \alpha)\alpha G_{n-1} + (1 - \alpha)^2 V_{n-1}  \nonumber \\
& \vdots \nonumber \\
&= (1-\alpha)^n V_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} G_i \\
\end{align}
$$

That is, a larger weight is used for recent observations compared to old observations. Moreover, if $\alpha=1$ then $V_{n+1} = G_t$ and we use the realized return as the estimate.

---

## The TD error

* The term $$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t),$$ is denoted the *temporal difference error* (*TD error*).
* Equals the difference between the current estimate $V(S_t)$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$.

---

## TD(0) prediction algorithm 

```{r, echo=FALSE, out.width="90%"}
knitr::include_graphics("img/td0-pred.png")
```

* Also works for continuing processes (infinite number of time-steps of inner loop).
* No stopping criterion is given but could stop when small for state-values differences. 

---

## TD prediction for action-values

* Since model-free we need to estimate action-values instead so we can improve the policy.
* Goal is to find $q_*$ for which the optimal action is the greedy action.  
* To find $q_*$, we first need to predict action-values $q_\pi$ for a policy $\pi$.
* The incremental update equation must be modified to use $Q$ values: $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\left[R_{t+1} + \gamma Q(S_{t+1}, A_t) - Q(S_t, A_t)\right].$$
* Given policy $\pi$ you need to know $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ or short SARSA. 
* This acronym is used to name the SARSA algorithm for control (next week). 
* To ensure exploration of all action-values we need e.g. an $\epsilon$-soft behavioural policy.  


<!-- # References -->

<!-- ```{r, results='asis', echo=FALSE} -->
<!-- PrintBibliography(bib) -->
<!-- ``` -->


```{r links, child="../book/links.md"}
```

```{r postprocess, include=FALSE}
system2("Rscript", args = "-e 'rmarkdown::render(\"index.Rmd\", quiet = TRUE)'")
file.copy("./slides.css", "./libs/", overwrite = T)
```
