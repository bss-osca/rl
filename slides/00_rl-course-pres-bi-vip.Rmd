---
title: "Reinforcement Learning for Business"
author: "Lars Relund Nielsen"
output:
  xaringan::moon_reader:
    self_contained: true
    css: "./libs/slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r, child = "setup.Rmd", echo=FALSE}
```

```{r include=FALSE, eval=FALSE}
# to preview
xaringan::inf_mr(cast_from = ".")
```

```{r, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      fig.width = 6, 
                      # fig.asp = 0.5,
                      fig.path="img/",
                      # out.width = "100%",
                      fig.align = "center",
                      dpi = 300,
                      # cache = TRUE, autodep = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

```{r, echo=FALSE}
module_name <- "bi-vip"
knitr::opts_chunk$set(fig.path=str_c("img/", module_name, "-"))
```

layout: true

---

## Course overview 

- The purpose of this course is to give an introduction and knowledge about reinforcement learning (RL).
- 5 ECTS.
- Lectures/tutorials each week.
- Exercises are given to support learning.
- R is used as programming tool. 
- 20 minutes oral exam.

For more information see the [course description](https://kursuskatalog.au.dk/en?search=Reinforcement%20Learning%20for%20Business).

---

## What is reinforcement learning

RL can be seen as 

* An approach of modelling sequential decision making problems.
* An approach for learning good decision making under uncertainty from experience.
* Mathematical models for learning-based decision making.
* Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.
* Estimating and finding near optimal decisions of a stochastic process with sequential decision making. 
* A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.

---

## Sequential decision problems

Examples with possible actions are:

* Playing backgammon (how to move the checkers).
* Driving a car (left, right, forward, back, break, stop, ...).
* How to invest/maintain a portfolio of stocks (buy, sell, amount).  
* Control an inventory (wait, buy, amount).
* Maintain a spare-part (wait, maintain).
* Robot operations (sort, move, ...)
* Dairy cow treatment/replacement (treat, replace, ...)

Note current decisions have an impact on the future. 

---

## RL and intuition

RL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance:

* totally random trials (in the start),
* sophisticated tactics and superhuman skills (in the end). 

That is, as the agent learn, the reward estimate of a given action becomes better. 

As humans, we often learn by trial and error too:

* Learning to walk (by falling/pain).
* Learning to play (strategy is based on the game rules and what we have experienced works based on previous plays). 

This can also be seen as learning the reward of our actions. 


---

## RL in a Machine Learning framework

.pull-left[
* **Supervised learning:** Given data $(x_i, y_i)$ learn to predict $y$ from $x$, i.e. find $y \approx f(x)$ (e.g. regression).
* **Unsupervised learning:** Given data $(x_i)$ learn patterns using $x$, i.e. find $f(x)$ (e.g. clustering).
<!-- * Often assume that data are independent and identically distributed (iid).  -->
* **RL:** Given state $x$ you take an action and observe the reward $r$ and the new state $x'$.
 - There is no supervisor $y$, only a reward signal $r$.
 - Your goal is to find a policy that optimize the total reward function.
]

.pull-right[
```{r echo=FALSE, out.width="100%", fig.align = "center", fig.cap = str}
str <- str_c("Adopted from ", Citet(bib, "Silver15"), ".") 
knitr::include_graphics("img/rl-ml.png")
```
]

---

layout: true

## The RL data-stream

```{r, include=FALSE}
## plot an RL (agent/environment relation)
library(ggraph)
library(tidygraph)
library(tidyverse)

plotRL <- function(active = c('F', 'T', 'F'), label = c("A[0]", "O[0]", "R[1]"), lblAgent = "") {
   nodes <- tibble(name = c('Environment', 'Agent', lblAgent))
   # lbl <- str_c(c("A[", "O[", "R["), t, c("]", "]", "]"))
   edges <-tibble(
       from = c(2, 1, 1),
       to =   c(1, 2, 2),
       label = label,
       active = active,
       cap = c(circle(20, 'mm'), circle(20, 'mm'), circle(10, 'mm')))
   gr <- tbl_graph(nodes, edges) 
   p <- ggraph(gr, layout = "manual", x = c(1, 1, 1), y = c(1, 2, 2.1)) +
      geom_edge_fan2(
         aes(label = label, end_cap = cap, col = active), 
         arrow = arrow(length = unit(4, 'mm')),
         hjust = 1.5, 
         label_parse = TRUE,
         strength = -1,
         fontface = "bold",
         show.legend = F, 
         label_colour = NA,
         label_size = 8
      ) +
      scale_edge_color_manual(values = c('T' = "black", 'F' = NA)) +
      geom_node_label(aes(filter = name != lblAgent, label = name), label.padding = unit(1, "lines"), fontface = "bold", size = 10) +
      geom_node_text(aes(filter = name == lblAgent, label = name), parse = TRUE, size = 7) +
      theme_graph(base_size = 30, background = NA, border = T, plot_margin = margin(30,30,10,50)) + 
      coord_cartesian(clip = "off")
   return(p)
}
```

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0)$
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('F', 'T', 'F'), label = c("A[0]", "O[0]", "R[1]"))
```
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0, A_0)$
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('T', 'F', 'F'), label = c("A[0]", "O[0]", "R[1]"))
```
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0, A_0, R_1, O_1)$
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('F', 'T', 'T'), label = c("A[0]", "O[1]", "R[1]"))
```
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0, A_0, R_1, O_1, A_1)$
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('T', 'F', 'F'), label = c("A[1]", "O[1]", "R[1]"))
```
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0, A_0, R_1, O_1, A_1, R_2, O_2)$
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('F', 'T', 'T'), label = c("A[0]", "O[2]", "R[2]"))
```
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * $(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \ldots)$
- History at time $t$: $$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t)$$
- Your goal is to find a policy that maximize the total future reward.
]

.right-column-small[
```{r, echo=FALSE, fig.cap = "Agent-environment representation."}
plotRL(active = c('T', 'T', 'T'), label = c("A[t-1]", "O[t]", "R[t]"))
```
]

---

layout:false

## Reward a closer look

- The reward $R_t$ is a number representing the reward at time $t$ (negative if a cost).
   * Playing backgammon (0 (when play), 1 (when win), -1 (when loose)).
   * How to invest/maintain a portfolio of stocks (the profit).  
   * Control an inventory (inventory cost, lost sales cost).
   * Vehicle routing (transportation cost).
--

- Reward may be delayed, not instantaneous (the consequences of you decision now is revealed later).
--

- RL assumption: all goals can be transformed into the maximisation of expected total future (cumulative) reward.

<!-- - Time really matters (sequential/dynamic system, non iid data). -->
<!-- - Agentâ€™s actions affect the subsequent data it receives. -->

---

layout:true

## History vs state

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards $$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$
]
.right-column-small[
```{r, echo=FALSE}
plotRL(active = c('F', 'T', 'T'), label = c("A[t]", "O[t]", "R[t]"), lblAgent = "")
```
]

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards $$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$
- The state $S_t$ is the information used to take the next action.
]
.right-column-small[
```{r, echo=FALSE}
plotRL(active = c('T', 'F', 'F'), label = c("A[t]", "O[t]", "R[t]"), lblAgent = "S[t]")
```
]

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards $$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$
- The state $S_t$ is the information used to take the next action.
- The next action $A_t$ depends on the history, i.e. a state is a function of the history $S_t = f(H_t)$.
  * Choosing $S_t = H_t$ is bad.
  * Instead just store the information needed for taking the next action. 
  * Markov state: given the present state the future is independent of the past.  
]
.right-column-small[
```{r, echo=FALSE}
plotRL(active = c('T', 'F', 'F'), label = c("A[t]", "O[t]", "R[t]"), lblAgent = "S[t]")
```
]

---

layout:false

## Policy 

- A *policy* is the agentâ€™s behaviour
- It is a map from state to action, i.e. a function 
  $$a = \pi(s)$$ 
  saying that given the agent is in state $s$ we choose action $a$.
- Given state $S_t$ the goal is to find a policy that maximize the total future reward.

---

## Value of a state

- We use the *value function* to predict the future reward in state $S$ e.g. expected discounted future reward:
$$V_\pi(s) = \mathbb{E}_\pi(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S = s).$$ 
- Discount factor $\gamma=0$: Only care about present reward.
- Discount factor $\gamma=1$: Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite.
- Discount factor $\gamma<1$: Rewards near to the present more beneficial. Note $V(s)$ will converge to a number even if the time-horizon is infinite.
- Policy that maximize the total future reward given state $s$: $$\pi^* = \arg\max_{\pi\in\Pi}(V_\pi(s)).$$

<!-- ## Model free vs Model based -->

<!-- --- -->


<!-- ## Exploitation vs Exploration -->

<!-- - Exploitation: Taking the action assumed to be optimal with respect to the data observed so far.  -->
<!--   * Give better predictions of the value function (given the current policy).  -->
<!--   * Prevents the agent from discovering potential better decisions (a better policy). -->
<!-- - Exploration: Not taking the action that seems to be optimal.  -->
<!--   * The agent explore to find states we may not see and hence can update the value function for this state.   -->
<!-- - Examples -->
<!--   * Movies recommendation: recommending the userâ€™s best rated movie type (exploitation) or trying another movie type (exploration). -->
<!--   * Oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration). -->

---

## Let's play Tic-Tac-Toe

We start with an empty board and have at most 9 moves (a player may win before). The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. Reward for a player is 1 for 'win', 0.5 for 'draw', and 0 for 'loss'. These values can be seen as the probability of winning. 

```{r, echo=FALSE}
library(tidyverse)
library(kableExtra)
library(R6)
library(hash)
library(zoo)
library(patchwork)
library(diagram)
```

```{r, message=FALSE, echo=FALSE}
plot_board_state_cat <- function(state) {
   s <- str_split(state, "")[[1]]
   r1 <- str_c("|  ", s[1], "  |  ", s[2], "   |  ", s[3],"  |\n")
   r2 <- str_c("|  ", s[4], "  |  ", s[5], "   |  ", s[6],"  |\n")
   r3 <- str_c("|  ", s[7], "  |  ", s[8], "   |  ", s[9],"  |\n")
   str <- str_c("|------------------|\n", r1, "|------------------|\n", r2, "|------------------|\n", r3, "|------------------|\n")
   return(cat(str))
}

plot_board_state <- function(state) {
   s <- str_split(state, "")[[1]]
   tbl <- matrix(s, nrow = 3, byrow = T)
   tbl <- as_tibble(tbl, .name_repair = "minimal")
   tbl <- tbl %>% 
      kbl(align = "c", col.names = NULL, table.attr = "border: 1px solid black;") %>% 
      kable_styling(full_width = F, bootstrap_options = c("bordered")) 
   return(tbl)
}
```

```{r, message=FALSE, echo=FALSE}
tblW <- plot_board_state("..X.X.XOO")
tblL <- plot_board_state("X.X.X.OOO")
tblD <- plot_board_state("XXOOOXXXO")
```

<table style="width: 100%; border: 0px !important;">
  <tr>
    <td>`r tblW`</td>
    <td>`r tblL`</td>
    <td>`r tblD`</td>
  </tr>
</table>



---

## Gameplay

Let $S_t$ denote the board state before the opponent makes a move.

```{r hgfFunc, include=FALSE}
#' Plot parts of the state expanded hypergraph
#' 
#' A plot is created based on a grid. Each grid point is numbered from bottom to top and next from left to right,
#' i.e. given grid coordinates (row,col) the grid id is (col-1)*rows + (1 + rows - row). 
#' 
#' @param gridDim A 2-dim vector (rows,cols) representing the size of the grid.
#' @param states A data frame containing columns: sId = state id, gId = grid id, label = text and draw = boolean 
#' @param actions A list with mandatory items head = state id, tails = state and voluntary items label, ids, lwd, lty, highlight and col.
#'   if highlight is true then highlight the action (useful if want to show the policy).
#' @param showGrid If true show the grid points (good for debugging).
#' @param radx Node size scaling (same with rady).
#' @param ... Graphical parameters e.g. \code{cex=0.5} to control text size. 
#'   
#' @return NULL
plotHypergraph<-function(gridDim,states=NULL,actions=NULL,showGrid=FALSE, 
                         radx = 0.03, rady=0.07, cex=1, marX=0.035, marY=0.07, ...)
{
   # internal functions
   gMap<-function(sId) return(states$gId[states$sId %in% sId])		# return gId given sId
   sMap<-function(gId) return(states$sId[states$gId %in% gId])		# return sId given gId
   
   pos <- coordinates(rep(gridDim[1], gridDim[2]), hor = F)  # coordinates of each point in the grid
   openplotmat(xlim=c(min(pos[,1])-marX,max(pos[,1])+marX), 
               ylim=c(min(pos[,2])-marY,max(pos[,2])+marY) )  #main = "State expanded hypergraph"
   
   # plot actions
   if (!is.null(actions)) {
      for (i in seq_along(actions)) {
         head <- actions[[i]]$state
         tails <- actions[[i]]$trans
         lwd <- if_else(is.null(actions[[i]]$lwd), 1, actions[[i]]$lwd)
         lty <- if_else(is.null(actions[[i]]$lty), 1, actions[[i]]$lty)
         col <- if_else(is.null(actions[[i]]$col), "black", actions[[i]]$col)
         label <- if_else(is.null(actions[[i]]$label), "", actions[[i]]$label)
         if (str_length(label) != 0) label <- parse(text = label)
         highlight <- if_else(is.null(actions[[i]]$highlight), F, actions[[i]]$highlight)
         if (highlight) lwd <- lwd + 1
         pt <- splitarrow(to = pos[gMap(tails), ], from = pos[gMap(head),], lwd=lwd, lty=lty, arr.type = "none",
                        # arr.side = 1, arr.pos = 0.7, arr.type="curved", arr.lwd = 0.5, arr.length = 0.25, arr.width = 0.2, 
                        lcol=col)
         textempty(pt, lab=label, adj=c(2, 1), cex=cex, ...)
      }
   }	
   
   # plot states
   if (!is.null(states)) {
      for (i in 1:length(states$gId)) { 
         label <- ""
         if (str_length(states$label[i]) != 0) label <- parse(text = states$label[i])
         if (states$draw[i]) textellipse(pos[states$gId[i], ], lab = label, radx = radx, rady=rady, shadow.size = 0, lwd=0.5, cex=cex) 
      }
   }
   
   # Plot rewards
   if (!is.null(actions)) {
      for (i in seq_along(actions)) {
         if (!is.null(actions[[i]]$reward)) {
            label <- parse(text = actions[[i]]$reward$label)
            state <- actions[[i]]$reward$state
            textempty(pos[gMap(state), ], lab=label, adj=c(2.1, 0), cex=cex, ...)
         }
      }
   }	
   
   # visual view of the point numbers (for figuring out how to map stateId to gridId)
   if (showGrid) {
      for (i in 1:dim(pos)[1]) textrect(pos[i, ], lab = i, radx = 0.0, cex=cex)
   }
   return(invisible(NULL))
}
```

```{r hgf, echo=FALSE, out.height="100%", fig.width=12}
par(mar=c(0,0,0,0), omi=c(0,0,0,0))
set.seed(567)
stateN <- 9   # states/stage
stages <- 10   # stages
gridDim <- c(stateN, stages)
states <- tibble(sId = 1:(stages * stateN), gId = 1:(stages * stateN), label = "", draw = rep(T, stages * stateN))
path <- c(6, 15, 23, 31, 38, 49, 59, 66, 75, 86)  # using starte ids
states <- states %>% mutate(draw = if_else(sId %in% setdiff(1:9, path[1]), F, T))
states$label[path[seq_along(path) %% 2 == 1]] <- str_c("S[", seq_along(path[seq_along(path) %% 2 == 1])-1, "]")
actions <- list()
addHArc <- function(actions, harc) {
   actions[[length(actions)+1]] <- harc
   return(actions)
}
for (i in 1:(length(path)-1)) {
   turn <- ((i-1) %% 2) == 1
   if (turn) {
      harc <- list(
         state = path[i],
         trans =  path[i + 1]
         # label = str_c("A[", i/2 - 1, "]")
      )
      actions <- addHArc(actions, harc)
      nxt <-
         sample(setdiff((i * stateN + 1):((i + 1) * stateN), path[i + 1]), 9-i)
      for (s in nxt) {
         harc <- list(state = path[i],
                      trans =  s,
                      lty = 2)
         actions <- addHArc(actions, harc)
      }
   }
   else {
      harc <- list(state = path[i],
                   trans =  path[i + 1],
                   col = "grey")
      actions <- addHArc(actions, harc)
   }
}
plotHypergraph(gridDim, states, actions, showGrid = F, cex = 1, radx = 0.02, rady=0.025, marX=0.02, marY=0.07)
```

---

## Learning to play

- Define $V(S)$ to be 1 if we win, 0 if we loose and 0.5 otherwise (reward/pr of winning).
- Most of the time we *exploit* our knowledge with $pr = 1-\epsilon$, i.e. choose the action which gives us the highest estimated reward and update the value of a state using $$V(S_t) = V(S_t) + \alpha(V(S_{t+1}-V(S_t)))$$ where $\alpha$ is the *step-size* parameter. 
- Some times we *explore* with $pr = \epsilon$ and choose another action/move than what seems optimal. 

---

```{r}
stateStr <- function(sV) {
   str <- str_c(sV, collapse = "")
   return(str)
}
stateVec <- function(s) {
   sV <- str_split(s, "")[[1]]
   return(sV)
}
```

```{r}
# return 1 (win), 0 (loose) or 0.5 (draw/unknown)
win <- function(pfx, sV) {
   idx <- which(sV == pfx)
   mineV <- rep(0, 9)
   mineV[idx] <- 1
   mineM <- matrix(mineV, 3, 3, byrow = TRUE)
   if (any(rowSums(mineM) == 3) ||  # win
      any(colSums(mineM) == 3) ||
      sum(diag(mineM)) == 3 ||
      sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 3) return(1)
   idx <- which(sV == ".")
   mineV[idx] <- 1
   mineM <- matrix(mineV, 3, 3, byrow = TRUE)
   if (any(rowSums(mineM) == 0) ||  # loose
      any(colSums(mineM) == 0) ||
      sum(diag(mineM)) == 0 ||
      sum(mineM[1,3] + mineM[2,2] + mineM[3,1]) == 0) return(0)
   return(0.5)  # draw
}
```

```{r}
PlayerRL <- R6Class("PlayerRL",
   public = list(
      pfx = "",
      hV = NA,
      control = list(epsilon = 0.2, alpha = 0.3),
      clearLearning = function() clear(self$hV),
      initialize = function(pfx = "", control = list(epsilon = 0.2, alpha = 0.3)) {
         self$pfx <- pfx
         self$control <- control
         self$hV <- hash()
      },
      finalize = function() {
         # cat("FIN\n")
         clear(self$hV)
      },
      move = function(sP, sV) { # previous state (before opponent move) and current state (before we move)
         idx <- which(sV == ".")
         state <- stateStr(sP)
         if (!has.key(state, self$hV)) self$hV[[state]] <- 0.5
         keys <- c()
         keysV <- NULL
         for (i in idx) {  # find possible moves
            sV[i] <- self$pfx
            str <- str_c(sV, collapse = "")
            keys <- c(keys, str)
            keysV <- rbind(keysV, sV)
            sV[i] <- "."
         }
         # add missing states
         idx <- which(!has.key(keys, self$hV))
         if (length(idx) > 0) {
            for (i in 1:nrow(keysV)) {
               self$hV[keys[i]] <- win(self$pfx, keysV[i,])
            }
         }
         # cat("Player", pfx, "\n")
         # print(self$hV)
         # update and find next state
         val <- values(self$hV[keys])
         # cat("Moves:"); print(val)
         m <- max(val)
         if (rbinom(1,1, self$control$epsilon) > 0 & any(val < m) & m < 1) { # explore
            idx <- which(val < m)
            idx <- idx[sample(length(idx), 1)]
            nextS <- names(val)[idx] 
            # cat("Explore - ")
         } else { # exploit
            idx <- which(val == m)
            idx <- idx[sample(length(idx), 1)]
            nextS <- names(val)[idx] # pick one
            self$hV[[state]] <- self$hV[[state]] + self$control$alpha * (m - self$hV[[state]])
            # cat("Exploit - ")
         }
         # cat("Next:", nextS, "\n")
         return(str_split(nextS, "")[[1]])
      }
   )
)
```

```{r}
## Implementation with players use OO reference class R6
PlayerRandom <- R6Class("PlayerRandom",
   public = list(
      pfx = NA,
      initialize = function(pfx) {
         self$pfx <- pfx
      },
      move = function(sP, sV) {  # previous state (before opponent move) and current state (before we move)
         idx <- which(sV == ".")
         state <- stateStr(sV)
         keys <- c()
         keysV <- NULL
         for (i in idx) {  # find possible moves
            sV[i] <- self$pfx
            str <- str_c(sV, collapse = "")
            keys <- c(keys, str)
            keysV <- rbind(keysV, sV)
            sV[i] <- "."
         }
         # check if can win in one move
         for (i in 1:nrow(keysV)) {
            if (win(self$pfx, keysV[i,]) == 1) {
               return(keysV[i,])  # next state is the win state
            }
         }
         # else pick one random
         return(keysV[sample(nrow(keysV), 1),])
      }
   )
)
```

```{r}
PlayerFirst <- R6Class("PlayerFirst",
   public = list(
      pfx = NA,
      initialize = function(pfx) {
         self$pfx <- pfx
      },
      move = function(sP, sV) { # previous state (before opponent move) and current state (before we move)
         idx <- which(sV == ".")
         sV[idx[1]] <- self$pfx
         return(sV)
      }
   )
)
```

```{r}
playGame <- function(player1, player2, verbose = FALSE) {
   sP2 <- rep(".", 9)  # start state / game state
   sP1 <- sP2          # state from player 1s viewpoint
   for (i in 1:5) { # at most 4.5 rounds
      ## player 1
      if (verbose) cat("Player ", player1$pfx, ":\n", sep="")
      sP1 <- player1$move(sP1, sP2)  # new state from player 1s viewpoint
      # states <- c(states, stateChr(sV))
      # cat(stateStr(sV), " | ", sep = "")
      if (verbose) plot_board_state_cat(stateStr(sP1))
      if (win(player1$pfx, sP1) == 1) {
         return(player1$pfx)
         break
      }
      if (i == 5) break  # a draw
      ## player 2
      if (verbose) cat("Player ", player2$pfx, ":\n", sep="")
      sP2 <- player2$move(sP2, sP1)
      # states <- c(states, stateChr(sV))
      # cat(stateStr(sV), " | ", sep = "")
      if (verbose) plot_board_state_cat(stateStr(sP2))
      if (win(player2$pfx, sP2) == 1) {
         return(player2$pfx)
         break
      }
   }
   return(NA)
}
```

```{r}
#' @param playerA Player A.
#' @param playerB Player B.
#' @param games Number of games
#' @param prA Probability of `playerA` starts.
#' @return A list with results (a data frame and a plot).  
playGames <- function(playerA, playerB, games, prA = 0.5) {
   winSeq <- rep(NA, games)
   for (g in 1:games) {
      # find start player
      if (sample(0:1, 1, prob = c(prA, 1-prA)) == 0) {
         player1 <- playerA
         player2 <- playerB
      } else {
         player2 <- playerA
         player1 <- playerB
      }
      winSeq[g] <- playGame(player1, player2)
   }
   # process the data
   dat <- tibble(game = 1:length(winSeq), winner = winSeq) %>% 
      mutate(
         players = str_c(playerA$pfx, playerB$pfx),
         winA := case_when(
            winner == playerA$pfx ~ 1,
            winner == playerB$pfx ~ 0,
            TRUE ~ 0.5
         ),
         winsA_r = rollapply(winA, ceiling(games/10), mean, align = "right", fill = NA)  #, fill = 0, partial = T
      )
   # make a plot
   pt <- dat %>% 
      ggplot(aes(x = game, y = winA)) +
      geom_line(aes(y = winsA_r), size = 0.2) +
      geom_smooth(se = F) +
      labs(y = str_c("Avg. wins player ", playerA$pfx),
           title = str_c("Wins ", playerA$pfx, " = ", round(mean(dat$winA), 2), " ", playerB$pfx, " = ", round(1-mean(dat$winA), 2)))
   return(list(dat = dat, plot = pt))
}
```

## Play against a player who moves randomly:

```{r, out.height="100%", fig.width = 12}
playerR <- PlayerRandom$new(pfx = "R")
lst <- list(list(epsilon = 0, alpha = 0.1), 
            list(epsilon = 0, alpha = 0.5), 
            list(epsilon = 0, alpha = 0.9),
            list(epsilon = 0.05, alpha = 0.9),
            list(epsilon = 0.9, alpha = 0.9))
res <- list()
for (i in seq_along(lst)) {
   playerA <- PlayerRL$new(pfx = "A", control = lst[[i]])
   res[[i]] <- playGames(playerA, playerR, games = 2000)$dat %>% mutate(control = str_c(c("e = ", "a = "), as.character(lst[[i]]), collapse = " ")) 
}
res <- map_dfr(res, ~ .x)
res %>% 
   ggplot(aes(x = game, y = winA, color = control)) +
   geom_line(aes(y = winsA_r, color = control), size = 0.2) +
   geom_smooth(se = F) +
   labs(title = "Number of wins for player A against R given different parameters",
        color = "Control") +
   theme(legend.position="bottom")
```

---

## Play against a "first free" player

```{r, out.height="100%", fig.width = 12}
playerF <- PlayerFirst$new(pfx = "F")
res <- list()
for (i in seq_along(lst)) {
   playerA <- PlayerRL$new(pfx = "A", control = lst[[i]])
   res[[i]] <- playGames(playerA, playerF, games = 2000)$dat %>% mutate(control = str_c(c("e = ", "a = "), as.character(lst[[i]]), collapse = " ")) 
}
res <- map_dfr(res, ~ .x)
res %>% 
   ggplot(aes(x = game, y = winA, color = control)) +
   geom_line(aes(y = winsA_r, color = control), size = 0.2) +
   geom_smooth(se = F) +
   labs(title = "Number of wins for player A against F given different parameters",
        color = "Control") +
   theme(legend.position="bottom")
```

---

## Play against another RL player

```{r, out.height="100%", fig.width = 12}
playerB <- PlayerRL$new(pfx = "B", control = list(epsilon = 0.1, alpha = 0.5))
lst <- list(list(epsilon = 0.05, alpha = 0.5),
            list(epsilon = 0.1,  alpha = 0.5),
            list(epsilon = 0.9,  alpha = 0.5),
            list(epsilon = 0.1,  alpha = 0.1),
            list(epsilon = 0.1,  alpha = 0.9)
            )
res <- list()
for (i in seq_along(lst)) {
   playerA <- PlayerRL$new(pfx = "A", control = lst[[i]])
   playerB$clearLearning()
   res[[i]] <- playGames(playerA, playerB, games = 2000)$dat %>% mutate(control = str_c(c("e = ", "a = "), as.character(lst[[i]]), collapse = " ")) 
}
res <- map_dfr(res, ~ .x)
res %>% 
   ggplot(aes(x = game, y = winA, color = control)) +
   geom_line(aes(y = winsA_r, color = control), size = 0.2) +
   geom_smooth(se = F) +
   labs(title = "Number of wins for player A against B (e = 0.1, a = 0.5) given different parameters",
        color = "Control") +
   theme(legend.position="bottom")
```










```{r links, child="../book/links.md"}
```

```{r copy to docs, include=FALSE}
system2("Rscript", args = "-e 'rmarkdown::render(\"index.Rmd\", quiet = TRUE)'")
file.copy(list.files(pattern = ".html"), "../docs/slides/", overwrite = T)
file.copy("./slides.css", "./libs/", overwrite = T)
file.copy("libs", "../docs/slides/", recursive = T)
file.copy("img", "../docs/slides/", recursive = T)
```
