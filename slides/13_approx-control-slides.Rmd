---
title: "On-Policy Control with Approximation"
author: "Lars Relund Nielsen"
output:
  xaringan::moon_reader:
    css: "./libs/slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
      slideNumberFormat: ""
editor_options: 
  chunk_output_type: console
---

```{r, child = "setup.Rmd", echo=FALSE}
```

```{r include=FALSE, eval=FALSE}
# to preview
xaringan::inf_mr(cast_from = ".", quiet = T)
```

layout: true
  
```{r, echo=FALSE}
module_name <- "approx-control"
module_number <- "13"
here::i_am(str_c("slides/", module_number, "_", module_name, "-slides.Rmd"))
library(htmltools)
footerHtml <- withTags({
   div(class="my-footer",
      span(
         a(href=str_c("https://bss-osca.github.io/rl/sec-", module_name, ".html"), target="_blank", "Notes"), 
         " | ",
         a(href=str_c("https://bss-osca.github.io/rl/slides/", module_number, "_", module_name, "-slides.html"), target="_blank", "Slides"),    
         " | ",
         a(href=str_c("https://github.com/bss-osca/rl/blob/master/slides/", module_number, "_", module_name, "-slides.Rmd"), target="_blank", "Source"),  
      )
   )
})
footerHtml
knitr::opts_chunk$set(fig.path=str_c("img/", module_name, "-"))
```



<!-- Templates -->
<!-- .pull-left[] .pull-right[] -->
<!-- knitr::include_graphics("img/bandit.png") -->
<!-- .left-column-wide[]  .right-column-small[] -->

---

## Learning outcomes

- Describe how to extend semi-gradient prediction to action-value approximation 
- Implement episodic one-step semi-gradient SARSA with $\epsilon$-greedy improvement.
- Be able to describe how to generalize to $n$-step semi-gradient SARSA in episodic tasks and explain the bias–variance trade-off as $n$ increases (from TD toward Monte Carlo).  
- Grasp why in continuing tasks with function approximation the discounted objective lacks a reliable local improvement guarantee, motivating a shift to average-reward.
- Define and interpret differential returns and differential value functions for the average-reward setting.
- Derive the Bellman equations under the average reward criterion.
- Describe differential TD errors and corresponding semi-gradient updates (state-value and action-value forms) using a running estimate of the average reward.
- Explain how to update the estimate of the average reward.

---

## On-Policy Control with Approximation

- In previous module: Focus on predicting the state values of a policy using function approximation. 
- Now: Emphasis on control, i.e. finding an optimal policy through function approximation of action values $\hat q(s, a, \textbf{w})$. 
- The focus is on on-policy methods.
- Episodic case: Extension from prediction to control is straightforward
- Continuing case: Discounting is not suitable to find an optimal policy. Here we have to switch from the discounting objective to an average-reward objective.

---

## Episodic Semi-gradient Control - Action values

Consider episodic tasks. 

- Goal is to find a good policy. 
- The action-value function is approximated by $\hat q(s,a,\mathbf{w}) \approx q^\pi(s,a)$ with weights $\mathbf{w}$.
- Training examples are now $(S_t, A_t) \mapsto U_t$, where $U_t$ is a target approx. $q^\pi(S_t,A_t)$. 
- One-step semi-gradient: $$U_t = R_{t+1} + \gamma\, \hat q(S_{t+1}, A_{t+1}, \mathbf{w}_t).$$ which bootstraps from the next state–action estimate.
- An $\varepsilon$-greedy policy can be used for exploration. 
- Learning is done using semi-gradient stochastic gradient descent on the squared error: $$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha\big[U_t - \hat q(S_t,A_t,\mathbf{w}_t)\big]\nabla_{\mathbf{w}} \hat q(S_t,A_t,\mathbf{w}_t).$$

---

## Episodic Semi-gradient Control - Improvement

- The action-value is an analogue of semi-gradient TD for state values used for prediction.
- Policy improvement and action selection are needed for doing control.
- If action set is discrete and not too large: Can use techniques developed so far:
  - Exploration using an $\varepsilon$-greedy policy.
  - Update action values using SARSA targets.
- Generalized policy iteration can be done and converge if use on-policy methods.

--

Will it work if action space is continuous? 

---

## Pseudo code for the episodic semi-gradient SARSA

```{r, echo=FALSE}
knitr::include_graphics("img/1001_Ep_Semi_Grad_Sarsa.png")
```

---

## What if use an off-policy algorithm?

- Can this algorithm can be modified to use Q-learning? 
- In general this may not work since this is an off-policy algorithm.
- Here we may diverge due to the “deadly triad” (off-policy + bootstrapping + approximation). There is no general convergence guarantee. 
- This is true even with linear features and fixed $\epsilon$-greedy behaviour.
- For further details you may read Chapter 11 in the book.

What about modifying the algorithm to use expected SARSA? 

---

## $n$-step SARSA

- To extend one-step SARSA we may extend our forward view to $n$-steps, i.e. target accumulates the next $n$ rewards before bootstrapping: $$G_{t:t+n} = U_t = \sum_{k=1}^{n} \gamma^{k-1}R_{t+k}\;+\;\gamma^{n}\,\hat q(S_{t+n}, A_{t+n}, \mathbf{w}).$$
- If the episode terminates  before $t+n$ then $G_{t:t+n}$ is just the episodic return ($G_t$). 
- The update now becomes $$\mathbf w \leftarrow \mathbf w + \alpha\big[G_{t:t+n} - \hat q(S_t,A_t,\mathbf w)\big]\nabla_w \hat q(S_t,A_t,\mathbf w).$$
<!-- - Exploration could be $\epsilon$-greedy with respect to $\hat q$. -->
- The choice of look-ahead steps ($n$) involves a bias-variance trade-off: 
  - $n=1$ enables rapid learning but can be shortsighted.
  - large $n$ approaches Monte Carlo methods, increasing variance. 
  - In practice, small to moderate $n$ works best (faster learning and good stability).

---

## The Average Reward Criterion

The average-reward formulation treats continuing tasks by optimizing the long-run reward rate instead of discounted returns. The performance of a policy $\pi$ is defined as the *steady-state average*
$$
\begin{align}
r(\pi) &= \lim_{h\to\infty}\frac{1}{h}\,\mathbb{E}_\pi\!\left[\sum_{t=1}^{h} R_t\right] \\
  &= \sum_s \mu_\pi(s) \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)r \\
  &= \sum_s \mu_\pi(s) \sum_a \pi(a|s) \sum_{s'} p(s'|s,a)r(s,a),
\end{align}
$$

Holds if the Markov chain (MDP under $\pi$) is *ergodic*, i.e., all states are reached with the same steady-state distribution $\mu_\pi(s)$, regardless of the starting state.

---

## Bellman equations (average reward criterion)

To measure preferences between states and actions without discounting, we introduce *differential returns* that subtract the average rate at each step: $$G_t \doteq \sum_{k=0}^{\infty}\big(R_{t+1+k}-r(\pi)\big).$$

The *differential action-value* functions then becomes
$$
\begin{align}
q^\pi(s,a) &= \mathbb{E}_\pi[G_t\mid S_t=s, A_t=a] \\
  &= \sum_{s',r} p(s',r\mid s,a)\Big(r - r(\pi) + \sum_{a'} \pi(a'\mid s')\,q^\pi(s',a')\Big).
\end{align}
$$
They satisfy Bellman relations analogous to the discounted case but without $\gamma$ and with rewards centered by $r(\pi)$.

---

## Control for Continuing Tasks (Average Reward)

- Replace discounted TD errors with differential TD errors.
- Keep an estimate of the average reward $\bar R$. 
- With function approximation on $\hat q(s,a,w)$ and a running estimate of $\bar R_t \approx r(\pi)$: $$U_t =  R_{t+1}-\bar R_t + \hat q(S_{t+1},A_{t+1},\textbf w_t).$$
- The Semi-gradient update becomes $$\textbf w \leftarrow \textbf w + \alpha\big[U_t - \hat q(S_t,A_t,\mathbf w)\big]\,\nabla_w \hat q(S_t,A_t,\textbf w).$$
- How to update the average-reward estimate? This can be done incrementally with a small step size to ensure stability, e.g.
$$
\bar R \leftarrow \bar R + \beta\delta_t^q, \qquad \delta_t^q = U_t - q(S_t,A_t,\mathbf w).
$$

<!-- Control replaces policy terms with maximization as usual, defining optimal differential values $v^*$ and $q^*$ and coupling learning with $\epsilon$-greedy improvement over $\hat q$. -->
- Note, the properties under discounting holds. We just use differential returns instead.

---

## Why avoidig discounted reward?

- Using the discounted reward criterion is ill-suited for truly continuing tasks once function approximation enters the picture. 
- Note, function approximation creates bias among states, i.e. changing $\mathbf w$ change the action values in multiple states.
- The policy improvement theorem does not apply with function approximation in the discounted setting. 
- The approximation errors can be amplified by discounting, and greedy improvement is not guaranteed to improve the policy. 
- This loss of a policy improvement theorem means discounted control lacks a firm local-improvement foundation under approximation.
- Best solution is to replace the discounted criterion with the average-reward and use differential values. Here the policy improvement theorem holds.

---

## Colab

Let us consider the an example in the [Colab tutorial][colab-13-approx-control].


<!-- # References -->

<!-- ```{r, results='asis', echo=FALSE} -->
<!-- PrintBibliography(bib) -->
<!-- ``` -->


```{r links, child="../book/links.md"}
```

```{r postprocess, include=FALSE}
system2("Rscript", args = "-e 'rmarkdown::render(\"index.Rmd\", quiet = TRUE)'")
file.copy("./slides.css", "./libs/", overwrite = T)
```
