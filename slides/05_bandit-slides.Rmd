---
title: "Multi-armed bandits"
author: "Lars Relund Nielsen"
output:
  xaringan::moon_reader:
    css: "./libs/slides.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false
      slideNumberFormat: ""
editor_options: 
  chunk_output_type: console
---

```{r, child = "setup.Rmd", echo=FALSE}
```

```{r include=FALSE, eval=FALSE}
# to preview
xaringan::inf_mr(cast_from = ".")
```

layout: true
  
```{r, echo=FALSE}
module_name <- "bandit"
module_number <- "05"
here::i_am(str_c("slides/", module_number, "_", module_name, "-slides.Rmd"))
library(htmltools)
footerHtml <- withTags({
   div(class="my-footer",
      span(
         a(href=str_c("https://bss-osca.github.io/rl/mod-", module_name, ".html"), target="_blank", "Notes"), 
         " | ",
         a(href=str_c("https://bss-osca.github.io/rl/slides/", module_number, "_", module_name, "-slides.html"), target="_blank", "Slides"),    
         " | ",
         a(href=str_c("https://github.com/bss-osca/rl/blob/master/slides/", module_number, "_", module_name, "-slides.Rmd"), target="_blank", "Source"),  
      )
   )
})
footerHtml
knitr::opts_chunk$set(fig.path=str_c("img/", module_name, "-"))
```

---

## Learning outcomes 

* Define a k-armed bandit and understand the nature of the problem. 
* Define the reward of an action (action-reward).
* Describe different methods for estimating the action-reward.
* Explain the differences between exploration and exploitation.
* Formulate an $\epsilon$-greedy algorithm for selecting the next action.
* Interpret the sample-average (variable step-size) versus exponential recency-weighted average (constant step-size) action-reward estimation.
* Argue why we might use a constant stepsize in the case of non-stationarity. 
* Understand the effect of optimistic initial values.
* Formulate an upper confidence bound action selection algorithm.

---

## The k-armed bandit problem

.pull-left[
* Multi-armed bandits attempt to find the best action (among $k$ actions) by learning through trial and error. 
* The name derives from “one-armed bandit,” a slang term for a slot machine. 
* After each choice, you receive a reward from an unknown probability dist.
* Objective is to maximise total reward over some time period.
* Natural strategy: try each action at random for a while (exploration). After a while start playing the higher paying ones (exploitation). 
]

.pull-right[
* The agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. 
* The process only has a single state $\Rightarrow$ find a policy that chooses the action with the highest expected reward. 

```{r echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("img/bandit.png")
```
]

---

## Possible application

* Multi-armed bandits can  be used in e.g. [digital advertising](https://research.facebook.com/blog/2021/4/auto-placement-of-ad-campaigns-using-multi-armed-bandits/). 
* You are an advertiser seeking to optimize which ads ( $k$ to choose among) to show a visitor on a particular website. 
* Your goal is to maximize the number of clicks over time. 

```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/bandit-choose.png")
```

---

## Estimating the value of an action

* Want to estimate the expected reward of an action $q_*(a) = \mathbb{E}[R_t | A_t = a]$. 
* Assume that at time $t$ action $a$ has been chosen $N_t(a)$ times. Then the estimated action value is
$$Q_t(a) = \frac{R_1+R_2+\cdots+R_{N_t(a)}}{N_t(a)},$$
* Storing $Q_t(a)$ this way is cumbersome since memory and computation requirements grow over time. Instead an *incremental* approach is better. 
* If we assume that $N_t(a) = n-1$ and set $Q_t(a) = Q_n$ then $Q_{n+1}$ is 
$$Q_{n+1} = Q_n + \frac{1}{n} \left[R_n - Q_n\right].$$
* We can update the estimate of the value of $a$ using the previous estimate, the observed reward and how many times the action has occurred. 

---

## Incremential value estimation

* A greedy approach for selecting the next action is
\begin{equation}
A_t =\arg \max_a Q_t(a).
\end{equation}
Here $\arg\max_a$ means the value of $a$ for which $Q_t(a)$ is maximised. 
* A pure greedy approach do not explore other actions. 
* For exploration use an $\varepsilon$-greedy approach: with probability $\varepsilon$ we take a random draw among all of the actions (choosing each action with equal probability).

---

## Let us try to code the algorithm

   - We need two classes: the agent and the environment.

   * The agent class store
     - $Q_t(a)$ (a vector)
     - $N_t(a)$ (a vector)
     - A method (function) that select the next action based on an $\varepsilon$-greedy approach.
     - A method (function) that update the $Q_t(a)$ using the incremental formula. 
  
--
  
   - The environment class store
     - The true mean rewards $\mu(a)$ (in a real life application these are unknown).
     - A method (function) that return a sample reward from $R \sim N(\mu(a),1)$.

---

## The role of the step-size

* In general we update the reward estimate of an action using 
\begin{equation}
	Q_{n+1} = Q_n +\alpha_n(a) \left[R_n - Q_n\right]
\end{equation}
* Sample average $\alpha_n(a)= 1/n$; however, other choices of $\alpha_n(a)$ is possible. In general we will converge to the true reward if
\begin{equation}
    \sum_n \alpha_n(a) = \infty \quad\quad \mathsf{and} \quad\quad  \sum_n \alpha_n(a)^2 < \infty.
\end{equation}
Meaning that the coefficients must be large enough to recover from initial fluctuations, but not so large that they do not converge in the long run. 

---

## The role of the step-size (2)

* Non-stationary process: the expected reward of an action change over time.
* Convergence is undesirable and we may want to use a constant $\alpha_n(a)= \alpha \in (0, 1]$ instead:
\begin{align}
Q_{n+1} &= Q_n +\alpha \left[R_n - Q_n\right] \nonumber \\
&= \alpha R_n + (1 - \alpha)Q_n \nonumber \\
& \vdots \nonumber \\
&= (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i \\
\end{align}
* More recent rewards are given more weight. 
* The weight given to each reward decays exponentially into the past (exponential recency-weighted average).

---

## Upper-Confidence Bound Action Selection

* $\epsilon$-greedy algorithm: choose the action to explore with equal probability in an exploration step. 
* Better to select among non-greedy actions according to their potential.
* This can be done by taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. 
* One way to do this is to select actions using the *upper-confidence bound*:
\begin{equation}
	A_t = \arg\max_a \left(Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right),
\end{equation}

---

## The square root term is a measure of the uncertainty

.left-column-wide[
* The more time has passed, and the less we have sampled an action, the higher UCB. 
* As the timesteps increases, the denominator dominates the numerator as the ln term flattens. 
* Each time we select an action our uncertainty decreases because $N$ is the denominator. 
* If $N_t(a) = 0$ then we consider $a$ as a maximal action, i.e. we select first among actions with $N_t(a) = 0$.
* The parameter $c>0$ controls the degree of exploration. Higher $c$ results in more weight on the uncertainty. 
]

 .right-column-small[
```{r srt, echo=FALSE}
n <- NULL
ctr = 1
for (i in 1:1000) {
   ctr <- ctr + rbinom(1,1,runif(1,0.01,0.05))
   n <- c(n, ctr)
}
dat <- tibble(t = 1:1000, c1 = sqrt(log(t)/n), c5 = 5*sqrt(log(t)/n), c20 = 20*sqrt(log(t)/n)) %>% 
   pivot_longer(!t, names_to = "c", values_to = "sr", names_prefix = "c")
p1 <- dat %>% ggplot(aes(x = t, y = sr, col = c)) +
   geom_line() + ylab("square root term")
dat <- tibble(t = 1:1000, visits = n)
p2 <- dat %>% ggplot(aes(x = t, y = visits)) +
   geom_line()
library(patchwork)
p1 / p2 + plot_layout(heights = c(2, 1))
```
]


```{r links, child="../book/links.md"}
```

```{r postprocess, include=FALSE}
system2("Rscript", args = "-e 'rmarkdown::render(\"index.Rmd\", quiet = TRUE)'")
file.copy("./slides.css", "./libs/", overwrite = T)
```
