<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>An introduction to Reinforcement Learning (RL)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Lars Relund Nielsen" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="libs/slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# An introduction to<br/>Reinforcement Learning (RL)
]
.author[
### Lars Relund Nielsen
]

---











layout: true
  
<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

---

## Learning outcomes 

* Describe what RL is. 
* Be able to identify different sequential decision problems.
* Know what Business Analytics are and identify RL in that framework.
* Memorise different names for RL and how it fits in a Machine Learning framework.
* Formulate the blocks of a RL model (environment, agent, data, states, actions, rewards and policies).
* Run your first RL algorithm and evaluate on its solution.

---

## What is reinforcement learning

RL can be seen as

* An approach of modelling sequential decision making problems.
* An approach for learning good decision making under uncertainty from experience.
* Mathematical models for learning-based decision making.
* Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.
* Estimating and finding near optimal decisions of a stochastic process with sequential decision making. 
* A model where, given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.

---

## Sequential decision problems

Examples (with possible actions) are:

* Playing backgammon (how to move the checkers).
* [Driving a car](https://arxiv.org/pdf/1807.00412.pdf) (left, right, forward, back, break, stop, ...).
* How to [invest/maintain a portfolio of stocks](https://medium.com/ibm-data-ai/reinforcement-learning-the-business-use-case-part-2-c175740999) (buy, sell, amount).  
* [Control an inventory](https://www.youtube.com/watch?v=pxWkg2N0l9c) (wait, buy, amount).
* Vehicle routing (routes).
* Maintain a spare-part (wait, maintain).
* [Robot operations](https://arxiv.org/pdf/2103.14295.pdf) (sort, move, ...)
* [Dairy cow treatment/replacement](http://dx.doi.org/10.1016/j.ejor.2019.01.050) (treat, replace, ...)
* Recommender systems e.g. [Netflix recommendations](https://scale.com/blog/Netflix-Recommendation-Personalization-TransformX-Scale-AI-Insights) (videos)

Note current decisions have an impact on the future. 

---

## RL and intuition

RL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance:

* totally random trials (in the start),
* sophisticated tactics and superhuman skills (in the end). 

That is, as the agent learn, the reward estimate of a given action becomes better. 

As humans, we often learn by trial and error too:

* Learning to walk (by falling/pain).
* Learning to play (strategy is based on the game rules and what we have experienced works based on previous plays). 

This can also be seen as learning the reward of our actions. 


---

layout: true

## RL in a Business Analytics framework



---

background-image: url("./img/analytics_plot1.png")

---

background-image: url("./img/analytics_plot2.png")

---

background-image: url("./img/analytics_plot3.png")

---

background-image: url("./img/analytics_plot4.png")

---

background-image: url("./img/analytics_plot5.png")

---

background-image: url("./img/analytics_plot6.png")

---

background-image: url("./img/analytics_plot7.png")

---

background-image: url("./img/analytics_plot8.png")

---

background-image: url("./img/analytics_plot9.png")

---

layout: true

<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

---

## RL in different research deciplines

.pull-left[
RL is used in many research fields using different names
- RL (most used) originated from computer science and AI.
- *Approximate dynamic programming (ADP)* is mostly used within operations research. 
- *Neuro-dynamic programming* or *Deep RL* is used when states or actions are represented using a neural network.
- RL is closely related to *Markov decision processes*, which is a mathematical model for a sequential decision problem.
]

.pull-right[


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-names.png" alt="Adopted from Silver (2015)." width="100%" /&gt;
&lt;p class="caption"&gt;Adopted from Silver (2015).&lt;/p&gt;
&lt;/div&gt;
]

---

## RL in a Machine Learning framework

.pull-left[
* **Supervised learning:** Given data `\((x_i, y_i)\)` learn to predict `\(y\)` from `\(x\)`, i.e. find `\(y \approx f(x)\)` (e.g. regression).
* **Unsupervised learning:** Given data `\((x_i)\)` learn patterns using `\(x\)`, i.e. find `\(f(x)\)` (e.g. clustering).
&lt;!-- * Often assume that data are independent and identically distributed (iid).  --&gt;
* **RL:** Given state `\(x\)` you take an action and observe the reward `\(r\)` and the new state `\(x'\)`.
 - There is no supervisor `\(y\)`, only a reward signal `\(r\)`.
 - Your goal is to find a policy that optimize the total reward function.
]

.pull-right[


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-ml.png" alt="Adopted from Silver (2015)." width="100%" /&gt;
&lt;p class="caption"&gt;Adopted from Silver (2015).&lt;/p&gt;
&lt;/div&gt;
]

---

layout: true

<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

## The RL data-stream



---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0)\)`
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-11-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0, A_0)\)`
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-12-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0, A_0, R_1, O_1)\)`
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-13-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0, A_0, R_1, O_1, A_1)\)`
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-14-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0, A_0, R_1, O_1, A_1, R_2, O_2)\)`
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-15-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

.left-column-wide[
- Agent: The one who takes the action (computer, robot, decision maker).
- Environment: The system/world where observations and rewards are found. 
- Data are revealed sequentially as you take actions:
  * `\((O_0, A_0, R_1, O_1, A_1, R_2, O_2, \ldots)\)`
- History at time `\(t\)`: `$$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t)$$`
- Your goal is to find a policy that maximize the total future reward.
]

.right-column-small[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/rl-intro-unnamed-chunk-16-1.png" alt="Agent-environment representation." width="100%" /&gt;
&lt;p class="caption"&gt;Agent-environment representation.&lt;/p&gt;
&lt;/div&gt;
]

---

layout:false

<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

## Reward a closer look

- The reward `\(R_t\)` is a number representing the reward at time `\(t\)` (negative if a cost).
   * Playing backgammon (0 (when play), 1 (when win), -1 (when loose)).
   * How to invest/maintain a portfolio of stocks (the profit).  
   * Control an inventory (inventory cost, lost sales cost).
   * Vehicle routing (transportation cost).
--

- Reward may be delayed, not instantaneous (the consequences of you decision now is revealed later).
--

- RL assumption: all goals can be transformed into the maximisation of expected total future (cumulative) reward.

&lt;!-- - Time really matters (sequential/dynamic system, non iid data). --&gt;
&lt;!-- - Agent’s actions affect the subsequent data it receives. --&gt;

---

layout:true

<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

## History vs state

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards `$$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$`
]
.right-column-small[
&lt;img src="img/rl-intro-unnamed-chunk-19-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards `$$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$`
- The state `\(S_t\)` is the information used to take the next action.
]
.right-column-small[
&lt;img src="img/rl-intro-unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

.left-column-wide[
- The history is the sequence of observations, actions and rewards `$$H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).$$`
- The state `\(S_t\)` is the information used to take the next action.
- The next action `\(A_t\)` depends on the history, i.e. a state is a function of the history `\(S_t = f(H_t)\)`.
  * Choosing `\(S_t = H_t\)` is bad.
  * Instead just store the information needed for taking the next action. 
  * Markov state: given the present state the future is independent of the past.  
]
.right-column-small[
&lt;img src="img/rl-intro-unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

layout:false

layout:true

<div class="my-footer">
<span>
<a href="https://bss-osca.github.io/rl/sec-rl-intro.html" target="_blank">Notes</a>
 | 
<a href="https://bss-osca.github.io/rl/slides/02_rl-intro-slides.html" target="_blank">Slides</a>
 | 
<a href="https://github.com/bss-osca/rl/blob/master/slides/02_rl-intro-slides.Rmd" target="_blank">Source</a>
</span>
</div>

---

## Policy 

- A *policy* is the agent’s behaviour
- It is a map from state to action, i.e. a function 
  `$$a = \pi(s)$$` 
  saying that given the agent is in state `\(s\)` we choose action `\(a\)`.
- Given state `\(S_t\)` the goal is to find a policy that maximize the total future reward.

---

## Value of a state

- We use the *value function* to predict the future reward in state `\(S\)` e.g. expected discounted future reward:
`$$V_\pi(s) = \mathbb{E}_\pi(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S = s).$$` 
- Discount factor `\(\gamma=0\)`: Only care about present reward.
- Discount factor `\(\gamma=1\)`: Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite.
- Discount factor `\(\gamma&lt;1\)`: Rewards near to the present more beneficial. Note `\(V(s)\)` will converge to a number even if the time-horizon is infinite.
- Policy that maximize the total future reward given state `\(s\)`: `$$\pi^* = \arg\max_{\pi\in\Pi}(V_\pi(s)).$$`

&lt;!-- ## Model free vs Model based --&gt;

---


## Exploitation vs Exploration

- Exploitation: Taking the action assumed to be optimal with respect to the data observed so far. 
  * Give better predictions of the value function (given the current policy). 
  * Prevents the agent from discovering potential better decisions (a better policy).
- Exploration: Not taking the action that seems to be optimal. 
  * The agent explore to find states we may not see and hence can update the value function for this state.  
- Examples
  * Movies recommendation: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration).
  * Oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration).



---

layout:false

# References

Silver, D. (2015). _Lectures on Reinforcement Learning_. URL:
[https://www.davidsilver.uk/teaching/](https://www.davidsilver.uk/teaching/).



[BSS]: https://bss.au.dk/en/
[bi-programme]: https://masters.au.dk/businessintelligence

[course-help]: https://github.com/bss-osca/rl/issues
[cran]: https://cloud.r-project.org
[cheatsheet-readr]: https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf
[course-welcome-to-the-tidyverse]: https://github.com/rstudio-education/welcome-to-the-tidyverse
[Colab]: https://colab.google/
[colab-01-intro-colab]: https://colab.research.google.com/drive/1o_Dk4FKTsDxPYxTXBRAUEsfPYU3dJhxg?usp=sharing
[colab-03-rl-in-action]: https://colab.research.google.com/drive/18O9MruUBA-twpIDpc-9boXQw-cSjkRoD?usp=sharing
[colab-03-rl-in-action-ex]: https://colab.research.google.com/drive/18O9MruUBA-twpIDpc-9boXQw-cSjkRoD#scrollTo=JUKOdK_UqKRJ&amp;line=3&amp;uniqifier=1
[colab-04-python]: https://colab.research.google.com/drive/1_TQoJVTJPiXbynegeUtzTWBgktpL5VQT?usp=sharing
[colab-04-debug-python]: https://colab.research.google.com/drive/1JHVxbE89iJ8CGJuwY-A4aEEbWYXMH4dp?usp=sharing
[colab-05-bandit]: https://colab.research.google.com/drive/19-tUda-gBb40NWHjpSQboqWq18jYpHPs?usp=sharing
[colab-05-ex-bandit-adv]: https://colab.research.google.com/drive/19-tUda-gBb40NWHjpSQboqWq18jYpHPs#scrollTo=Df1pWZ-DZB7v&amp;line=1
[colab-05-ex-bandit-coin]: https://colab.research.google.com/drive/19-tUda-gBb40NWHjpSQboqWq18jYpHPs#scrollTo=gRGiE26m3inM
[colab-08-dp]: https://colab.research.google.com/drive/1PrLZ2vppqnq0xk0_Qu7UiW3fASftZUX6?usp=sharing
[colab-08-dp-ex-storage]: https://colab.research.google.com/drive/1PrLZ2vppqnq0xk0_Qu7UiW3fASftZUX6#scrollTo=nY6zWiv_3ikg&amp;line=21&amp;uniqifier=1
[colab-08-dp-sec-dp-gambler]: https://colab.research.google.com/drive/1PrLZ2vppqnq0xk0_Qu7UiW3fASftZUX6#scrollTo=GweToDSPd5gj&amp;line=1&amp;uniqifier=1
[colab-08-dp-sec-dp-maintain]: https://colab.research.google.com/drive/1PrLZ2vppqnq0xk0_Qu7UiW3fASftZUX6#scrollTo=HQnlVuuufR_Q&amp;line=1&amp;uniqifier=1
[colab-08-dp-sec-dp-car]: https://colab.research.google.com/drive/1PrLZ2vppqnq0xk0_Qu7UiW3fASftZUX6#scrollTo=xERxGYQDkR87&amp;line=1&amp;uniqifier=1
[colab-09-mc]: https://colab.research.google.com/drive/1I4gBqDqYQAEPOVlMqTyBG1AKSHTgyDm-?usp=sharing
[colab-09-mc-sec-mc-seasonal-ex]: https://colab.research.google.com/drive/1I4gBqDqYQAEPOVlMqTyBG1AKSHTgyDm-#scrollTo=1BzUCPQxstvQ&amp;line=3&amp;uniqifier=1
[colab-10-td-pred]: https://colab.research.google.com/drive/1JhLDAtc-5lJ3fzp7natjT_ea_JRiQS7d?usp=sharing
[colab-10-td-pred-sec-ex-td-pred-random]: https://colab.research.google.com/drive/1JhLDAtc-5lJ3fzp7natjT_ea_JRiQS7d#scrollTo=1BzUCPQxstvQ&amp;line=4&amp;uniqifier=1



[DataCamp]: https://www.datacamp.com/
[datacamp-signup]: https://www.datacamp.com/groups/shared_links/45955e75eff4dd8ef9e8c3e7cbbfaff9e28e393b38fc25ce24cb525fb2155732
[datacamp-r-intro]: https://learn.datacamp.com/courses/free-introduction-to-r
[datacamp-r-rmarkdown]: https://campus.datacamp.com/courses/reporting-with-rmarkdown
[datacamp-r-communicating]: https://learn.datacamp.com/courses/communicating-with-data-in-the-tidyverse
[datacamp-r-communicating-chap3]: https://campus.datacamp.com/courses/communicating-with-data-in-the-tidyverse/introduction-to-rmarkdown
[datacamp-r-communicating-chap4]: https://campus.datacamp.com/courses/communicating-with-data-in-the-tidyverse/customizing-your-rmarkdown-report
[datacamp-r-intermediate]: https://learn.datacamp.com/courses/intermediate-r
[datacamp-r-intermediate-chap1]: https://campus.datacamp.com/courses/intermediate-r/chapter-1-conditionals-and-control-flow
[datacamp-r-intermediate-chap2]: https://campus.datacamp.com/courses/intermediate-r/chapter-2-loops
[datacamp-r-intermediate-chap3]: https://campus.datacamp.com/courses/intermediate-r/chapter-3-functions
[datacamp-r-intermediate-chap4]: https://campus.datacamp.com/courses/intermediate-r/chapter-4-the-apply-family
[datacamp-r-functions]: https://learn.datacamp.com/courses/introduction-to-writing-functions-in-r
[datacamp-r-tidyverse]: https://learn.datacamp.com/courses/introduction-to-the-tidyverse
[datacamp-r-strings]: https://learn.datacamp.com/courses/string-manipulation-with-stringr-in-r
[datacamp-r-dplyr]: https://learn.datacamp.com/courses/data-manipulation-with-dplyr
[datacamp-r-dplyr-bakeoff]: https://learn.datacamp.com/courses/working-with-data-in-the-tidyverse
[datacamp-r-ggplot2-intro]: https://learn.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2
[datacamp-r-ggplot2-intermediate]: https://learn.datacamp.com/courses/intermediate-data-visualization-with-ggplot2
[dplyr-cran]: https://CRAN.R-project.org/package=dplyr

[google-form]: https://forms.gle/s39GeDGV9AzAXUo18
[google-grupper]: https://docs.google.com/spreadsheets/d/1DHxthd5AQywAU4Crb3hM9rnog2GqGQYZ2o175SQgn_0/edit?usp=sharing
[GitHub]: https://github.com/
[git-install]: https://git-scm.com/downloads
[github-actions]: https://github.com/features/actions
[github-pages]: https://pages.github.com/
[gh-rl-student]: https://github.com/bss-osca/rl-student
[gh-rl]: https://github.com/bss-osca/rl

[happy-git]: https://happygitwithr.com
[hg-install-git]: https://happygitwithr.com/install-git.html
[hg-why]: https://happygitwithr.com/big-picture.html#big-picture
[hg-github-reg]: https://happygitwithr.com/github-acct.html#github-acct
[hg-git-install]: https://happygitwithr.com/install-git.html#install-git
[hg-exist-github-first]: https://happygitwithr.com/existing-github-first.html
[hg-exist-github-last]: https://happygitwithr.com/existing-github-last.html
[hg-credential-helper]: https://happygitwithr.com/credential-caching.html
[hypothes.is]: https://web.hypothes.is/

[Jupyter]: https://jupyter.org/

[osca-programme]: https://masters.au.dk/operationsandsupplychainanalytics

[Peergrade]: https://peergrade.io
[peergrade-signup]: https://app.peergrade.io/join
[point-and-click]: https://en.wikipedia.org/wiki/Point_and_click
[pkg-bookdown]: https://bookdown.org/yihui/bookdown/
[pkg-openxlsx]: https://ycphs.github.io/openxlsx/index.html
[pkg-ropensci-writexl]: https://docs.ropensci.org/writexl/
[pkg-jsonlite]: https://cran.r-project.org/web/packages/jsonlite/index.html
[Python]: https://www.python.org/
[Positron]: https://positron.posit.co/
[PyCharm]: https://www.jetbrains.com/pycharm/
[VSCode]: https://code.visualstudio.com/

[R]: https://www.r-project.org
[RStudio]: https://rstudio.com
[rstudio-cloud]: https://rstudio.cloud/spaces/176810/join?access_code=LSGnG2EXTuzSyeYaNXJE77vP33DZUoeMbC0xhfCz
[r-cloud-mod12]: https://rstudio.cloud/spaces/176810/project/2963819
[r-cloud-mod13]: https://rstudio.cloud/spaces/176810/project/3020139
[r-cloud-mod14]: https://rstudio.cloud/spaces/176810/project/3020322
[r-cloud-mod15]: https://rstudio.cloud/spaces/176810/project/3020509
[r-cloud-mod16]: https://rstudio.cloud/spaces/176810/project/3026754
[r-cloud-mod17]: https://rstudio.cloud/spaces/176810/project/3034015
[r-cloud-mod18]: https://rstudio.cloud/spaces/176810/project/3130795
[r-cloud-mod19]: https://rstudio.cloud/spaces/176810/project/3266132
[rstudio-download]: https://rstudio.com/products/rstudio/download/#download
[rstudio-customizing]: https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio
[rstudio-key-shortcuts]: https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts
[rstudio-workbench]: https://www.rstudio.com/wp-content/uploads/2014/04/rstudio-workbench.png
[r-markdown]: https://rmarkdown.rstudio.com/
[ropensci-writexl]: https://docs.ropensci.org/writexl/
[r4ds-pipes]: https://r4ds.had.co.nz/pipes.html
[r4ds-factors]: https://r4ds.had.co.nz/factors.html
[r4ds-strings]: https://r4ds.had.co.nz/strings.html
[r4ds-iteration]: https://r4ds.had.co.nz/iteration.html


[stat-545]: https://stat545.com
[stat-545-functions-part1]: https://stat545.com/functions-part1.html
[stat-545-functions-part2]: https://stat545.com/functions-part2.html
[stat-545-functions-part3]: https://stat545.com/functions-part3.html
[slides-welcome]: https://bss-osca.github.io/rl/slides/00-rl_welcome.html
[slides-m1-3]: https://bss-osca.github.io/rl/slides/01-welcome_r_part.html
[slides-m4-5]: https://bss-osca.github.io/rl/slides/02-programming.html
[slides-m6-8]: https://bss-osca.github.io/rl/slides/03-transform.html
[slides-m9]: https://bss-osca.github.io/rl/slides/04-plot.html
[slides-m83]: https://bss-osca.github.io/rl/slides/05-joins.html
[sutton-notation]: https://bss-osca.github.io/rl/misc/sutton-notation.pdf

[tidyverse-main-page]: https://www.tidyverse.org
[tidyverse-packages]: https://www.tidyverse.org/packages/
[tidyverse-core]: https://www.tidyverse.org/packages/#core-tidyverse
[tidyverse-ggplot2]: https://ggplot2.tidyverse.org/
[tidyverse-dplyr]: https://dplyr.tidyverse.org/
[tidyverse-tidyr]: https://tidyr.tidyverse.org/
[tidyverse-readr]: https://readr.tidyverse.org/
[tidyverse-purrr]: https://purrr.tidyverse.org/
[tidyverse-tibble]: https://tibble.tidyverse.org/
[tidyverse-stringr]: https://stringr.tidyverse.org/
[tidyverse-forcats]: https://forcats.tidyverse.org/
[tidyverse-readxl]: https://readxl.tidyverse.org
[tidyverse-googlesheets4]: https://googlesheets4.tidyverse.org/index.html
[tutorial-markdown]: https://commonmark.org/help/tutorial/
[tfa-course]: https://bss-osca.github.io/tfa/

[video-install]: https://vimeo.com/415501284
[video-rstudio-intro]: https://vimeo.com/416391353
[video-packages]: https://vimeo.com/416743698
[video-projects]: https://vimeo.com/319318233
[video-r-intro-p1]: https://www.youtube.com/watch?v=vGY5i_J2c-c
[video-r-intro-p2]: https://www.youtube.com/watch?v=w8_XdYI3reU
[video-r-intro-p3]: https://www.youtube.com/watch?v=NuY6jY4qE7I
[video-subsetting]: https://www.youtube.com/watch?v=hWbgqzsQJF0&amp;list=PLjTlxb-wKvXPqyY3FZDO8GqIaWuEDy-Od&amp;index=10&amp;t=0s
[video-datatypes]: https://www.youtube.com/watch?v=5AQM-yUX9zg&amp;list=PLjTlxb-wKvXPqyY3FZDO8GqIaWuEDy-Od&amp;index=10
[video-control-structures]: https://www.youtube.com/watch?v=s_h9ruNwI_0
[video-conditional-loops]: https://www.youtube.com/watch?v=2evtsnPaoDg
[video-functions]: https://www.youtube.com/watch?v=ffPeac3BigM
[video-tibble-vs-df]: https://www.youtube.com/watch?v=EBk6PnvE1R4
[video-dplyr]: https://www.youtube.com/watch?v=aywFompr1F4

[wiki-snake-case]: https://en.wikipedia.org/wiki/Snake_case
[wiki-camel-case]: https://en.wikipedia.org/wiki/Camel_case
[wiki-interpreted]: https://en.wikipedia.org/wiki/Interpreted_language
[wiki-literate-programming]: https://en.wikipedia.org/wiki/Literate_programming
[wiki-csv]: https://en.wikipedia.org/wiki/Comma-separated_values
[wiki-json]: https://en.wikipedia.org/wiki/JSON


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "ratio": "16:9",
  "highlightLines": true,
  "highlightStyle": "solarized-light",
  "countIncrementalSlides": false,
  "slideNumberFormat": ""
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
