<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Monte Carlo methods for prediction and control – Reinforcement Learning for Business (RL)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./10_td-pred.html" rel="next">
<link href="./08_dp.html" rel="prev">
<link href="./assets/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-d58359eb3ecc31d3bec1ed479c2cbb3a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="assets/style.css">
<meta property="og:title" content="9&nbsp; Monte Carlo methods for prediction and control – Reinforcement Learning for Business (RL)">
<meta property="og:description" content="Course notes for the ‘Reinforcement Learning for Business’ course.">
<meta property="og:image" content="https://github.com/bss-osca/rl/raw/master/book/img/logo.png">
<meta property="og:site_name" content="Reinforcement Learning for Business (RL)">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./09_mc.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo methods for prediction and control</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/logo.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./img/logo.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Reinforcement Learning for Business (RL)</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/bss-osca/rl/tree/master/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Introduction</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About the course notes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_rl-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">An introduction to RL</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_rl-in-action.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">RL in action</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">An introduction to Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Tabular methods</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_bandit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multi-armed bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_mdp-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markov decision processes (MDPs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_mdp-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policies and value functions for MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_mc.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo methods for prediction and control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_td-pred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Temporal difference methods for prediction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_td-control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Temporal difference methods for control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99_1_appdx.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99_2_appdx.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Getting help</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#learning-outcomes" id="toc-learning-outcomes" class="nav-link active" data-scroll-target="#learning-outcomes"><span class="header-section-number">9.1</span> Learning outcomes</a></li>
  <li><a href="#textbook-readings" id="toc-textbook-readings" class="nav-link" data-scroll-target="#textbook-readings"><span class="header-section-number">9.2</span> Textbook readings</a></li>
  <li><a href="#mc-prediction-evaluation" id="toc-mc-prediction-evaluation" class="nav-link" data-scroll-target="#mc-prediction-evaluation"><span class="header-section-number">9.3</span> MC prediction (evaluation)</a></li>
  <li><a href="#mc-control-improvement" id="toc-mc-control-improvement" class="nav-link" data-scroll-target="#mc-control-improvement"><span class="header-section-number">9.4</span> MC control (improvement)</a></li>
  <li><a href="#sec-mc-off-policy" id="toc-sec-mc-off-policy" class="nav-link" data-scroll-target="#sec-mc-off-policy"><span class="header-section-number">9.5</span> Off-policy MC prediction</a></li>
  <li><a href="#off-policy-control-improvement" id="toc-off-policy-control-improvement" class="nav-link" data-scroll-target="#off-policy-control-improvement"><span class="header-section-number">9.6</span> Off-policy control (improvement)</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">9.7</span> Summary</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">9.8</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/bss-osca/rl/edit/master/book/09_mc.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bss-osca/rl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/bss-osca/rl/blob/master/book/09_mc.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-mc" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo methods for prediction and control</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- Various algorithms for the RL course -->
<p>The term “Monte Carlo” (MC) is often used for an estimation method which involves a random component. MC methods of RL learn state and action values by sampling and averaging returns. MC do not use dynamics where we estimate the value in the current state using the value in the next state (like in dynamic programming). Instead the MC methods estimate the values by considering different <em>sample-paths</em> (state, action and reward realizations). Compared to a Markov decision process, MC methods are model-free since they not require full knowledge of the transition probabilities and rewards (a model of the environment) instead MC methods learn the value function directly from experience. Often though, the sample-path is generated using simulation, i.e.&nbsp;some knowledge about the environment is given, but it is only used to generate sample transitions. For instance, consider an MDP model for the game Blackjack. Here calculating all the transition probabilities may be tedious and error-prone in terms of coding and numerical precision. Instead we can simulate a game (a sample-path) and use the simulations to evaluate/predict the value function of a policy and then use control to find a good policy. That is, we still use a generalised policy iteration framework, but instead of computing the value function using the MDP model a priori, we learn it from experience.</p>
<p>MC methods can be used for processes with episodes, i.e.&nbsp;where there is a terminal state. This reduces the length of the sample-path and the value of the states visited on the path can be updated based on the reward received.</p>
<section id="learning-outcomes" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="learning-outcomes"><span class="header-section-number">9.1</span> Learning outcomes</h2>
<p>By the end of this module, you are expected to:</p>
<ul>
<li>Identify the difference between model-based and model-free RL.</li>
<li>Identify problems that can be solved using Monte-Carlo methods.</li>
<li>Describe how MC methods can be used to estimate value functions from sample data.</li>
<li>Do MC prediction to estimate the value function for a given policy.</li>
<li>Explain why it is important to maintain exploration in MC algorithms.</li>
<li>Do policy improvement (control) using MC in a generalized policy improvement algorithm.</li>
<li>Compare different ways of exploring the state-action space.</li>
<li>Argue why off-policy learning can help deal with the exploration problem.</li>
<li>Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.</li>
<li>Use importance sampling in off-policy learning to predict the value-function of a target policy.</li>
<li>Explain how to modify the MC prediction and improvement algorithm for off-policy learning.</li>
</ul>
<p>The learning outcomes relate to the <a href="index.html#sec-lg-course">overall learning goals</a> number 3, 4, 9 and 12 of the course.</p>
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</section>
<section id="textbook-readings" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="textbook-readings"><span class="header-section-number">9.2</span> Textbook readings</h2>
<p>For this module, you will need to read Chapter 5-5.7 in <span class="citation" data-cites="Sutton18">Sutton and Barto (<a href="references.html#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module. A summary of the book notation can be seen <a href="https://bss-osca.github.io/rl/misc/sutton-notation.pdf">here</a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
Slides for this module can be seen
<a href="https://bss-osca.github.io/rl/slides/09_mc-slides.html" target="_blank">here.</a>
You do not have to look at them before the lecture!
</div>
</div>
</div>
</section>
<section id="mc-prediction-evaluation" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="mc-prediction-evaluation"><span class="header-section-number">9.3</span> MC prediction (evaluation)</h2>
<p>Given a policy <span class="math inline">\(\pi\)</span>, we want to estimate the state-value function. Recall that the state value function is <span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s].
\]</span> where the return is <span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}
\]</span> Now given policy <span class="math inline">\(\pi\)</span> and a sample-path (episode) <span class="math inline">\(S_0, A_0, R_1, S_1, A_1, \ldots, S_{T-1}, A_{T-1}, R_T\)</span> ending in the terminal state at time <span class="math inline">\(T\)</span>, we can calculate the realized return for each state in the sample-path. Each time we have a new sample-path a new realized return for the states is given and the average for the returns in a state is an estimate of the state-value. With enough observations, the sample average converges to the true state-value under the policy <span class="math inline">\(\pi\)</span>.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and a set of sample-paths, there are two ways to estimate the state values <span class="math inline">\(v_\pi(s)\)</span>:</p>
<ul>
<li>First visit MC: average returns from first visit to state <span class="math inline">\(s\)</span>.</li>
<li>Every visit MC: average returns following every visit to state <span class="math inline">\(s\)</span>.</li>
</ul>
<p>First visit MC generates iid estimates of <span class="math inline">\(v_\pi(s)\)</span> with finite variance, so the sequence of estimates converges to the expected value by the law of large numbers as the number of observations grow. Every visit MC does not generate independent estimates, but still converges.</p>
<p>An algorithm for first visit MC is given in <a href="#fig-mc-prediction-alg" class="quarto-xref">Figure&nbsp;<span>9.1</span></a>. The state-value estimate is stored in a vector <span class="math inline">\(V\)</span> and the returns for each state in a list. Given a sample-path we add the return to the states on the path by scanning the path backwards and updating <span class="math inline">\(G\)</span>. Note since the algorithm considers first visit MC, a check of occurrence of the state earlier in the path done. If this check is dropped, we have a every visit MC algorithm instead. Moreover, the computation needed to update the state-value does not depend on the size of the process/MDP but only of the length of the sample-path.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mc-prediction-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mc-prediction-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mc-prediction.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="629">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mc-prediction-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: MC policy prediction <span class="citation" data-cites="Sutton18">(<a href="references.html#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The algorithm maintains a list of all returns for each state which may require a lot of memory. Instead as incremental update of <span class="math inline">\(V\)</span> can be done. Adapting <a href="05_bandit.html#eq-avg" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>, we have that the sample average can be updated using:</p>
<p><span class="math display">\[
  V(s) \leftarrow V(s) + \frac{1}{n} \left[G - V(s)\right].
\]</span> where <span class="math inline">\(n\)</span> denote the number of realized returns found for state <span class="math inline">\(s\)</span> and <span class="math inline">\(G\)</span> the current realized return. The state-value vector must be initialized to zero and a vector counting the number of returns found for each state must be stored.</p>
<!-- ### Blackjack - MC prediction -->
<section id="mc-prediction-of-action-values" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="mc-prediction-of-action-values"><span class="header-section-number">9.3.1</span> MC prediction of action-values</h3>
<p>With a model of the environment we only need to estimate the state-value function, since it is easy to determine the policy from the state-values using the Bellman optimality equations <a href="07_mdp-2.html#eq-bell-opt-state-policy" class="quarto-xref">Equation&nbsp;<span>7.3</span></a>. However, if we do not know the expected reward and transition probabilities state values are not enough. In that case, it is useful to estimate action-values since the optimal policy can be found using <span class="math inline">\(q_*\)</span> (see <a href="07_mdp-2.html#eq-bell-opt-state-policy" class="quarto-xref">Equation&nbsp;<span>7.3</span></a>). To find <span class="math inline">\(q_*\)</span>, we first need to predict action-values for a policy <span class="math inline">\(\pi\)</span>. This is essentially the same as for state-values, only we now talk about state-action pairs being visited, i.e.&nbsp;taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> instead.</p>
<p>If <span class="math inline">\(\pi\)</span> is deterministic, then we will only estimate the values of actions that <span class="math inline">\(\pi\)</span> dictates. Therefore some exploration are needed in order to have estimates for all action-values. Two possibilities are:</p>
<ol type="1">
<li>Make <span class="math inline">\(\pi\)</span> stochastic, e.g.&nbsp;<span class="math inline">\(\varepsilon\)</span>-soft that that have non-zero probability of selecting each state-action pair.</li>
<li>Use <em>exploring starts</em>, which specifies that every state-action pair has a non-zero probability of being selected as the starting state of a sample-path.</li>
</ol>
</section>
</section>
<section id="mc-control-improvement" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="mc-control-improvement"><span class="header-section-number">9.4</span> MC control (improvement)</h2>
<p>We are now ready to formulate a generalized policy iteration (GPI) algorithm using MC to predict the action-values <span class="math inline">\(q(s,a)\)</span>. Policy improvement is done by selecting the next policy greedy with respect to the action-value function: <span class="math display">\[
    \pi(s) = \arg\max_a q(s, a).
\]</span> That is, we generate a sequence of policies and action-value functions <span class="math display">\[\pi_0 \xrightarrow[]{E} q_{\pi_0} \xrightarrow[]{I} \pi_1 \xrightarrow[]{E} q_{\pi_1} \xrightarrow[]{I} \pi_2 \xrightarrow[]{E} q_{\pi_2} \xrightarrow[]{I} \ldots \xrightarrow[]{I} \pi_* \xrightarrow[]{E} q_{*}.\]</span> Hence the policy improvement theorem applies for all <span class="math inline">\(s \in \mathcal{S}\)</span>:</p>
<p><span class="math display">\[\begin{align}
    q_{\pi_k}(s, a=\pi_{k+1}(s)) &amp;= q_{\pi_k}(s, \arg\max_a q_{\pi_k}(s, a)) \\
                    &amp;= \max_a q_{\pi_k}(s, a) \\
                    &amp;\geq q_{\pi_k}(s, \pi_k(s))\\
                    &amp;= v_{\pi_k}(s)
\end{align}\]</span></p>
<p>That is, <span class="math inline">\(\pi_{k+1}\)</span> is better than <span class="math inline">\(\pi_k\)</span> or optimal.</p>
<p>It is important to understand the major difference between model-based GPI (remember that a model means the transition probability matrix and reward distribution are known) and model-free GPI. We cannot simply use a 100% greedy strategy all the time, since all our action-values are estimates. As such, we now need to introduce an element of exploration into our algorithm to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy:</p>
<ol type="1">
<li><em>Infinite exploration</em>: all state-action <span class="math inline">\((s,a)\)</span> pairs should be explored infinitely many times as the number of iterations go to infinity (in the limit), i.e.&nbsp;as the number of iterations <span class="math inline">\(k\)</span> goes to infinity the number of visits <span class="math inline">\(n_k\)</span> does too <span class="math display">\[\lim_{k\rightarrow\infty} n_k(s, a) = \infty.\]</span></li>
<li><em>Greedy in the limit</em>: while we maintain infinite exploration, we do eventually need to converge to the optimal policy: <span class="math display">\[\lim_{k\rightarrow\infty} \pi_k(a|s) = 1 \text{ for } a = \arg\max_a q(s, a).\]</span></li>
</ol>
<section id="gpi-with-exploring-starts" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="gpi-with-exploring-starts"><span class="header-section-number">9.4.1</span> GPI with exploring starts</h3>
<p>An algorithm using exploring starts and first visit MC is given in <a href="#fig-mc-gpi-es-alg" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>. It satisfies the convergence properties and and incremental implementation can be used to update <span class="math inline">\(Q\)</span>. Note that to predict the action-values for a policy, we in general need a large number of sample-paths. However, much like we did with value iteration, we do not need to fully evaluate the value function for a given policy. Instead we can merely move the value toward the correct value and then switch to policy improvement thereafter. To stop the algorithm from having infinitely many sample-paths we may stop the algorithm once the <span class="math inline">\(q_{\pi_k}\)</span> stop moving within a certain error.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mc-gpi-es-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mc-gpi-es-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mc-gpi-es.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="628">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mc-gpi-es-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: GPI using MC policy prediction with exploring starts <span class="citation" data-cites="Sutton18">(<a href="references.html#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="gpi-using-epsilon-soft-policies" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="gpi-using-epsilon-soft-policies"><span class="header-section-number">9.4.2</span> GPI using <span class="math inline">\(\epsilon\)</span>-soft policies</h3>
<p>Note by using exploring starts in <a href="#fig-mc-gpi-es-alg" class="quarto-xref">Figure&nbsp;<span>9.2</span></a>, the ‘infinite exploration’ convergence assumption is satisfied. However exploring starts may be hard to use in practice. Another approach to ensure infinite exploration is to use a soft policy, i.e.&nbsp;assign a non-zero probability to each possible action in a state. An on-policy algorithm using <span class="math inline">\(\epsilon\)</span>-greedy policies is given in <a href="#fig-mc-gpi-on-policy-alg" class="quarto-xref">Figure&nbsp;<span>9.3</span></a>. Here we put probability <span class="math inline">\(1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}\)</span> on the maximal action and <span class="math inline">\(\frac{\varepsilon}{|\mathcal{A}(s)|}\)</span> on each of the others. Note using <span class="math inline">\(\epsilon\)</span>-greedy policy selection will improve the current policy; otherwise we have found best policy amongst the <span class="math inline">\(\epsilon\)</span>-soft policies. If we want to find the optimal policy we have to ensure the ‘greedy in the limit’ convergence assumption. This can be done by decreasing <span class="math inline">\(\epsilon\)</span> as the number of iterations increase (e.g.&nbsp;<span class="math inline">\(\epsilon = 1/k\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mc-gpi-on-policy-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mc-gpi-on-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mc-gpi-on-policy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="628">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mc-gpi-on-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: On-policy GPI using MC policy prediction <span class="citation" data-cites="Sutton18">(<a href="references.html#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>An incremental approach for updating <span class="math inline">\(Q\)</span> can be used by storing the number of times <span class="math inline">\(n_a\)</span>, action <span class="math inline">\(a\)</span> has been visited in state <span class="math inline">\(s\)</span> and then update <span class="math inline">\(Q(s,a)\)</span> using <span class="math display">\[Q_{n+1} = Q_n + \frac{1}{n_a}(G-Q_n),\]</span> where <span class="math inline">\(Q_n\)</span> denote the previous value.</p>
<p>Finally, the algorithm in <a href="#fig-mc-gpi-on-policy-alg" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> do not mention how to find the start state of an episode. In general all states that we want to approximate must be used as start state.</p>
</section>
<section id="gpi-using-upper-confience-bound-action-selection" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="gpi-using-upper-confience-bound-action-selection"><span class="header-section-number">9.4.3</span> GPI using upper-confience bound action selection</h3>
<p>GPI using exploring starts or <span class="math inline">\(\epsilon\)</span>-soft policies may be slow. Often speed-ups can be done by using e.g.&nbsp;upper-confidence bounds (UCB) for action selection. Recall from <a href="05_bandit.html" class="quarto-xref"><span>Chapter 5</span></a> that UCB select actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. That is, the next action <span class="math inline">\(a'\)</span> given a state <span class="math inline">\(s\)</span> is selected using: <span class="math display">\[
    a' = \arg\max_a \left(Q(s,a) + c\sqrt{\frac{\ln n_s}{n_a}}\right),
\]</span> where <span class="math inline">\(n_s\)</span> denote the number of times state <span class="math inline">\(s\)</span> has been visited and <span class="math inline">\(n_a\)</span> denote the number of times action <span class="math inline">\(a\)</span> has been visited (both numbers must be stored). The parameter <span class="math inline">\(c&gt;0\)</span> controls the degree of exploration. Higher <span class="math inline">\(c\)</span> results in more weight on the uncertainty. However, one problem with UCB is that it is hard to decide on which value of <span class="math inline">\(c\)</span> to use in advance.<br>
<!-- Preliminary testing showed that using UCB instead of $\epsilon$-greedy policies worked better.  --></p>
</section>
<section id="sec-mc-seasonal" class="level3" data-number="9.4.4">
<h3 data-number="9.4.4" class="anchored" data-anchor-id="sec-mc-seasonal"><span class="header-section-number">9.4.4</span> Example - Seasonal inventory and sales planning</h3>
<p>In the following example, we try to implement an algorithm that uses generalised policy iteration with every-visit estimation using epsilon-greedy action selection.</p>
<p>We consider seasonal product such as garden furnitures. Assume that the maximum inventory level is <span class="math inline">\(Q\)</span> items, i.e.&nbsp;we can buy at most <span class="math inline">\(Q\)</span> items at the start of the season for a price of $14. The product can be sold for at most <span class="math inline">\(T\)</span> weeks and at the end of the period (week <span class="math inline">\(T\)</span>), the remaining inventory is sold to an outlet store for $5 per item.</p>
<p>The demand depends on the sales price which based on historic observations is assumed in the interval <span class="math inline">\([10,25].\)</span> In general a higher sales price result in a lower demand. Moreover, in the first half part of the season the demand is on average 10% higher given a fixed sales price compared to the last half part of the season. Historic observed demands can be seen in <a href="#fig-demand" class="quarto-xref">Figure&nbsp;<span>9.4</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-demand" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demand-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="09_mc_files/figure-html/fig-demand-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demand-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Observed demands given price (scaled based on number of observations).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let <span class="math inline">\(s = (q,t)\)</span> denote the state of the system in the start of a week, where <span class="math inline">\(q\)</span> is the inventory and <span class="math inline">\(t\)</span> the week number. Then the state space is <span class="math display">\[\mathcal{S} = \{ s = (q,t) | 1 \leq q \leq Q, 1 \leq t \leq T \} \cup \{ 0 \},\]</span> where state <span class="math inline">\(s = 0\)</span> denote the terminal state (inventory empty). Let us limit us to actions <span class="math display">\[\mathcal{A}(q,t) = \{ 10,15,20,25 \}, \mathcal{A}(0) = \{ d \}, \]</span> where action <span class="math inline">\(a\)</span> denote the price and <span class="math inline">\(d\)</span> denote the dummy action with deterministic transition to state <span class="math inline">\(0\)</span>.</p>
<p>The inventory dynamics for transitions not to the terminal state are <span class="math display">\[t' = t + 1,\]</span> <span class="math display">\[q' = q - min(q, D),\]</span> where <span class="math inline">\(D\)</span> denote the demand. Moreover, if <span class="math inline">\(t = T\)</span> or <span class="math inline">\(q' = 0\)</span>, then a transition to the terminal state happens.</p>
<p>For <span class="math inline">\(t=1\)</span> the reward of an state <span class="math inline">\((q,t)\)</span> is sales price times the number of sold items minus the purchase cost. For <span class="math inline">\(1&lt;t&lt;T\)</span> the reward is sales price times the number of sold (we assume an inventory cost of zero), while for <span class="math inline">\(t=T\)</span> the reward is the scrap price times the inventory.</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>During the lecture for example, we will work with the <a href="https://colab.research.google.com/drive/1I4gBqDqYQAEPOVlMqTyBG1AKSHTgyDm-?usp=sharing">notebook</a>. We implement the MC algorithms. Have a look at the notebook and try to understand the content.</p>
</div>
</div>
</section>
</section>
<section id="sec-mc-off-policy" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="sec-mc-off-policy"><span class="header-section-number">9.5</span> Off-policy MC prediction</h2>
<p>Until now we have only considered what is denoted <em>on-policy</em> algorithms for finding the optimal policy. Here we both evaluate or improve the policy that is used to make decisions. To ensure infinite exploration we use for instance exploring starts or <span class="math inline">\(\epsilon\)</span>-soft policies. <em>Off-policy</em> methods use a different approach by considering two policies: a policy <span class="math inline">\(b\)</span> used to generate the sample-path (behaviour policy) and a policy <span class="math inline">\(\pi\)</span> that is learned for control (target policy). We update the target policy using the sample-paths from the behaviour policy. The behaviour policy explores the environment for us during training and must ensure infinite exploration. Moreover, the <em>coverage</em> assumption must be satisfied: <span class="math display">\[\pi(a|s) &gt; 0 \rightarrow b(a|s) &gt; 0\]</span> That is, every action in <span class="math inline">\(\pi\)</span> must also be taken, at least occasionally, by <span class="math inline">\(b\)</span>. Put differently, to learn <span class="math inline">\(\pi\)</span> we must sample paths that occur when using <span class="math inline">\(\pi\)</span>. Note target policy <span class="math inline">\(\pi\)</span> may be deterministic by using greedy selection with respect to action-value estimates (greedy in the limit satisfied).</p>
<p>Off-policy learning methods are powerful and more general than on-policy methods (on-policy methods being a special case of off-policy where target and behaviour policies are the same). They can be used to learn from data generated by a conventional non-learning controller or from a human expert.</p>
<p>But how do we estimate the expected return using the target policy when we only have sample-paths from the behaviour policy? For this we need to introduce <em>importance sampling</em>, a general technique for estimating expected values under one distribution given samples from another. Let us first explain it using two distributions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> where we want to estimated the mean of <span class="math inline">\(a\)</span> given data/samples from <span class="math inline">\(b\)</span>, then <span class="math display">\[
\begin{align}
  \mathbb{E}_{a}[X] &amp;= \sum_{x\in X} a(x)x \\
  &amp;= \sum_{x\in X} a(x)\frac{b(x)}{b(x)}x \\
  &amp;= \sum_{x\in X} b(x)\frac{a(x)}{b(x)}x \\
  &amp;= \sum_{x\in X} b(x)\rho(x)x \\
  &amp;= \mathbb{E}_{b}\left[\rho(X)X\right].
\end{align}
\]</span> Hence to the mean of <span class="math inline">\(a\)</span> can be found by finding the mean of <span class="math inline">\(\rho(X)X\)</span> where <span class="math inline">\(X\)</span> is has a <span class="math inline">\(b\)</span> distribution and <span class="math inline">\(\rho(x) = a(x)/b(x)\)</span> denote the <em>importance sampling ratio</em>. Note given samples <span class="math inline">\((x_1,\ldots,x_n)\)</span> from <span class="math inline">\(b\)</span> we then can calculate the sample average using <span id="eq-is-approx"><span class="math display">\[
\begin{align}
  \mathbb{E}_{a}[X] &amp;= \mathbb{E}_{b}\left[\rho(X)X\right] \\
  &amp;\approx \frac{1}{n}\sum_{i = 1}^n \rho(x_i)x_i \\
\end{align}
\tag{9.1}\]</span></span></p>
<!-- Example with two distributions -->
<p>Now let us use importance sampling on the target policy <span class="math inline">\(\pi\)</span> and behaviour policy <span class="math inline">\(b\)</span>. Given state <span class="math inline">\(S_t\)</span> and sample path, we want to find <span class="math display">\[v_\pi(s) = \mathbb{E}_{\pi}[G_t|S_t = s] = \mathbb{E}_{b}[\rho(G_t)G_t|S_t = s],\]</span> or since we base our estimates on sample-paths, we are in fact interested in estimating the action-values <span class="math display">\[q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] = \mathbb{E}_{b}[\rho(G_t)G_t|S_t = s, A_t = a].\]</span> For this we need the importance sampling ratio given a certain sample-path <span class="math inline">\(S_t, A_t, R_{t+1}, \ldots, R_T, S_T\)</span> with return <span class="math inline">\(G_t\)</span>: <span id="eq-isr"><span class="math display">\[
\begin{align}
    \rho(G_t) &amp;= \frac{\Pr{}(S_t, A_t, \dots S_T| S_t = s, A_t = a, \pi)}{\Pr{}(S_t, A_t, \dots, S_T)| S_t = s, A_t = a, b)} \\
                 &amp;= \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)\Pr{}(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)\Pr{}(S_{k+1}|S_k, A_k)}\\
                 &amp;=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}.
\end{align}
\tag{9.2}\]</span></span> Note the transition probabilities cancel out, i.e.&nbsp;the ratio does not depend on the MDP dynamics by only the policies. Moreover, importance sampling ratios are only non-zero for sample-paths where the target-policy has non-zero probability of acting exactly like the behaviour policy <span class="math inline">\(b\)</span>. So, if the behaviour policy takes 10 steps in an sample-path, each of these 10 steps have to have been possible by the target policy, else <span class="math inline">\(\pi(a|s) = 0\)</span> and <span class="math inline">\(\rho_{t:T-1} = 0\)</span>.</p>
<p>We can now approx. <span class="math inline">\(q_\pi(s,a)\)</span> by rewriting <a href="#eq-is-approx" class="quarto-xref">Equation&nbsp;<span>9.1</span></a> for <span class="math inline">\(\pi\)</span> given returns from <span class="math inline">\(b\)</span> to <span id="eq-ois"><span class="math display">\[
    q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] \approx \frac{1}{n} \sum_{i = 1}^n \rho_iG_i,
\tag{9.3}\]</span></span> where we assume that given the sample-paths (episodes), have <span class="math inline">\(n\)</span> observations of the return <span class="math inline">\((G_1, \ldots, G_n)\)</span> in state <span class="math inline">\(s\)</span> taking action <span class="math inline">\(a\)</span> with the importance sampling ratio <span class="math inline">\(\rho_i\)</span> calculated using Eq. <a href="#eq-isr" class="quarto-xref">Equation&nbsp;<span>9.2</span></a>. As a result if we consider the prediction algorithm in <a href="#fig-mc-prediction-alg" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> it must be modified by:</p>
<ul>
<li>Generate an sample-path using policy <span class="math inline">\(b\)</span> instead of <span class="math inline">\(\pi\)</span>.</li>
<li>Add a variable W representing the importance sampling ratio which must be set to 1 on line containing <span class="math inline">\(G \leftarrow 0\)</span>.</li>
<li>Modify line <span class="math inline">\(G \leftarrow \gamma G + R_{t+1}\)</span> to <span class="math inline">\(G \leftarrow \gamma WG + R_{t+1}\)</span> since we now need to multiply with the importance sampling ratio.</li>
<li>Add a line after the last with <span class="math inline">\(W \leftarrow W \pi(A_t|S_t)/b(A_t|S_t)\)</span>, i.e.&nbsp;we update the importance sampling ratio.</li>
<li>Note if <span class="math inline">\(\pi(A_t|S_t) = 0\)</span> then we may stop the inner loop earlier (<span class="math inline">\(W=0\)</span> for the remaining <span class="math inline">\(t\)</span>).</li>
<li>Finally, an incremental update of <span class="math inline">\(V\)</span> can be done having a vector counting the number of of returns found for each state. Then the incremental update is <span id="eq-upd"><span class="math display">\[
V(s) \leftarrow V(s) + \frac{1}{n} \left[WG - V(s)\right].
\tag{9.4}\]</span></span> where <span class="math inline">\(n\)</span> denote the number of realized returns found for state <span class="math inline">\(s\)</span> and <span class="math inline">\(G\)</span> the current realized return.</li>
</ul>
<section id="weighted-importance-sampling" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="weighted-importance-sampling"><span class="header-section-number">9.5.1</span> Weighted importance sampling</h3>
<p>When using a sample average the importance sampling method is called <em>ordinary importance sampling</em>. Ordinary importance sampling may result in a high variance which is not good. As a result we may use other weights and instead of Eq. <a href="#eq-ois" class="quarto-xref">Equation&nbsp;<span>9.3</span></a> use the estimate (<em>weighted importance sampling</em>): <span class="math display">\[
    q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] \approx \frac{1}{\sum_{i = 1}^n \rho_i} \sum_{i = 1}^n \rho_iG_i.
\]</span> An incremental update then becomes:</p>
<p><span id="eq-wpd"><span class="math display">\[
\begin{align}
    q_\pi(s,a) &amp;\approx V_{n+1} \\
    &amp;= \frac{1}{\sum_{i = 1}^n \rho_i} \sum_{i = 1}^n \rho_iG_i \\
    &amp;= \frac{1}{C_n} \sum_{i = 1}^n W_iG_i \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n-1}\frac{1}{C_{n-1}} \sum_{i = 1}^{n-1} W_iG_i) \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n-1}V_n) \\
    &amp;= \frac{1}{C_n} (W_nG_n + (C_{n} - W_{n}) V_n) \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n}V_n - W_{n} V_n) \\
    &amp;= V_n + \frac{W_n}{C_n} (G_n  - V_n),
\end{align}
\tag{9.5}\]</span></span> where <span class="math inline">\(C_n = \sum_{i = 1}^n \rho_i\)</span> is the sum of the ratios and and <span class="math inline">\(W_n\)</span> the ratio for the n’th return. Using weighted importance sampling gives a smaller variance and hence faster convergence. An off-policy prediction algorithm using weighted importance sampling and incremental updates is given in <a href="#fig-mc-pred-off-policy-alg" class="quarto-xref">Figure&nbsp;<span>9.5</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mc-pred-off-policy-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mc-pred-off-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mc-off-policy-prediction.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="630">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mc-pred-off-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: Off-policy MC prediction <span class="citation" data-cites="Sutton18">(<a href="references.html#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note both <a href="#eq-upd" class="quarto-xref">Equation&nbsp;<span>9.4</span></a> and <a href="#eq-wpd" class="quarto-xref">Equation&nbsp;<span>9.5</span></a> follows the general incremental formula: <span class="math display">\[\begin{equation}
New Estimate \leftarrow Old Estimate + Step Size \left[Observation - Old Estimate \right].
\end{equation}\]</span> For ordinary importance sampling the step-size is <span class="math inline">\(1/n\)</span> and for weighted importance sampling the step-size is <span class="math inline">\(W_n/C_n\)</span>.</p>
</section>
</section>
<section id="off-policy-control-improvement" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="off-policy-control-improvement"><span class="header-section-number">9.6</span> Off-policy control (improvement)</h2>
<p>Having discussed a framework for off-policy MC prediction, we can now give a GPI algorithm for off-policy MC control that estimate <span class="math inline">\(\pi_*\)</span> and <span class="math inline">\(q_*\)</span> by using rewards obtained through behaviour policy <span class="math inline">\(b\)</span>. We will focus on using weighted importance sampling with incremental updates. The algorithm is given in <a href="#fig-mc-gpi-off-policy-alg" class="quarto-xref">Figure&nbsp;<span>9.6</span></a>. The target policy <span class="math inline">\(\pi\)</span> is the greedy policy with respect to <span class="math inline">\(Q\)</span>, which is an estimate of <span class="math inline">\(q_\pi\)</span>. This algorithm converges to <span class="math inline">\(q_\pi\)</span> as long as an infinite number of returns are observed for each state-action pair. This can be achieved by making <span class="math inline">\(b\)</span> <span class="math inline">\(\varepsilon\)</span>-soft. The policy <span class="math inline">\(\pi\)</span> converges to <span class="math inline">\(\pi_*\)</span> at all encountered states even if <span class="math inline">\(b\)</span> changes (to another <span class="math inline">\(\varepsilon\)</span>-soft policy) between or within sample-paths. Note we exit the inner loop if <span class="math inline">\(A_t \neq \pi(S_t)\)</span> which implies <span class="math inline">\(W=0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-mc-gpi-off-policy-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mc-gpi-off-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/mc-off-policy-gpi.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="629">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mc-gpi-off-policy-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: Off-policy GPI <span class="citation" data-cites="Sutton18">(<a href="references.html#ref-Sutton18" role="doc-biblioref">Sutton and Barto 2018</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that this policy only learns from sample-paths in which <span class="math inline">\(b\)</span> selects only greedy actions after some timestep. This can greatly slow learning.</p>
</section>
<section id="summary" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="summary"><span class="header-section-number">9.7</span> Summary</h2>
<p>Read Chapter 5.10 in <span class="citation" data-cites="Sutton18">Sutton and Barto (<a href="references.html#ref-Sutton18" role="doc-biblioref">2018</a>)</span>.</p>
</section>
<section id="exercises" class="level2" data-number="9.8">
<h2 data-number="9.8" class="anchored" data-anchor-id="exercises"><span class="header-section-number">9.8</span> Exercises</h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="99_2_appdx.html">help page</a>. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<section id="sec-mc-seasonal-ex" class="level3" data-number="9.8.1">
<h3 data-number="9.8.1" class="anchored" data-anchor-id="sec-mc-seasonal-ex"><span class="header-section-number">9.8.1</span> Exercise - Seasonal inventory and sales planning</h3>
<p>Consider the seasonal product in <a href="#sec-mc-seasonal" class="quarto-xref">Example&nbsp;<span>9.4.4</span></a> and <a href="https://colab.research.google.com/drive/1I4gBqDqYQAEPOVlMqTyBG1AKSHTgyDm-#scrollTo=1BzUCPQxstvQ&amp;line=3&amp;uniqifier=1">this section</a> in the Colab notebook to see the questions. Use your own copy if you already have one.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Sutton18" class="csl-entry" role="listitem">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./08_dp.html" class="pagination-link" aria-label="Dynamic programming">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic programming</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./10_td-pred.html" class="pagination-link" aria-label="Temporal difference methods for prediction">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Temporal difference methods for prediction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bss-osca/rl/edit/master/book/09_mc.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bss-osca/rl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/bss-osca/rl/blob/master/book/09_mc.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div></div></footer><script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>