<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-10-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />

<link rel="next" href="mod-rl-intro.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.23/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the course notes</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#purpose-of-the-course"><i class="fa fa-check"></i>Purpose of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-goals-of-the-course"><i class="fa fa-check"></i>Learning goals of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reinforcement-learning-textbook"><i class="fa fa-check"></i>Reinforcement learning textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-organization"><i class="fa fa-check"></i>Course organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programming-software"><i class="fa fa-check"></i>Programming software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ack"><i class="fa fa-check"></i>Acknowledgements and license</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex"><i class="fa fa-check"></i>Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-annotate"><i class="fa fa-check"></i>Exercise - How to annotate</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-templates"><i class="fa fa-check"></i>Exercise - Templates</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL</b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings</a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning</a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics</a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines</a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning</a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream</a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies</a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration</a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play</a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay</a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#summary"><i class="fa fa-check"></i><b>1.11</b> Summary</a></li>
<li class="chapter" data-level="1.12" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.12</b> Exercises</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.12.1</b> Exercise - Self-Play</a></li>
<li class="chapter" data-level="1.12.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-sym"><i class="fa fa-check"></i><b>1.12.2</b> Exercise - Symmetries</a></li>
<li class="chapter" data-level="1.12.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-greedy"><i class="fa fa-check"></i><b>1.12.3</b> Exercise - Greedy Play</a></li>
<li class="chapter" data-level="1.12.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-exploit"><i class="fa fa-check"></i><b>1.12.4</b> Exercise - Learning from Exploration</a></li>
<li class="chapter" data-level="1.12.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-other"><i class="fa fa-check"></i><b>1.12.5</b> Exercise - Other Improvements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods</b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings</a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem</a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action</a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-step-size"><i class="fa fa-check"></i><b>2.5</b> The role of the step-size</a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.6</b> Optimistic initial values</a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#upper-confidence-bound-action-selection"><i class="fa fa-check"></i><b>2.7</b> Upper-Confidence Bound Action Selection</a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
<li class="chapter" data-level="2.9" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-adv"><i class="fa fa-check"></i><b>2.9.1</b> Exercise - Advertising</a></li>
<li class="chapter" data-level="2.9.2" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-coin"><i class="fa fa-check"></i><b>2.9.2</b> Exercise - A coin game</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html"><i class="fa fa-check"></i><b>3</b> Markov decision processes (MDPs)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#learning-outcomes-2"><i class="fa fa-check"></i><b>3.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#textbook-readings-2"><i class="fa fa-check"></i><b>3.2</b> Textbook readings</a></li>
<li class="chapter" data-level="3.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#an-mdp-as-a-model-for-the-agent-environment"><i class="fa fa-check"></i><b>3.3</b> An MDP as a model for the agent-environment</a></li>
<li class="chapter" data-level="3.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#rewards-and-the-objective-function-goal"><i class="fa fa-check"></i><b>3.4</b> Rewards and the objective function (goal)</a></li>
<li class="chapter" data-level="3.5" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#summary-2"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#sec-mdp-1-ex"><i class="fa fa-check"></i><b>3.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-seq"><i class="fa fa-check"></i><b>3.6.1</b> Exercise - Sequential decision problems</a></li>
<li class="chapter" data-level="3.6.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-exp-return"><i class="fa fa-check"></i><b>3.6.2</b> Exercise - Expected return</a></li>
<li class="chapter" data-level="3.6.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-gambler"><i class="fa fa-check"></i><b>3.6.3</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="3.6.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-storage"><i class="fa fa-check"></i><b>3.6.4</b> Exercise - Factory storage</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html"><i class="fa fa-check"></i><b>4</b> Policies and value functions for MDPs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#learning-outcomes-3"><i class="fa fa-check"></i><b>4.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="4.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#textbook-readings-3"><i class="fa fa-check"></i><b>4.2</b> Textbook readings</a></li>
<li class="chapter" data-level="4.3" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#policies-and-value-functions"><i class="fa fa-check"></i><b>4.3</b> Policies and value functions</a></li>
<li class="chapter" data-level="4.4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-opt"><i class="fa fa-check"></i><b>4.4</b> Optimal policies and value functions</a></li>
<li class="chapter" data-level="4.5" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#optimality-vs-approximation"><i class="fa fa-check"></i><b>4.5</b> Optimality vs approximation</a></li>
<li class="chapter" data-level="4.6" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#semi-mdps-non-fixed-time-length"><i class="fa fa-check"></i><b>4.6</b> Semi-MDPs (non-fixed time length)</a></li>
<li class="chapter" data-level="4.7" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#summary-3"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
<li class="chapter" data-level="4.8" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-2-ex"><i class="fa fa-check"></i><b>4.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-policy"><i class="fa fa-check"></i><b>4.8.1</b> Exercise - Optimal policy</a></li>
<li class="chapter" data-level="4.8.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-car"><i class="fa fa-check"></i><b>4.8.2</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mod-dp.html"><a href="mod-dp.html"><i class="fa fa-check"></i><b>5</b> Dynamic programming</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mod-dp.html"><a href="mod-dp.html#learning-outcomes-4"><i class="fa fa-check"></i><b>5.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="5.2" data-path="mod-dp.html"><a href="mod-dp.html#textbook-readings-4"><i class="fa fa-check"></i><b>5.2</b> Textbook readings</a></li>
<li class="chapter" data-level="5.3" data-path="mod-dp.html"><a href="mod-dp.html#sec-dp-pe"><i class="fa fa-check"></i><b>5.3</b> Policy evaluation</a></li>
<li class="chapter" data-level="5.4" data-path="mod-dp.html"><a href="mod-dp.html#policy-improvement"><i class="fa fa-check"></i><b>5.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="5.5" data-path="mod-dp.html"><a href="mod-dp.html#policy-iteration"><i class="fa fa-check"></i><b>5.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="5.6" data-path="mod-dp.html"><a href="mod-dp.html#value-iteration"><i class="fa fa-check"></i><b>5.6</b> Value Iteration</a></li>
<li class="chapter" data-level="5.7" data-path="mod-dp.html"><a href="mod-dp.html#generalized-policy-iteration"><i class="fa fa-check"></i><b>5.7</b> Generalized policy iteration</a></li>
<li class="chapter" data-level="5.8" data-path="mod-dp.html"><a href="mod-dp.html#example---factory-storage"><i class="fa fa-check"></i><b>5.8</b> Example - Factory Storage</a></li>
<li class="chapter" data-level="5.9" data-path="mod-dp.html"><a href="mod-dp.html#summary-4"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
<li class="chapter" data-level="5.10" data-path="mod-dp.html"><a href="mod-dp.html#exercises"><i class="fa fa-check"></i><b>5.10</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-gambler"><i class="fa fa-check"></i><b>5.10.1</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="5.10.2" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-maintain"><i class="fa fa-check"></i><b>5.10.2</b> Exercise - Maintenance problem</a></li>
<li class="chapter" data-level="5.10.3" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-rental"><i class="fa fa-check"></i><b>5.10.3</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mod-mc.html"><a href="mod-mc.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo methods for prediction and control</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mod-mc.html"><a href="mod-mc.html#learning-outcomes-5"><i class="fa fa-check"></i><b>6.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="mod-mc.html"><a href="mod-mc.html#textbook-readings-5"><i class="fa fa-check"></i><b>6.2</b> Textbook readings</a></li>
<li class="chapter" data-level="6.3" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-evaluation"><i class="fa fa-check"></i><b>6.3</b> MC prediction (evaluation)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-of-action-values"><i class="fa fa-check"></i><b>6.3.1</b> MC prediction of action-values</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="mod-mc.html"><a href="mod-mc.html#mc-control-improvement"><i class="fa fa-check"></i><b>6.4</b> MC control (improvement)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mod-mc.html"><a href="mod-mc.html#gpi-with-exploring-starts"><i class="fa fa-check"></i><b>6.4.1</b> GPI with exploring starts</a></li>
<li class="chapter" data-level="6.4.2" data-path="mod-mc.html"><a href="mod-mc.html#gpi-using-epsilon-soft-policies"><i class="fa fa-check"></i><b>6.4.2</b> GPI using <span class="math inline">\(\epsilon\)</span>-soft policies</a></li>
<li class="chapter" data-level="6.4.3" data-path="mod-mc.html"><a href="mod-mc.html#gpi-using-upper-confience-bound-action-selection"><i class="fa fa-check"></i><b>6.4.3</b> GPI using upper-confience bound action selection</a></li>
<li class="chapter" data-level="6.4.4" data-path="mod-mc.html"><a href="mod-mc.html#mc-seasonal"><i class="fa fa-check"></i><b>6.4.4</b> Example - Seasonal inventory and sales planning</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mod-mc.html"><a href="mod-mc.html#sec-mc-off-policy"><i class="fa fa-check"></i><b>6.5</b> Off-policy MC prediction</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="mod-mc.html"><a href="mod-mc.html#weighted-importance-sampling"><i class="fa fa-check"></i><b>6.5.1</b> Weighted importance sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="mod-mc.html"><a href="mod-mc.html#off-policy-control-improvement"><i class="fa fa-check"></i><b>6.6</b> Off-policy control (improvement)</a></li>
<li class="chapter" data-level="6.7" data-path="mod-mc.html"><a href="mod-mc.html#summary-5"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
<li class="chapter" data-level="6.8" data-path="mod-mc.html"><a href="mod-mc.html#exercises-1"><i class="fa fa-check"></i><b>6.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="mod-mc.html"><a href="mod-mc.html#ex-mc-seasonal"><i class="fa fa-check"></i><b>6.8.1</b> Exercise - Seasonal inventory and sales planning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mod-td-pred.html"><a href="mod-td-pred.html"><i class="fa fa-check"></i><b>7</b> Temporal difference methods for prediction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#learning-outcomes-6"><i class="fa fa-check"></i><b>7.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="7.2" data-path="mod-td-pred.html"><a href="mod-td-pred.html#textbook-readings-6"><i class="fa fa-check"></i><b>7.2</b> Textbook readings</a></li>
<li class="chapter" data-level="7.3" data-path="mod-td-pred.html"><a href="mod-td-pred.html#what-is-td-learning"><i class="fa fa-check"></i><b>7.3</b> What is TD learning?</a></li>
<li class="chapter" data-level="7.4" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction"><i class="fa fa-check"></i><b>7.4</b> TD prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction-for-action-values"><i class="fa fa-check"></i><b>7.4.1</b> TD prediction for action-values</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mod-td-pred.html"><a href="mod-td-pred.html#benefits-of-td-methods"><i class="fa fa-check"></i><b>7.5</b> Benefits of TD methods</a></li>
<li class="chapter" data-level="7.6" data-path="mod-td-pred.html"><a href="mod-td-pred.html#exercises-2"><i class="fa fa-check"></i><b>7.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#ex-td-pred-random"><i class="fa fa-check"></i><b>7.6.1</b> Exercise - A randow walk</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mod-td-control.html"><a href="mod-td-control.html"><i class="fa fa-check"></i><b>8</b> Temporal difference methods for control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mod-td-control.html"><a href="mod-td-control.html#learning-outcomes-7"><i class="fa fa-check"></i><b>8.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.2" data-path="mod-td-control.html"><a href="mod-td-control.html#textbook-readings-7"><i class="fa fa-check"></i><b>8.2</b> Textbook readings</a></li>
<li class="chapter" data-level="8.3" data-path="mod-td-control.html"><a href="mod-td-control.html#sarsa---on-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.3</b> SARSA - On-policy GPI using TD</a></li>
<li class="chapter" data-level="8.4" data-path="mod-td-control.html"><a href="mod-td-control.html#q-learning---off-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.4</b> Q-learning - Off-policy GPI using TD</a></li>
<li class="chapter" data-level="8.5" data-path="mod-td-control.html"><a href="mod-td-control.html#expected-sarsa---gpi-using-td"><i class="fa fa-check"></i><b>8.5</b> Expected SARSA - GPI using TD</a></li>
<li class="chapter" data-level="8.6" data-path="mod-td-control.html"><a href="mod-td-control.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="mod-td-control.html"><a href="mod-td-control.html#exercises-3"><i class="fa fa-check"></i><b>8.7</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R</a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups</a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention</a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes</a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help</a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals</a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon</a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Reinforcement Learning for Business (RL)</h1>
<h2 class="subtitle"><em>Course notes</em></h2>
<p class="author"><em>Lars Relund Nielsen</em></p>
<p class="date"><em>2022-10-31</em></p>
</div>
<div id="mod-intro" class="section level1 unnumbered hasAnchor">
<h1>About the course notes<a href="index.html#mod-intro" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This site contains course notes for the course “Reinforcement Learning for Business” held at <a href="https://bss.au.dk/en/">Aarhus BSS</a>. It consists of a set of learning modules. The course is an elective course mainly for the <a href="https://kandidat.au.dk/en/operationsandsupplychainanalytics/">Operations and Supply Chain Analytics</a> and <a href="https://kandidat.au.dk/en/businessintelligence/">Business Intelligence</a> programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated.</p>
<div id="learning-outcomes" class="section level2 unnumbered hasAnchor">
<h2>Learning outcomes<a href="index.html#learning-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<ul>
<li>Understand the prerequisites and the goals for the course.</li>
<li>Have downloaded the textbook.</li>
<li>Know how the course is organized.</li>
<li>Installed R and RStudio.</li>
<li>Annotated the online notes.</li>
</ul>
<p>The learning outcomes relate to the <a href="mod-lg-course.html#mod-lg-course">overall learning goals</a> number 3, 5 and 6 of the course.</p>
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="purpose-of-the-course" class="section level2 unnumbered hasAnchor">
<h2>Purpose of the course<a href="index.html#purpose-of-the-course" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The purpose of this course is to give an introduction and knowledge about reinforcement learning (RL).</p>
<p>RL may be seen as</p>
<ul>
<li>An approach of modelling sequential decision making problems.</li>
<li>An approach for learning good decision making under uncertainty from experience.</li>
<li>Mathematical models for learning-based decision making.</li>
<li>Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.</li>
<li>Estimating and finding near optimal decisions of a stochastic process with sequential decision making.</li>
<li>A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.</li>
</ul>
<p>RL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions.</p>
<p>A classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge.</p>
<p>The course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control.</p>
</div>
<div id="learning-goals-of-the-course" class="section level2 unnumbered hasAnchor">
<h2>Learning goals of the course<a href="index.html#learning-goals-of-the-course" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After having participated in the course, you must, in addition to achieving general academic skills, demonstrate:</p>
<p>Knowledge of</p>
<ol style="list-style-type: decimal">
<li>RL for Bandit problems</li>
<li>Markov decision processes and ways to optimize them</li>
<li>the exploration vs exploitation challenge in RL and approaches for addressing this challenge</li>
<li>the role of policy evaluation with stochastic approximation in the context of RL</li>
</ol>
<p>Skills to</p>
<ol start="5" style="list-style-type: decimal">
<li>define the key features of RL that distinguishes it from other machine learning techniques</li>
<li>discuss fundamental concepts in RL</li>
<li>describe the mathematical framework of Markov decision processes</li>
<li>formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives</li>
<li>apply fundamental techniques, results and concepts of RL on selected RL problems.</li>
<li>given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model)</li>
</ol>
<p>Competences to</p>
<ol start="11" style="list-style-type: decimal">
<li>identify areas where RL are valuable</li>
<li>select and apply the appropriate RL model for a given business problem</li>
<li>interpret and communicate the results from RL</li>
</ol>
</div>
<div id="reinforcement-learning-textbook" class="section level2 unnumbered hasAnchor">
<h2>Reinforcement learning textbook<a href="index.html#reinforcement-learning-textbook" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The course uses the free textbook <em>Reinforcement Learning: An Introduction</em> by <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course.</p>
<p>Sutton and Barto’s book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it’s totally free. Just follow the citation link to download it.</p>
</div>
<div id="course-organization" class="section level2 unnumbered hasAnchor">
<h2>Course organization<a href="index.html#course-organization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Each week considers a learning module. A learning module is related to a chapter in the textbook. The learning path in a typical week are</p>
<ul>
<li>Before lectures: Read the chapter in the textbook and consider the extra module material.</li>
<li>Lectures (at campus).</li>
<li>After lectures: Module Exercises (in groups).</li>
</ul>
<p>Lectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials are given and we focus on a specific RL problem.</p>
<p>This module gives a short introduction to the course. Next, the site consists of different parts each containing teaching modules about specific topics:</p>
<ul>
<li><p><a href="mod-rl-intro.html#mod-rl-intro">Part I</a> gives you a general introduction to RL and the bandit problem.</p></li>
<li><p><a href="mod-bandit.html#mod-bandit">Part II</a> consider RL sequential decision problems where the state and state and action spaces are small enough so values can be represented as arrays, or tables. We start by considering bandit problems (Module <a href="mod-bandit.html#mod-bandit">2</a>) a RL problem in which there is only a single state. Next, Markov decision processes (the full model known) are considered as a general modelling framework (Module <a href="mod-mdp-1.html#mod-mdp-1">3</a>) and the concept of policies and value functions are discussed (Module <a href="mod-mdp-2.html#mod-mdp-2">4</a>). Model-based algorithms for finding the optimal policy (dynamic programming) are given in Module <a href="mod-dp.html#mod-dp">5</a>. The next modules consider model-free methods for finding the optimal policy, i.e. methods that do not require full knowledge of the transition probabilities and rewards of the process. Monte Carlo sampling methods are presented in Module <a href="mod-mc.html#mod-mc">6</a> and …</p></li>
</ul>
<!-- The next three chapters describe three fundamental classes of methods for solving finite -->
<!-- Markov decision problems: dynamic programming, Monte Carlo methods, and temporaldi↵ -->
<!-- erence learning. Each class of methods has its strengths and weaknesses. Dynamic -->
<!-- programming methods are well developed mathematically, but require a complete and -->
<!-- accurate model of the environment. Monte Carlo methods don’t require a model and are -->
<!-- conceptually simple, but are not well suited for step-by-step incremental computation. -->
<!-- Finally, temporal-di↵erence methods require no model and are fully incremental, but are -->
<!-- more complex to analyze. The methods also di↵er in several ways with respect to their -->
<!-- efficiency and speed of convergence. -->
<!-- The remaining two chapters describe how these three classes of methods can be -->
<!-- combined to obtain the best features of each of them. In one chapter we describe how -->
<!-- the strengths of Monte Carlo methods can be combined with the strengths of temporaldi↵ -->
<!-- erence methods via multi-step bootstrapping methods. In the final chapter of this part -->
<!-- of the book we show how temporal-di↵erence learning methods can be combined with -->
<!-- model learning and planning methods (such as dynamic programming) for a complete -->
<!-- and unified solution to the tabular reinforcement learning problem. -->
<p><!-- * [Part III](#mod-vba-random-numbers) contains extra modules useful when coding in VBA.  -->
<!-- * [Part IV](#mod-vba-project) presents the mandatory VBA project. -->
<!-- * [Part V](#mod-r-install) focuses on helping you install the needed programs on your computer (Module \@ref(mod-r-install)) and gives you a short introduction to R (Module \@ref(mod-r-workflow)). -->
<!-- * [Part VI](#mod-r-loops-cond) gives you an overview over programming in R including loops and conditionals (Module \@ref(mod-r-loops-cond)) and functions (Module \@ref(mod-r-functions)). --></p>
<p><!-- * [Part VII](#mod-r-tidyverse-intro) the focus is on import/export, tidy and transformation of data. Module \@ref(mod-r-tidyverse-intro) first gives you an introduction to the [tidyverse][tidyverse-main-page] packages and introduces you to literal programming using R Markdown. Next, Module \@ref(mod-r-io) shows you how to export and import data. Finally, Module \@ref(mod-r-transform) focuses on transformation of data. -->
<!-- * [Part VIII](#mod-r-plot) considers visualization of data in R (Module \@ref(mod-r-plot)). -->
<!-- * [Part IX](#mod-project) presents the mandatory R project. -->
<!-- * [Part X](#mod-r-maps) contains extra modules useful when coding in R.  --></p>
<p>The appendix contains different modules that may be helpful for you including hints on <a href="groups.html#groups">how to work in groups</a>, <a href="help.html#help">how to get help</a> if you are stuck and <a href="annotate.html#annotate">how to annotate</a> the course notes.</p>
</div>
<div id="programming-software" class="section level2 unnumbered hasAnchor">
<h2>Programming software<a href="index.html#programming-software" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use <a href="https://www.r-project.org">R</a> as programming software and it is assumed that you are familiar with using R. <a href="https://www.r-project.org">R</a> is a programming language and free software environment. R can be run from a terminal but in general you use an IDE (integrated development environment) <a href="https://rstudio.com">RStudio</a> for running R and to saving your work. R and RStudio can either be run from your laptop or using <a href="https://rstudio.cloud/spaces/176810/join?access_code=LSGnG2EXTuzSyeYaNXJE77vP33DZUoeMbC0xhfCz">RStudio Cloud</a> which run R in the cloud using your browser.</p>
<p>It is assumed as a prerequisite that you know how to use R. If you need a brush-up on your R programming skills then have a look at Module <a href="mod-r-setup.html#mod-r-setup">A</a> in the appendix.</p>
</div>
<div id="ack" class="section level2 unnumbered hasAnchor">
<h2>Acknowledgements and license<a href="index.html#ack" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Materials are taken from various places:</p>
<ul>
<li>The notes are based on <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>.</li>
<li>The bookdown skeleton and some notes are based on the <a href="https://bss-osca.github.io/tfa/">Tools for Analytics</a> course.</li>
<li>Some notes are adopted from <a href="https://github.com/enjeeneer/sutton_and_barto">Scott Jeen</a>,
<a href="https://github.com/brynhayder/reinforcement_learning_an_introduction">Bryn Elesedy</a> and <a href="https://github.com/peter-can-write/david-silver-rl-notes">Peter Goldsborough</a>.</li>
<li>Some slides are inspired by the <a href="https://www.coursera.org/specializations/reinforcement-learning">RL specialization</a> at Coursera.</li>
<li>Some exercises are taken from <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span> and modified slightly.</li>
</ul>
<p>I would like to thank all for their inspiration.</p>
<!-- Also thanks to Solveig for proofreading the draft. -->
<p>This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.</p>
</div>
<div id="sec-intro-ex" class="section level2 unnumbered hasAnchor">
<h2>Exercises<a href="index.html#sec-intro-ex" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="sec-intro-ex-annotate" class="section level3 unnumbered hasAnchor">
<h3>Exercise - How to annotate<a href="index.html#sec-intro-ex-annotate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The online course notes can be annotated using <a href="https://web.hypothes.is/">hypothes.is</a>. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help indicate spell errors, unclear content etc. in the notes.</p>
<!-- <img src="img/annotate.png" align="right" alt="Hypothes.is icons" title="Hypothes.is icons" style = "float: right; margin: 0 0 0 15px;"> -->
<ol style="list-style-type: decimal">
<li>Sign-up at <a href="https://web.hypothes.is/">hypothes.is</a>. If you are using Chrome you may also install the <a href="https://chrome.google.com/webstore/detail/hypothesis-web-pdf-annota/bjfhmglciegochdpefhhlphglcehbmek">Chrome extension</a>.</li>
<li>Go back to this page and login in the upper right corner (there should be some icons e.g. <code>&lt;</code>).</li>
<li>Select some text and try to annotate it using both a private and public annotation (you may delete it again afterwards).</li>
<li>Go to the <a href="https://bss-osca.github.io/rl/slides/00_intro-slides.html#6">slides for this module</a> and try to annotate the page with a private comment.</li>
</ol>
</div>
<div id="sec-intro-ex-templates" class="section level3 unnumbered hasAnchor">
<h3>Exercise - Templates<a href="index.html#sec-intro-ex-templates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A template in RMarkdown of the course notes and exercises are available at <a href="gh-rl-student">GitHub</a>. You can download the repository and keep your own notes during the course by having an R project with it.</p>
<ol style="list-style-type: decimal">
<li>Open R studio and do: <strong>File &gt; New Project … &gt; Version Control &gt; Git</strong>. Add <code>https://github.com/bss-osca/rl-student</code> as repository url and create the project.</li>
<li>Run <code>renv::restore()</code> from the R command line to install needed packages (this may take some time). If you experience errors then try to install the packages one at a time using <code>install.packages("pkg name")</code>.</li>
<li>Open e.g. the file <code>01_rl-intro.Rmd</code> and try to knit it using the Knit button in the upper left corner. A html file with the output will be made. You should be able to add you own notes and solve the exercises using the Rmd file for each module.</li>
</ol>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="mod-rl-intro.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
