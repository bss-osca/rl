---
output: html_document
editor_options: 
  chunk_output_type: console
---






# Dynamic programming {#mod-dp}

The term *Dynamic Programming* (*DP*) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP). A DP model must satisfy the *principle of optimality*. That is, an optimal policy must consist for optimal sub-polices or alternatively the optimal value function in a state can be calculated using optimal value functions in future states. This is indeed what is described with the Bellman optimality equations. 

DP do both *policy evaluation* (prediction) and *control*. Policy evaluation give us the value function $v_\pi$ given a policy $\pi$. Control refer to finding the best policy or optimizing the value function. This can be done using the Bellman optimality equations.

Two main problems arise with DP. First, often we do not have full information about the MDP model, e.g. the rewards or transition probabilities are unknown. Second, we need to calculate the value function in all states using the rewards, actions, and transition probabilities. Hence, using DP may be computationally expensive if we have a large number of states and actions.

Note the term programming in DP have nothing to do with a computer program but comes from that the mathematical model is called a "program". 


## Learning outcomes 

By the end of this module, you are expected to:

* Describe the distinction between policy evaluation and control.
* Identify when DP can be applied, as well as its limitations.
* Explain and apply iterative policy evaluation for estimating state-values given a policy.
* Interpret the policy improvement theorem.
* Explain and apply policy iteration for finding an optimal policy.
* Explain and apply value iteration for finding an optimal policy.
* Describe the ideas behind generalized policy iteration.
* Interpret the distinction between synchronous and asynchronous dynamic programming methods.

The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2, 4, 6, 7, 8, 10 and 12 of the course.

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Textbook readings

For this week, you will need to read Chapter 4-4.7 in @Sutton18. Read it before continuing this module. A summary of the book notation can be seen [here][sutton-notation].


```{=html}
<div>
Slides for this module can be seen
<a href="https://bss-osca.github.io/rl/slides/05_dp-slides.html" target="_blank">here.</a>
You do not have to look at them before the lecture!
</div>
```

## Policy evaluation {#sec-dp-pe}

The state-value function can be represented using the Bellman equation \@ref(eq:bell-state):
$$
v_\pi(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_\pi(s')\right).            
(\#eq:bm-pol-eval)
$$

If the dynamics are known perfectly, this becomes a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns $v_\pi(s), s \in \mathcal{S}$. This linear system can be solved using e.g. some software. However, inverting the matrix can be computationally expensive for a large state space. Instead we consider an iterative method and a sequence of value function approximations $v_0, v_1, v_2, \ldots$, with initial approximation $v_0$ chosen arbitrarily e.g. $v_0(s) = 0 \:  \forall s$ (ensuring terminal state = 0). We can use *a sweep* with the Bellman equation to update the values:

\begin{equation}
v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_k(s')\right) 
\end{equation}

We call this update an *expected update* because it is based on the expectation over all possible next states, rather than a sample of reward from the next state. This update will converge to $v_\pi$ after a number of sweeps of the state-space. Since we do not want an infinite number of sweeps we introduce a threshold $\theta$ (see Figure \@ref(fig:policy-eval-alg)). Note the algorithm uses two arrays to maintain the state-value ($v$ and $V$). Alternatively, a single array could be used that update values in place, i.e. $V$ is used in place of $v$. Hence, state-values are updated faster. 

<div class="figure" style="text-align: center">
<img src="img/policy-evalution.png" alt="Iterative policy evaluation [@Sutton18]."  />
<p class="caption">(\#fig:policy-eval-alg)Iterative policy evaluation [@Sutton18].</p>
</div>


## Policy Improvement

From the Bellman optimality equation \@ref(eq:bell-opt-state) we have 

$$
\begin{align}
\pi_*(s) &= \arg\max_{a \in \mathcal{A}} q_*(s, a) \\
         &= \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_*(s')\right).
\end{align}
(\#eq:pi-det)
$$
That is, a deterministic optimal policy can be found by choosing *greedy* the best action given the optimal value function. If we apply this greed action selection to the value function for a policy $\pi$ and pick the action with most $q$:
$$
\begin{align}
\pi'(s) &= \arg\max_{a \in \mathcal{A}} q_\pi(s, a) \\
         &= \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_\pi(s')\right),
\end{align}
(\#eq:pi-mark-det)
$$
then 
$$
q_\pi(s, \pi'(s)) \geq q_\pi(s, \pi(s)) = v_\pi(s) \quad \forall s \in \mathcal{S}.
$$
Note if $\pi'(s) = \pi(s), \forall s\in\mathcal{S}$ then the Bellman optimality equation \@ref(eq:bell-opt-state) holds and $\pi$ must be optimal; Otherwise, 
$$
\begin{align}
  v_\pi(s) &\leq q_\pi(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma (R_{t+2} + \gamma^2 v_\pi(S_{t+2})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2}, \pi'(S_{t+2})) | S_t = s] \\
&\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...)) | S_t = s] \\
&= v_{\pi'}(s),
\end{align}
$$
That is, policy $\pi'$ is strictly better than policy $\pi$ since there is at least one state $s$ for which $v_{\pi'}(s) > v_\pi(s)$. We can formalize the above deductions in a theorem.

::: {.theorem name="Policy improvement theorem"}
Let $\pi$, $\pi'$ be any pair of deterministic policies, such that
\begin{equation}
    q_\pi(s, \pi'(s)) \geq v_\pi(s) \quad \forall s \in \mathcal{S}.
\end{equation}
That is, $\pi'$ is as least as good as $\pi$.
:::


## Policy Iteration

Given the policy improvement theorem we can now improve policies iteratively until we find an optimal policy: 

1. Pick an arbitrary initial policy $\pi$.
2. Given a policy $\pi$, estimate $v_\pi(s)$ via the policy evaluation algorithm.
3. Generate a new, improved policy $\pi' \geq \pi$ by *greedily* picking $\pi' = \text{greedy}(v_\pi)$ using Eq. \@ref(eq:pi-mark-det). If $\pi'=\pi$ then stop ($\pi_*$ has been found); otherwise go to Step 2.

The algorithm is given in Figure \@ref(fig:policy-ite-alg). The sequence of calculations will be: 
$$\pi_0 \xrightarrow[]{E} v_{\pi_0} \xrightarrow[]{I} \pi_1 \xrightarrow[]{E} v_{\pi_1} \xrightarrow[]{I} \pi_2 \xrightarrow[]{E} v_{\pi_2}  \ldots \xrightarrow[]{I} \pi_* \xrightarrow[]{E} v_{*}$$
The number of steps of policy iteration needed to find the optimal policy are often low.

<div class="figure" style="text-align: center">
<img src="img/policy-iteration.png" alt="Policy iteration [@Sutton18]." width="70%" />
<p class="caption">(\#fig:policy-ite-alg)Policy iteration [@Sutton18].</p>
</div>


## Value Iteration

Policy iteration requires full policy evaluation at each iteration step. This could be an computationally expensive process which requires may sweeps of the state space. In *value iteration*, the policy evaluation is stopped after one sweep of the state space. Value iteration is achieved by turning the Bellman optimality equation into an update rule:
$$
v_{k+1}(s) = \max_a \left(r(s,a) + \gamma\sum_{s'} p(s'|s, a)v_k(s')\right)
$$
Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement, since it performs a greedy update while also evaluating the current policy. Also, it is important to understand that the value-iteration algorithm does not require a policy to work. No actions have to be chosen. Rather, the state-values are updated and after the last step of value-iteration the optimal policy $\pi_*$ is found:

$$
\pi_*(s) = \arg\max_{a \in \mathcal{A}} \left(r(s,a) + \gamma\sum_{s' \in \mathcal{S}} p(s' | s, a) v_*(s')\right),
$$
The algorithm is given in Figure \@ref(fig:value-ite-alg). Since we do not want an infinite number of iterations we introduce a threshold $\theta$. The sequence of calculations will be (where G denotes greedy action selection): 
$$v_{0} \xrightarrow[]{EI} v_{1} \xrightarrow[]{EI} v_{2}  \ldots \xrightarrow[]{EI} v_{*} \xrightarrow[]{G} \pi_{*}$$
<div class="figure" style="text-align: center">
<img src="img/value-iteration.png" alt="Value iteration [@Sutton18]." width="70%" />
<p class="caption">(\#fig:value-ite-alg)Value iteration [@Sutton18].</p>
</div>



## Generalized policy iteration

Generalised Policy Iteration (GPI) is the process of letting policy evaluation and policy improvement interact, independent of granularity. For instance, improvement/evaluation can be performed by doing complete sweeps of the state space (policy iteration), or improve the state-value using a single sweep of the state space (value iteration). GPI can also do *asynchronous* updates of the state-value where states are updated individually, in any order. This can
significantly improve computation. Examples on asynchronous DP are

* *In-place DP* mentioned in Section \@ref(sec-dp-pe) where instead of keeping a copy of the old and new value function in each value-iteration update, you can just update the value functions in-place. Hence asynchronous updates in other parts of the state-space will directly be affected resulting in faster updates. 

* *Prioritized sweeping* where we keep track of how "effective" or "significant" updates to our state-values are. States where the updates are more significant are likely further away from converging to the optimal value. As such, we would like to update them first. For this, we would compute the *Bellman error*:
$$|v_{k+1}(s) - v_k(s)|,$$
and keep these values in a priority queue. You can then efficiently pop the top of it to always get the state you should update next.

* *Prioritize local updates* where you update nearby states given the current state, e.g. if your robot is in a particular region of the grid, it is much more important to update nearby states than faraway ones.

GPI works and will convergence to the optimal policy and optimal value function if the states are visited (in theory) an infinite number of times. That is, you must explore the whole state space for GPI to work.



## Example - Factory Storage

Let us consider Exercise \@ref(ex-mdp-1-storage) where a factory has a storage tank with a capacity of 4 $\mathrm{m}^{3}$ for temporarily storing waste produced by the factory. Each week the factory produces $0,1$, 2 or 3 $\mathrm{m}^{3}$ waste with respective probabilities 
$$p_{0}=\displaystyle \frac{1}{8},\ p_{1}=\displaystyle \frac{1}{2},\ p_{2}=\displaystyle \frac{1}{4} \text{ and } p_{3}=\displaystyle \frac{1}{8}.$$ 
If the amount of waste produced in one week exceeds the remaining capacity of the tank, the excess is specially removed at a cost of $30 per cubic metre. At the end of each week there is a regular opportunity to remove all waste from the storage tank at a fixed cost of $25 and a variable cost of $5 per cubic metre. 

The problem can be modelled as a finite MDP where a state denote the amount of waste in the tank at the end of week $n$ just before the regular removal opportunity $$\mathcal{S} = \{ 0,1,2,3,4 \}.$$
The action space is $$\mathcal{A}(s) = \{ empty, keep \}.$$
The expected cost of a given state and action is the cost of empting the container and the expected cost of a special removal during the next week. Hence the expected reward is $$r(s, e) = -(25 + 5s)$$ 
and $$r(s,k) = -30\sum_{i>4-s} (s+i-4)p_i.$$
Finally, the transition probabilities are: 
$$
\begin{align} 
   p(s'|s,k) &= p_{s'-s}\text{ if } s\leq s' \leq 3, \\
   p(4|s,k)  &= \sum_{i\geq 4-s} p_i, \\
   p(s'|s,e) &= p_{s'}\text{ if }  0\leq s' \leq 4, \\
   p(s'|s,a) &=  0 \text{ otherwise.}
\end{align}
$$

For solving the MDP we implement an MDP R6 class in R:


```r
library(R6)
library(hash)
library(tidyverse)

#' R6 Class representing the MDP
#' 
#' Note since the MDP is a model with full information, we combine the agent and environment into a single class.
MDPClass <- R6Class("MDPClass",
   public = list(
      
      #' @field model The Markov decision process (model). The model is represented 
      #' using a hash list for the states. Each states contains a list with `actions`: a hash 
      #' list with actions and `pi` a vector with policy pr (named vector with only 
      #' positive values). The `actions` hash list contains actions with trans 
      #' pr `pr` (named vector only with positive values) and expected reward `reward`.
      model = NULL,  
      
      #' @field sV The state-values stored in a hash.
      v = NULL,
      
      #' @description Create an object (when call new).
      #' @return The new object.
      initialize = function() {
         self$model <- hash()
         self$v <- hash()
      },
      
      #' @description Add the states (keys/strings in the hash)
      #' @param s A vector of states (converted to strings).
      addStateSpace = function(s) {
         keys <- make.keys(s)
         self$model[keys] <- list(pi = NA)   # don't use pi = NULL since then won't be defined 
         self$setStateValue()  # so v defined
         return(invisible(NULL))
      },
      
      #' @description Add the actions to a state
      #' @param stateStr State key/string.
      #' @param a A vector of actions (converted to strings).
      addActionSpace = function(stateStr, a) {
         a <- make.keys(a)
         self$model[[stateStr]]$actions <- hash()
         self$model[[stateStr]]$actions[a] <- NA
         return(invisible(NULL))
      },
      
      #' @description Add expected reward and trans pr to an action
      #' @param stateStr State key/string.
      #' @param actionStr Action key/string.
      #' @param r The expected reward.
      #' @param pr A named vector with positive trans pr. The name of an element must be the state key.
      addAction = function(stateStr, actionStr, r, pr) {
         if (!has.key(stateStr, self$model)) {
            self$addStateSpace(stateStr)
            # self$model[make.keys(stateStr)] <- hash(pi = NA, actions = hash())
         } 
         self$model[[stateStr]]$actions[[actionStr]] <- list(r = r, pr = pr)
         return(invisible(NULL))
      },
      
      #' @description Set the state-value of states
      #' @param stateStr A vector of state keys.
      #' @param value The value.
      setStateValue = function(stateStr = keys(self$model), value = 0) {
         self$v[stateStr] <- value
         return(invisible(NULL))
      },
      
      #' @description Set the policy to a random determistic policy.
      setRandomDeterministicPolicy = function() {
         stateStr = keys(self$model)
         for (s in stateStr) {
            self$model[[s]]$pi <- 1
            names(self$model[[s]]$pi) <- sample(self$getActionKeys(s), 1)
         }
         return(invisible(NULL))
      },
      
      #' @description Set a deterministic policy
      #' @param sa A named vector with action keys and names equal state keys-
      setDeterministicPolicy = function(sa) {
         states <- names(sa)
         pi = 1
         for (i in 1:length(sa)) {
            names(pi) <- sa[i]
            s <- states[i]
            self$model[[s]]$pi <- pi
         }
         return(invisible(NULL))
      },
      
      #' @description Return the state keys
      getStateKeys = function() {
         keys(self$model)
      },
      
      #' @description Return the action keys
      #' @param s The state considered.
      getActionKeys = function(s) {
         keys(self$model[[s]]$actions) 
      },
      
      #' @description Return the expected reward and trans pr of actions in a state
      #' @param s The state considered.
      getActionInfo = function(s) {
         as.list(self$model[[s]]$actions) 
      },
      
      #' @description Return the current policy as a tibble
      getPolicy = function() {
         # if (all(sapply(self$model, FUN = function(s) {s$pi}) == 1)) { # deterministic policy
         #    sapply(self$model, FUN = function(s) {names(s$pi)})
         # } else {
         map_dfr(self$getStateKeys(), .f = function(s) {
               list(state = s, action = names(self$model[[s]]$pi), pr = self$model[[s]]$pi)
            })
      },
      
      #' @description Return the state-values as a tibble
      #' @param s A vector of state keys.
      getStateValues = function(s = keys(self$v)) {
         tibble(state = s, v = values(self$v, keys = s))
      },
      
      #' @description Return a matrix with trans pr for a given action. 
      #' @param a Action key.
      getTransPrActionMat = function(a) {
         states <- keys(self$model)
         m <- matrix(0, nrow = length(states), ncol = length(states))
         colnames(m) <- states
         rownames(m) <- states
         for (s in states) {
            m[s, names(self$model[[s]]$actions[[a]]$pr)] <- self$model[[s]]$actions[[a]]$pr
         }
         return(m)
      },
      
      #' @description Returns all rewards in a matrix
      getRewardMat = function() {
         states <- keys(self$model)
         actions <- unique(unlist(sapply(states, function(s) self$getActionKeys(s))))
         m <- matrix(NA, nrow = length(states), ncol = length(actions))
         colnames(m) <- actions
         rownames(m) <- states
         for (s in states) {
            for (a in self$getActionKeys(s)) {
               m[s, a] <- self$model[[s]]$actions[[a]]$r
            }
         }
         return(m)
      },
      
      #' @description Bellman calculations for a given state and action
      #' @param gamma Discount rate.
      #' @param s State key.
      #' @param a Action key.
      bellmanCalc = function(gamma, s, a) {
         pr <- self$model[[s]]$actions[[a]]$pr
         r <- self$model[[s]]$actions[[a]]$r
         nS <- names(pr)
         vS <- values(self$v, nS)
         return(r + gamma * sum(pr * vS))
      },
      
      #' @description Iterative policy evaluation of current policy (defined by pi)
      #' @param gamma Discount rate.
      #' @param theta Threshold parameter.
      #' @param maxIte Maximum number of iterations.
      #' @param reset If true set all state-values to 0.
      policyEval = function(gamma, theta = 0.00001, maxIte = 10000, reset = TRUE) {
         if (reset) self$setStateValue()  # set to 0
         for (ite in 1:maxIte) { 
            delta <- 0   # Bellman error 
            for (s in keys(self$model)) {
               v <- self$v[[s]]  
               # update
               pi <- self$model[[s]]$pi
               actions <- names(pi)
               val =  0
               for (a in actions) {
                  # pr <- self$model[[s]]$actions[[a]]$pr
                  # r <- self$model[[s]]$actions[[a]]$r
                  # nS <- names(pr)
                  # vS <- values(self$v, nS)
                  val <- val + pi[a] * self$bellmanCalc(gamma, s, a)
               }
               self$v[[s]] <- val
               delta <- max(delta, abs(v-val))
            }
            if (delta < theta) break
         }
         if (ite == maxIte) warning("Policy evaluation algorithm stopped at max iterations allowed:", maxIte)
      },

      #' @description Policy iteration using iterative policy eval
      #' @param gamma Discount rate.
      #' @param theta Threshold parameter.
      #' @param maxIteEval Maximum number of iterations when evaluate policy.
      #' @param maxItePolicy Maximum number of policy iterations.
      policyIte = function(gamma, theta = 0.00001, maxIteEval = 10000, maxItePolicy = 100) {
         self$setRandomDeterministicPolicy()
         for (ite in 1:maxItePolicy) {
            self$policyEval(gamma, theta, maxIteEval, reset = FALSE)
            # self$setStateValue()  # set to 0
            stable <- TRUE
            for (s in keys(self$model)) {
               piOld <- names(self$model[[s]]$pi)
               actions <- self$getActionKeys(s)
               vMax =  -Inf
               for (a in actions) {
                  val <- self$bellmanCalc(gamma, s, a)
                  if (val > vMax) {
                     names(self$model[[s]]$pi) <- a
                     vMax <- val
                  }
               }
               if (piOld != names(self$model[[s]]$pi) ) stable <- FALSE
            }
            if (stable) break
         }
         if (ite == maxItePolicy) warning("Policy Iteration algorithm stopped at max iterations allowed:", maxItePolicy)
         message(str_c("Policy iteration algorihm finished in ", ite, " iterations."))
         return(invisible(NULL))
      },
      
      #' @description Value iteration
      #' @param gamma Discount rate.
      #' @param theta Threshold parameter.
      #' @param maxIte Maximum number of iterations.
      #' @param reset If true set all state-values to 0.
      valueIte = function(gamma, theta = 0.00001, maxIte = 10000, reset = TRUE) {
         self$setRandomDeterministicPolicy()
         if (reset) self$setStateValue()  # set to 0
         for (ite in 1:maxIte) { 
            delta <- 0   # Bellman error 
            for (s in keys(self$model)) {
               v <- self$v[[s]]  
               actions <- self$getActionKeys(s)
               vMax =  -Inf
               for (a in actions) {
                  val <- self$bellmanCalc(gamma, s, a)
                  if (val > vMax) {
                     vMax <- val
                     names(self$model[[s]]$pi) <- a
                  }
               }
               self$v[[s]] <- vMax
               delta <- max(delta, abs(v-vMax))
            }
            if (delta < theta) break
         }
         if (ite == maxIte) warning("Value iteration algorithm stopped at max iterations allowed:", maxIte)         
         message(str_c("Value iteration algorihm finished in ", ite, " iterations."))
         return(invisible(NULL))
      }
   )
)
```

You may have a look at the code and try to get an overview. Let us now try to solve the problem. First we need some functions for calculating the reward and transition probabilities:


```r
#' Expected reward of an action
#' @param s Waste amount (state).
#' @param a Action (keep or empty).
#' @return The expected reward
reward <-function(s, a) {
   i <- as.numeric(s)
   p<-c(1/8, 1/2, 1/4, 1/8)
   if (a=="keep") {
      if (i<2) return(0)   # no excess waste
      k <- (4-i+1):3
      return(-30*sum( (i+k-4)*p[k+1] ) )
   }
   if (a=="empty") {
      return(-1*(25 + 5*i))
   }
   return(NULL)
}

#' Find transition probabilities
#' @param s Waste amount (state).
#' @param a Action (keep or empty).
#' @return The trans pr and id 
transPr<-function(s, a) {
   i <- as.numeric(s)
   p<-c(1/8, 1/2, 1/4, 1/8)
   pr<-NULL
   id<-NULL
   if (a=="keep") {
      if (i<4) for (j in i:3) {
         pr<-c(pr,p[j-i+1])
         id<-c(id,j)
      }
      if (i>0) {
         pr<-c(pr,sum(p[(4-i):3+1]))
         id<-c(id,4)
      }
   }
   if (a=="empty") {
      for (j in 0:3) {
         pr<-c(pr,p[j+1])
         id<-c(id,j)
      }
   }
   names(pr) <- id
   return(pr)
}
```

For instance the expected reward in state 4 under action keep is:


```r
s <- 3
a <- "keep"
reward(s, a)
#> [1] -15
transPr(s, a)
#>     3     4 
#> 0.125 0.875
```

We are now ready to build the model:


```r
mdp <- MDPClass$new()
mdp$addStateSpace(0:4)   # add state keys
for (s in mdp$getStateKeys()) mdp$addActionSpace(s, c("empty", "keep"))  # add action keys
# Add trans pr and rewards
for (s in mdp$getStateKeys()) {
   for (a in mdp$getActionKeys(s)) {
      mdp$addAction(s, a, r = reward(s,a), pr = transPr(s,a))
   }
}
```

We may check that we have built the MDP correct using: 


```r
mdp$getTransPrActionMat("keep")
#>       0     1     2     3     4
#> 0 0.125 0.500 0.250 0.125 0.000
#> 1 0.000 0.125 0.500 0.250 0.125
#> 2 0.000 0.000 0.125 0.500 0.375
#> 3 0.000 0.000 0.000 0.125 0.875
#> 4 0.000 0.000 0.000 0.000 1.000
mdp$getTransPrActionMat("empty")
#>       0   1    2     3 4
#> 0 0.125 0.5 0.25 0.125 0
#> 1 0.125 0.5 0.25 0.125 0
#> 2 0.125 0.5 0.25 0.125 0
#> 3 0.125 0.5 0.25 0.125 0
#> 4 0.125 0.5 0.25 0.125 0
mdp$getRewardMat()
#>   empty   keep empty keep empty keep empty keep empty keep
#> 0   -25   0.00    NA   NA    NA   NA    NA   NA    NA   NA
#> 1   -30   0.00    NA   NA    NA   NA    NA   NA    NA   NA
#> 2   -35  -3.75    NA   NA    NA   NA    NA   NA    NA   NA
#> 3   -40 -15.00    NA   NA    NA   NA    NA   NA    NA   NA
#> 4   -45 -41.25    NA   NA    NA   NA    NA   NA    NA   NA
mdp$getActionInfo("3")
#> $keep
#> $keep$r
#> [1] -15
#> 
#> $keep$pr
#>     3     4 
#> 0.125 0.875 
#> 
#> 
#> $empty
#> $empty$r
#> [1] -40
#> 
#> $empty$pr
#>     0     1     2     3 
#> 0.125 0.500 0.250 0.125
```

Let us try to evaluate a fixed policy given a discount rate of 0.5:


```r
sa <- c("0" = "keep", "1" = "keep", "2" = "keep", "3" = "keep", "4" = "empty")
mdp$setDeterministicPolicy(sa)
mdp$getPolicy()
#> # A tibble: 5 × 3
#>   state action    pr
#>   <chr> <chr>  <dbl>
#> 1 0     keep       1
#> 2 1     keep       1
#> 3 2     keep       1
#> 4 3     keep       1
#> 5 4     empty      1
mdp$policyEval(gamma = 0.5)
mdp$getStateValues()
#> # A tibble: 5 × 2
#>   state     v
#>   <chr> <dbl>
#> 1 0     -10.7
#> 2 1     -16.3
#> 3 2     -26.3
#> 4 3     -42.0
#> 5 4     -55.7
```

Let us try to find the optimal policy using policy iteration:


```r
mdp$policyIte(gamma = 0.5)
mdp$getPolicy()
#> # A tibble: 5 × 3
#>   state action    pr
#>   <chr> <chr>  <dbl>
#> 1 0     keep       1
#> 2 1     keep       1
#> 3 2     keep       1
#> 4 3     keep       1
#> 5 4     empty      1
mdp$getStateValues()
#> # A tibble: 5 × 2
#>   state     v
#>   <chr> <dbl>
#> 1 0     -10.7
#> 2 1     -16.3
#> 3 2     -26.3
#> 4 3     -42.0
#> 5 4     -55.7
```

If we do value iteration we should get the same results:


```r
mdp$valueIte(0.5)
mdp$getPolicy()
#> # A tibble: 5 × 3
#>   state action    pr
#>   <chr> <chr>  <dbl>
#> 1 0     keep       1
#> 2 1     keep       1
#> 3 2     keep       1
#> 4 3     keep       1
#> 5 4     empty      1
mdp$getStateValues()
#> # A tibble: 5 × 2
#>   state     v
#>   <chr> <dbl>
#> 1 0     -10.7
#> 2 1     -16.3
#> 3 2     -26.3
#> 4 3     -42.0
#> 5 4     -55.7
```

Finally, let us try to solve the problem with a discount rate of 0.99:


```r
mdp$policyIte(gamma =0.99)
mdp$getPolicy()
#> # A tibble: 5 × 3
#>   state action    pr
#>   <chr> <chr>  <dbl>
#> 1 0     keep       1
#> 2 1     keep       1
#> 3 2     keep       1
#> 4 3     empty      1
#> 5 4     empty      1
mdp$getStateValues()
#> # A tibble: 5 × 2
#>   state      v
#>   <chr>  <dbl>
#> 1 0     -1750.
#> 2 1     -1762.
#> 3 2     -1776.
#> 4 3     -1790.
#> 5 4     -1795.
```

Note we now empty also at 3 m$^3$.




## Summary 

Read Chapter 4.8 in @Sutton18.

## Exercises

Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the [help page](#help). Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!

### Exercise - Gambler's problem {#ex-dp-gambler}

Consider the gambler's problem in Exercise \@ref(ex-mdp-1-gambler).

<!-- Q1 -->




<div class="modal fade bs-example-modal-lg" id="Z9SVqmMWZXl9Gei4EyQk" tabindex="-1" role="dialog" aria-labelledby="Z9SVqmMWZXl9Gei4EyQk-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="Z9SVqmMWZXl9Gei4EyQk-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
#' MDP model for the gamblers problem.
#' @param prH Probability of head.
#' @return The mdp object (R6 class).
buildGamblerModel <- function(prH) {
   mdp <- MDPClass$new()      # initialize mdp object
   mdp$addStateSpace(0:101)   # add states (states are keys in a hash and are always converted to strings), 101 represent finished
   for (s in mdp$getStateKeys()) {  # add actions for each state (only the key of the action)
      i <- as.numeric(s)
      if (i > 0 & i < 100) mdp$addActionSpace(s, 1:min(i, 100-i))
      if (i == 0) mdp$addActionSpace(s, "Loose")  
      if (i == 100) mdp$addActionSpace(s, "Win")  
      if (i == 101) mdp$addActionSpace(s, "Dummy")
   }
   # Add trans pr and expected reward to the actions
   for (s in as.character(1:99)) {
      for (a in mdp$getActionKeys(s)) {
         iS <- as.numeric(s)
         iA <- as.numeric(a)
         transPr <- c(prH, 1- prH)
         names(transPr) <- c(iS+iA, iS-iA)
         mdp$addAction(s, a, r = 0, pr = transPr)
      }
   }
   # add special actions
   mdp$addAction(s = "0", a = "Loose", r = 0, pr = c("101" = 1))
   mdp$addAction(s = "100", a = "Win", r = 1, pr = c("101" = 1))
   mdp$addAction(s = "101", a = "Dummy", r = 0, pr = c("101" = 1))
   return(mdp)
}
```


<p>We make a function that return the mdp object</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#Z9SVqmMWZXl9Gei4EyQk">Solution</button>

1) Build the model in R.



<!-- Q2 -->

<div class="modal fade bs-example-modal-lg" id="RXw7ODsreTfdOyVkC5Nh" tabindex="-1" role="dialog" aria-labelledby="RXw7ODsreTfdOyVkC5Nh-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="RXw7ODsreTfdOyVkC5Nh-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
solveGambler <- function(prH) {
   mdp <- buildGamblerModel(prH)
   mdp$valueIte(gamma = 1, theta = 0.0001)
   df <- left_join(mdp$getStateValues(), mdp$getPolicy(), by = "state") %>% 
      select(-pr)
   df <- df %>% 
      mutate(state = as.numeric(state)) %>% 
      filter(state > 0, state < 100) %>% 
      mutate(action = as.numeric(action)) 
   ptPr <- ggplot(df, aes(x = state, y = v)) +
      geom_col() + 
      labs(title = str_c("State-values for prH=", prH))
   ptPolicy <- ggplot(df, aes(x = state, y = action)) +
      geom_col() +
      labs(title = str_c("Optimal policy for prH=", prH))
   return(list(ptPr = ptPr, ptPolicy = ptPolicy, policy = df))   
}
res04 <- solveGambler(prH = 0.4)
res04$ptPr
res04$ptPolicy
```

<img src="05_dp_files/figure-html/gambler-solve-1.png" width="672" style="display: block; margin: auto;" /><img src="05_dp_files/figure-html/gambler-solve-2.png" width="672" style="display: block; margin: auto;" />
<p>There is a lot of different optimal policies. In general since \(p_h\) is below 0.5 we want to find a policy with as few flips as possible.</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#RXw7ODsreTfdOyVkC5Nh">Solution</button>



2) Solve the problem using value iteration with $p_h = 0.4$. Plot the policy with states 1-99 on the x-axis and action on the y-axis. Plot the state-values given states 1-99 on the x-axis. Your policy may not look like the policy in the book, why?


<!-- Q3 -->

<div class="modal fade bs-example-modal-lg" id="AvMjlfNVI13hvCA5EVaa" tabindex="-1" role="dialog" aria-labelledby="AvMjlfNVI13hvCA5EVaa-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="AvMjlfNVI13hvCA5EVaa-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
# Solve the problems
res025 <- solveGambler(prH = 0.25)
res055 <- solveGambler(prH = 0.55)
# State-values
res025$policy %>% filter(state %in% c(10,67))
#> # A tibble: 2 × 3
#>   state       v action
#>   <dbl>   <dbl>  <dbl>
#> 1    10 0.00708     10
#> 2    67 0.309       17
res055$policy %>% filter(state %in% c(10,67))
#> # A tibble: 2 × 3
#>   state     v action
#>   <dbl> <dbl>  <dbl>
#> 1    10 0.855      1
#> 2    67 1.00       1
# Plot results
library(patchwork)
(res025$ptPr + res055$ptPr) / (res025$ptPolicy + res055$ptPolicy)
```

<img src="05_dp_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" />
</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#AvMjlfNVI13hvCA5EVaa">Solution</button>

3) Solve the problem using value iteration with $p_h = 0.22$ and $0.55$ and plot the results. What is the probability of winning in state 10 and 67?

### Exercise - Maintenance problem {#ex-dp-maintain}

At the beginning of each day a piece of equipment is inspected to reveal its actual working condition. The equipment will be found in one of the working conditions $s = 1,\ldots, N$ where the working condition $s$ is better than the working condition
$s+1$. 

The equipment deteriorates in time. If the present working condition is $s$ and no repair is done, then at the beginning of the next day the equipment has working condition $s'$ with probability $q_{ss'}$. It is assumed that $q_{ss'}=0$ for $s'<s$ and $\sum_{s'\geq s}q_{ss'}=1$. 

The working condition $s=N$ represents a malfunction that requires an enforced repair taking two days. For the intermediate states $s$ with $1<s<N$ there is a choice between preventively repairing the equipment and letting the equipment operate for the present day. A preventive repair takes only one day. A repaired system has the working condition $s=1$. 

The cost of an enforced repair upon failure is $C_{f}$ and the cost of a preemptive repair in working condition $s$ is $C_{p,s}$. We wish to determine a maintenance rule which minimizes the repair cost.

The problem can be formulated as an MDP. Since an enforced repair takes two days and the state of the system has to be defined at the beginning of each day, we need an auxiliary state for the situation in which an enforced repair is in progress already for one day.


<!-- Q1 -->

<div class="modal fade bs-example-modal-lg" id="dXcKidn2M8Qdn0nbarvk" tabindex="-1" role="dialog" aria-labelledby="dXcKidn2M8Qdn0nbarvk-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="dXcKidn2M8Qdn0nbarvk-title">Solution</h4></div><div class="modal-body">

<p>\[\mathcal{S}=\{1,2,\ \ldots,\ N,\ N+1\}.\] State \(s\) with \(1\leq s\leq N\) corresponds to the situation in which an inspection reveals working condition \(s\), while state \(N+1\) corresponds to the situation in which an enforced repair is in progress already for one day.</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#dXcKidn2M8Qdn0nbarvk">Solution</button>

1) Define the state space $\mathcal{S}$, i.e. the the set of possible states of the system.


<!-- Q2 -->

<div class="modal fade bs-example-modal-lg" id="7u8hFTzd8uMDExBHd9f9" tabindex="-1" role="dialog" aria-labelledby="7u8hFTzd8uMDExBHd9f9-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="7u8hFTzd8uMDExBHd9f9-title">Solution</h4></div><div class="modal-body">

<p>The set of possible actions in state \(s\) is chosen as \[\mathcal{A}(1)=\{0\},\ \mathcal{A}(s)=\{0,1\} \text{ for } 1<s<N, \mathcal{A}(N)=\mathcal{A}(N+1)=\{2\}.\]</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#7u8hFTzd8uMDExBHd9f9">Solution</button>

2) Consider actions
$$
a=\left\{\begin{array}{ll}
0 & \text{if no repair.}\\
1 & \text{if preventive repair.}\\
2 & \text{if forced repair.}\\
\end{array}\right.
$$
Define the action space $\mathcal{A}(s)$ for all states $s$.


<!-- Q3 -->

<div class="modal fade bs-example-modal-lg" id="RSgF0YE3IIdltiMBNGjs" tabindex="-1" role="dialog" aria-labelledby="RSgF0YE3IIdltiMBNGjs-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="RSgF0YE3IIdltiMBNGjs-title">Solution</h4></div><div class="modal-body">

<p>The expected reward is \(r(N+1,2) = 0\) and for \(0 < s < N\) is \(r(s,0) = 0\), \(r(s,1) = -C_{ps}\). Finally, \(r(N,2) = -C_{f}\) and \(r(N+1,2) = 0\).</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#RSgF0YE3IIdltiMBNGjs">Solution</button>

3) Assume that the number of possible working conditions equals $N=5$. What is the expected reward $r(s,a)$?


<!-- Q4 -->

<div class="modal fade bs-example-modal-lg" id="KSNGTEwv5hACRQEBJnfq" tabindex="-1" role="dialog" aria-labelledby="KSNGTEwv5hACRQEBJnfq-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="KSNGTEwv5hACRQEBJnfq-title">Solution</h4></div><div class="modal-body">

<p>The transition probabilities \(p(s'|s,a)\): \[p(s'|s,0) = q_{ij}, \text{ for } 1 \leq s<N,\] \[p(1|s,1) = 1 \text{ for } 1<s<N,\] \[p(N+1|N,2) = p(1|N+1,2) =1,\] and zero otherwise.</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#KSNGTEwv5hACRQEBJnfq">Solution</button>

4) What is the transition probabilities?


<!-- Q5 -->
5) Try make a drawing of the state-expanded hypergraph for stage $t$ and $t+1$.


<!-- Q6 -->

<div class="modal fade bs-example-modal-lg" id="hN2N5unyc2d0KeRe9C6C" tabindex="-1" role="dialog" aria-labelledby="hN2N5unyc2d0KeRe9C6C-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="hN2N5unyc2d0KeRe9C6C-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
Q <- matrix(c(
   0.90, 0.10, 0, 0, 0,
   0, 0.80, 0.10, 0.05, 0.05,
   0, 0, 0.70, 0.10, 0.20,
   0, 0, 0, 0.50, 0.50), nrow=4, byrow=T)
row.names(Q) <- str_c(1:4)
colnames(Q) <- str_c(1:5)

mdp <- MDPClass$new()      # initialize mdp object
mdp$addStateSpace(1:6)   # add states (states are keys in a hash and are always converted to strings), 101 represent finished
for (s in mdp$getStateKeys()) {  # add actions for each state (only the key of the action)
   i <- as.numeric(s)
   if (i == 1) mdp$addActionSpace(s, "nr")  
   if (i > 1 & i <= 4) mdp$addActionSpace(s, c("nr", "pr"))
   if (i == 5) mdp$addActionSpace(s, "fr")  
   if (i == 6) mdp$addActionSpace(s, "fr")
}
# check some of the keys
mdp$getActionKeys("2")
#> [1] "nr" "pr"
mdp$getActionKeys("1")
#> [1] "nr"
mdp$getActionKeys("6")
#> [1] "fr"

## Add trans pr and expected reward to the actions
cPR<-c(0,7,7,5,0,0)
for (s in mdp$getStateKeys()) {
   for (a in mdp$getActionKeys(s)) {
      i <- as.numeric(s)
      if (a == "nr") {
         rew <- 0
         transPr <- Q[i, Q[i,] > 0]
      }
      if (a == "pr") {
         rew <- -cPR[i]
         transPr <- 1
         names(transPr) <- "1"
      }
      if (a == "fr" & i == 5) {
         rew = -10
         transPr <- 1
         names(transPr) <- "6"
      } 
      if (a == "fr" & i == 6) {
         rew = 0
         transPr <- 1
         names(transPr) <- "1"
      } 
      mdp$addAction(s, a, r = rew, pr = transPr)
   }
}
# check some of the keys
mdp$getActionInfo("1")
#> $nr
#> $nr$r
#> [1] 0
#> 
#> $nr$pr
#>   1   2 
#> 0.9 0.1
mdp$getActionInfo("2")
#> $nr
#> $nr$r
#> [1] 0
#> 
#> $nr$pr
#>    2    3    4    5 
#> 0.80 0.10 0.05 0.05 
#> 
#> 
#> $pr
#> $pr$r
#> [1] -7
#> 
#> $pr$pr
#> 1 
#> 1
mdp$getActionInfo("5")
#> $fr
#> $fr$r
#> [1] -10
#> 
#> $fr$pr
#> 6 
#> 1
mdp$getActionInfo("6")
#> $fr
#> $fr$r
#> [1] 0
#> 
#> $fr$pr
#> 1 
#> 1
```


</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#hN2N5unyc2d0KeRe9C6C">Solution</button>

6) The repair costs are given by $C_{f}=10,\ C_{p2}=7,\ C_{p3}=7$ and $C_{p4}=5$ and the deterioration probabilities $q_{ij}$ are given by matrix Q:

   
   ```r
   Q <- matrix(c(
      0.90, 0.10, 0, 0, 0,
      0, 0.80, 0.10, 0.05, 0.05,
      0, 0, 0.70, 0.10, 0.20,
      0, 0, 0, 0.50, 0.50), nrow=4, byrow=T)
   Q
   #>      [,1] [,2] [,3] [,4] [,5]
   #> [1,]  0.9  0.1  0.0 0.00 0.00
   #> [2,]  0.0  0.8  0.1 0.05 0.05
   #> [3,]  0.0  0.0  0.7 0.10 0.20
   #> [4,]  0.0  0.0  0.0 0.50 0.50
   ```
   
   Build the MDP in R.


<!-- Q7 -->

<div class="modal fade bs-example-modal-lg" id="bwMwNYvznoU74pgtZFM9" tabindex="-1" role="dialog" aria-labelledby="bwMwNYvznoU74pgtZFM9-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="bwMwNYvznoU74pgtZFM9-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
mdp$valueIte(gamma = 0.6)
# mdp$policyIte(gamma = 0.6)
df <- left_join(mdp$getStateValues(), mdp$getPolicy(), by = "state") %>% 
select(-pr)
df
#> # A tibble: 6 × 3
#>   state        v action
#>   <chr>    <dbl> <chr> 
#> 1 1      -0.146  nr    
#> 2 2      -1.12   nr    
#> 3 3      -2.53   nr    
#> 4 4      -4.31   nr    
#> 5 5     -10.1    fr    
#> 6 6      -0.0876 fr
```


<p>In state 3 the discounted cost is 2.53.</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#bwMwNYvznoU74pgtZFM9">Solution</button>

7) Find the optimal policy given a discount rate of $\gamma = 0.6$. What is the average discounted cost of being in state 3?


<!-- Q8 -->

<div class="modal fade bs-example-modal-lg" id="p9Lu9E4WZ9iVrFIrh665" tabindex="-1" role="dialog" aria-labelledby="p9Lu9E4WZ9iVrFIrh665-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="p9Lu9E4WZ9iVrFIrh665-title">Solution</h4></div><div class="modal-body">

```{.r .fold-show}
mdp$valueIte(gamma = 0.99)
# mdp$policyIte(gamma = 0.99)
df1 <- left_join(mdp$getStateValues(), mdp$getPolicy(), by = "state") %>% 
select(-pr)
df
#> # A tibble: 6 × 3
#>   state        v action
#>   <chr>    <dbl> <chr> 
#> 1 1      -0.146  nr    
#> 2 2      -1.12   nr    
#> 3 3      -2.53   nr    
#> 4 4      -4.31   nr    
#> 5 5     -10.1    fr    
#> 6 6      -0.0876 fr
df1
#> # A tibble: 6 × 3
#>   state     v action
#>   <chr> <dbl> <chr> 
#> 1 1     -41.3 nr    
#> 2 2     -45.5 nr    
#> 3 3     -47.4 nr    
#> 4 4     -45.9 pr    
#> 5 5     -50.5 fr    
#> 6 6     -40.9 fr
```


<p>A higher discont rate put more weight on future values, i.e. you have to look further ahead resulting in more iterations. This also results in a higher disconted cost, since costs in the future have higher weight. The optimal policy looks a bit different compared to the policy we found for \(\gamma = 0.6\). However, if the policy is the same for two different discount rates then you will have the same rewards/costs and hence the same cost of the policy. The discounted cost will not be the same since you use the discount rate to say how much you value the future.</p>

</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#p9Lu9E4WZ9iVrFIrh665">Solution</button>

8) Find the optimal policy using value iteration given a discount rate of $\gamma = 0.99$. Why do the algorithm need more iterations compared to when $\gamma = 0.6$? Why are the average discounted costs higher compared to when $\gamma = 0.6$? Is it more costly to use this optimal policy?



### Exercise - Car rental {#ex-dp-rental}

Consider the car rental problem in Exercise \@ref(ex-mdp-2-car). Note the inventory dynamics (number of cars) at each parking lot is independent of the other given an action $a$. Let us consider Location 1 and assume that we are in state $x$ and chose action $a$.

The reward equals the reward of rentals minus the cost of movements. Note we have $\bar{x} = x - a$ and $\bar{y} = x + a$ after movement. Hence $$r(s,a) = \mathbb{E}[10(\min(D_1, \bar{x}) + \min(D_2, \bar{y}) )-2\mid a \mid]$$ 
where $$\mathbb{E}[\min(D, z)] = \sum_{i=0}^{z-1} i\Pr(D = i) + \Pr(D \geq z)z.$$

Let us make a reward function in R:


```r

# Mean of min(D,z). Assume z >= 0
meanMinD <- function(z, lambda) {
   if (z == 0) return(0)
   sum(dpois(0:(z-1), lambda) * 0:(z-1)) + ppois(z-1, lambda, lower.tail = F) * z
}

reward <- function(x, y, a) {
   10 * meanMinD(x-a, 3) + 10 * meanMinD(y+a, 4) - 2 * abs(a)
}

reward(20, 0, 0)
#> [1] 30
reward(20, 0, 5)
#> [1] 55.9
```

Let us have a look at the state transition, the number of cars after rental requests $\bar{x} - \min(D_1, \bar{x})$. Next, the number of returned cars are added: $\bar{x} - \min(D_1, \bar{x}) +  H_1$. Finally, note that if this number is above 20 (parking lot capacity), then we only have 20 cars, i.e. the inventory dynamics (number of cars at the end of the day) is $$X = \min(20,  \bar{x} - \min(D_1, \bar{x}) +  H_1))).$$ 
Similar for Location 2, if $\bar{y}= y+a$ we have $$Y = \min(20, \bar{y} - \min(D_2, \bar{y}) + H_2)).$$

Since the dynamics is independent given action a, the transition probabilities can be split: $$ p((x',y') | (x,y), a) = p(x' | x, a) p(y' | y, a).$$ 

Let us consider Location 1. If $x' < 20$ then 
$$
\begin{align} 
   p(x' | x, a) &= \Pr(x' = x-a - \min(D_1, x-a) +  H_1)\\ 
                &= \Pr(x' = \bar{x} - \min(D_1, \bar{x}) +  H_1)\\ 
                &= \Pr(H_1 - \min(D_1, \bar{x}) = x' - \bar{x}) \\ 
                &= \sum_{i=0}^{\bar{x}} \Pr(\min(D_1, \bar{x}) = i)\Pr(H_1 = x' - \bar{x} + i) \\ 
                &= \sum_{i=0}^{\bar{x}}\left( (\mathbf{1}_{(i<\bar{x})} \Pr(D_1 = i) + \mathbf{1}_{(i=\bar{x})} \Pr(D_1 \geq \bar{x}))\Pr(H_1 = x' - \bar{x} + i)\right) \\
                &= p(x' | \bar{x}).
\end{align}
$$ 

If $x' = 20$ then 
$$
\begin{align} 
   p(x' | x, a) &= \Pr(20 \leq \bar{x} - \min(D_1, \bar{x}) +  H_1)\\ 
                &= \Pr(H_1 - \min(D_1, \bar{x}) \geq 20 - \bar{x}) \\ 
                &= \sum_{i=0}^{\bar{x}} \Pr(\min(D_1, \bar{x}) = i)\Pr(H_1 \geq 20 - \bar{x} + i) \\ 
                &= \sum_{i=0}^{\bar{x}}\left( (\mathbf{1}_{(i<\bar{x})} \Pr(D_1 =i) + \mathbf{1}_{(i=\bar{x})} \Pr(D_1 \geq \bar{x}))\Pr(H_1 \geq 20 - \bar{x} + i)\right)\\
                &= p(x' = 20 | \bar{x}).
\end{align}
$$ 

Similar for Location 2. That is we need to calculate and store $p(x'| \bar{x})$ and $p(y'| \bar{y})$ to find $$ p((x',y') | (x,y), a) = p(x' | \bar{x} = x-a) p(y' | \bar{y} = y+a).$$ This is done in R using two matrices:


```r
library(tidyverse)
lD <- c(3,4)
lH <- c(3,2)

# Pr(min(D_i, k) = d)
# assume that d<=k
prMin <- function(i, k, d) {
   if (k == 0) return(1)
   v <- c(dpois(0:(k-1), lD[i]), ppois(k-1, lD[i], lower.tail = F))
   return(v[d+1])
   # if (d == k) return(1 - pD[i, d])     # 1-pr(D <= d-1)
   # if (d < k) return(dD[i, d+1])
   # return(NA)
}

# Pr(xN | x, a)
transPrX <- function(xN, xBar) {
   if (xN == 20) {
      return(sum(prMin(1, xBar, 0:(xBar)) * ppois(xN - xBar + 0:(xBar) - 1, lH[1], lower.tail = F)))
   }
   if (xN < 20) {
      return(sum(prMin(1, xBar, 0:(xBar)) * dpois(xN - xBar + 0:(xBar), lH[1])))
   }
   error("Error in calculations!")
}

# Pr(yN | y, a)
transPrY <- function(yN, yBar) {
   if (yN == 20) {
      return(sum(prMin(2, yBar, 0:(yBar)) * ppois(yN - yBar + 0:(yBar) - 1, lH[2], lower.tail = F)))
   }
   if (yN < 20) {
      return(sum(prMin(2, yBar, 0:(yBar)) * dpois(yN - yBar + 0:(yBar), lH[2])))
   }
   error("Error in calculations!")
}

mat <- matrix(0, nrow = 21, ncol = 21)
colnames(mat) <- str_c(0:20)
rownames(mat) <- str_c(0:20)
matTransPrX <- mat     # matTransPrX[r,c] = Pr(xN = c-1 | xBar = r-1)
matTransPrY <- mat
for (r in 1:21) {
   xBar <- r-1
   for (c in 1:21) {
      xN <- c-1
      matTransPrX[r,c] <- transPrX(xN, xBar)
      matTransPrY[r,c] <- transPrY(xN, xBar)
   }
}
# check
rowSums(matTransPrX)
#>  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
#>  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
rowSums(matTransPrY)
#>  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
#>  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
```



<!-- Q1 -->



<div class="modal fade bs-example-modal-lg" id="yO6vQjOVgI7eMTHLncYg" tabindex="-1" role="dialog" aria-labelledby="yO6vQjOVgI7eMTHLncYg-title"><div class="modal-dialog modal-lg" role="document"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="yO6vQjOVgI7eMTHLncYg-title">Hint</h4></div><div class="modal-body">

```r
mdp <- MDPClass$new()      # initialize mdp object
states <- expand_grid(x = 0:20, y = 0:20) %>% 
   mutate (state = str_c(x,",",y)) %>% 
   pull(state)
mdp$addStateSpace(states)   # add states (states are keys in a hash and are always converted to strings)
for (s in mdp$getStateKeys()) {  # add actions for each state (only the key of the action)
    x <- str_split(s, ",", simplify = T)
    y <- as.numeric(x[2])
    x <- as.numeric(x[1])
    ___
    mdp$addActionSpace(s, a)
}
# check some of the keys
mdp$getActionKeys("6,7")
mdp$getActionKeys("0,3")
mdp$getActionKeys("0,0")
mdp$getActionKeys("20,20")
mdp$getStateKeys()

# Add trans pr and expected reward to the actions. This may take some time!!
states <- expand_grid(x = 0:20, y = 0:20) %>% 
   mutate (state = str_c(x,",",y), pr = 0) 
for (s in mdp$getStateKeys()) {
   for (aS in mdp$getActionKeys(s)) {
      x <- str_split(s, ",", simplify = T)
      y <- as.numeric(x[2])
      x <- as.numeric(x[1])
      a <- as.numeric(aS) 
      xBar <- x-a
      yBar <- y+a
      ___
      mdp$addAction(s, aS, r = reward(x, y, a), pr = transPr)
   }
}
# check some of the keys
mdp$getActionInfo("20,20")
sum(mdp$getActionInfo("20,20")$"0"$pr)
mdp$getActionInfo("20,0")
sum(mdp$getActionInfo("20,20")$"0"$pr)
```


</div><div class="modal-footer"><button class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div><button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#yO6vQjOVgI7eMTHLncYg">Hint</button>

1) Build the model in R.



<!-- Q2 -->


2) Solve the problem using policy iteration with a discount rate $\gamma = 0.5$ and threshold parameter $\theta = 0.9$. What is the optimal action in state $(1,15)$? What is the expected total discounted reward of being in state $(10,3)$? How do theta affect the state-values? Make a plot of the optimal policy with $x$ on the y-axis and $y$ on the x-axis, plotting the action. 



<!-- Q3 -->


3) Solve the problem using value iteration. Check if the policy and state-values are the same as in Question 1. If not why could this be okay?



<!-- Q4 -->


4) Solve the problem using policy iteration with a discount rate $\gamma = 0.9$ and threshold parameter $\theta = 0.9$. Why are the state-values higher now?





[BSS]: https://bss.au.dk/en/
[bi-programme]: https://kandidat.au.dk/en/businessintelligence/

[course-help]: https://github.com/bss-osca/rl/issues
[cran]: https://cloud.r-project.org
[cheatsheet-readr]: https://rawgit.com/rstudio/cheatsheets/master/data-import.pdf
[course-welcome-to-the-tidyverse]: https://github.com/rstudio-education/welcome-to-the-tidyverse

[DataCamp]: https://www.datacamp.com/
[datacamp-signup]: https://www.datacamp.com/groups/shared_links/cbaee6c73e7d78549a9e32a900793b2d5491ace1824efc1760a6729735948215
[datacamp-r-intro]: https://learn.datacamp.com/courses/free-introduction-to-r
[datacamp-r-rmarkdown]: https://campus.datacamp.com/courses/reporting-with-rmarkdown
[datacamp-r-communicating]: https://learn.datacamp.com/courses/communicating-with-data-in-the-tidyverse
[datacamp-r-communicating-chap3]: https://campus.datacamp.com/courses/communicating-with-data-in-the-tidyverse/introduction-to-rmarkdown
[datacamp-r-communicating-chap4]: https://campus.datacamp.com/courses/communicating-with-data-in-the-tidyverse/customizing-your-rmarkdown-report
[datacamp-r-intermediate]: https://learn.datacamp.com/courses/intermediate-r
[datacamp-r-intermediate-chap1]: https://campus.datacamp.com/courses/intermediate-r/chapter-1-conditionals-and-control-flow
[datacamp-r-intermediate-chap2]: https://campus.datacamp.com/courses/intermediate-r/chapter-2-loops
[datacamp-r-intermediate-chap3]: https://campus.datacamp.com/courses/intermediate-r/chapter-3-functions
[datacamp-r-intermediate-chap4]: https://campus.datacamp.com/courses/intermediate-r/chapter-4-the-apply-family
[datacamp-r-functions]: https://learn.datacamp.com/courses/introduction-to-writing-functions-in-r
[datacamp-r-tidyverse]: https://learn.datacamp.com/courses/introduction-to-the-tidyverse
[datacamp-r-strings]: https://learn.datacamp.com/courses/string-manipulation-with-stringr-in-r
[datacamp-r-dplyr]: https://learn.datacamp.com/courses/data-manipulation-with-dplyr
[datacamp-r-dplyr-bakeoff]: https://learn.datacamp.com/courses/working-with-data-in-the-tidyverse
[datacamp-r-ggplot2-intro]: https://learn.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2
[datacamp-r-ggplot2-intermediate]: https://learn.datacamp.com/courses/intermediate-data-visualization-with-ggplot2
[dplyr-cran]: https://CRAN.R-project.org/package=dplyr
[debug-in-r]: https://rstats.wtf/debugging-r-code.html

[google-form]: https://forms.gle/s39GeDGV9AzAXUo18
[google-grupper]: https://docs.google.com/spreadsheets/d/1DHxthd5AQywAU4Crb3hM9rnog2GqGQYZ2o175SQgn_0/edit?usp=sharing
[GitHub]: https://github.com/
[git-install]: https://git-scm.com/downloads
[github-actions]: https://github.com/features/actions
[github-pages]: https://pages.github.com/
[gh-rl-student]: https://github.com/bss-osca/rl-student
[gh-rl]: https://github.com/bss-osca/rl

[happy-git]: https://happygitwithr.com
[hg-install-git]: https://happygitwithr.com/install-git.html
[hg-why]: https://happygitwithr.com/big-picture.html#big-picture
[hg-github-reg]: https://happygitwithr.com/github-acct.html#github-acct
[hg-git-install]: https://happygitwithr.com/install-git.html#install-git
[hg-exist-github-first]: https://happygitwithr.com/existing-github-first.html
[hg-exist-github-last]: https://happygitwithr.com/existing-github-last.html
[hg-credential-helper]: https://happygitwithr.com/credential-caching.html
[hypothes.is]: https://web.hypothes.is/

[osca-programme]: https://kandidat.au.dk/en/operationsandsupplychainanalytics/

[Peergrade]: https://peergrade.io
[peergrade-signup]: https://app.peergrade.io/join
[point-and-click]: https://en.wikipedia.org/wiki/Point_and_click
[pkg-bookdown]: https://bookdown.org/yihui/bookdown/
[pkg-openxlsx]: https://ycphs.github.io/openxlsx/index.html
[pkg-ropensci-writexl]: https://docs.ropensci.org/writexl/
[pkg-jsonlite]: https://cran.r-project.org/web/packages/jsonlite/index.html

[R]: https://www.r-project.org
[RStudio]: https://rstudio.com
[rstudio-cloud]: https://rstudio.cloud/spaces/176810/join?access_code=LSGnG2EXTuzSyeYaNXJE77vP33DZUoeMbC0xhfCz
[r-cloud-mod12]: https://rstudio.cloud/spaces/176810/project/2963819
[r-cloud-mod13]: https://rstudio.cloud/spaces/176810/project/3020139
[r-cloud-mod14]: https://rstudio.cloud/spaces/176810/project/3020322
[r-cloud-mod15]: https://rstudio.cloud/spaces/176810/project/3020509
[r-cloud-mod16]: https://rstudio.cloud/spaces/176810/project/3026754
[r-cloud-mod17]: https://rstudio.cloud/spaces/176810/project/3034015
[r-cloud-mod18]: https://rstudio.cloud/spaces/176810/project/3130795
[r-cloud-mod19]: https://rstudio.cloud/spaces/176810/project/3266132
[rstudio-download]: https://rstudio.com/products/rstudio/download/#download
[rstudio-customizing]: https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio
[rstudio-key-shortcuts]: https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts
[rstudio-workbench]: https://www.rstudio.com/wp-content/uploads/2014/04/rstudio-workbench.png
[r-markdown]: https://rmarkdown.rstudio.com/
[ropensci-writexl]: https://docs.ropensci.org/writexl/
[r4ds-pipes]: https://r4ds.had.co.nz/pipes.html
[r4ds-factors]: https://r4ds.had.co.nz/factors.html
[r4ds-strings]: https://r4ds.had.co.nz/strings.html
[r4ds-iteration]: https://r4ds.had.co.nz/iteration.html


[stat-545]: https://stat545.com
[stat-545-functions-part1]: https://stat545.com/functions-part1.html
[stat-545-functions-part2]: https://stat545.com/functions-part2.html
[stat-545-functions-part3]: https://stat545.com/functions-part3.html
[slides-welcome]: https://bss-osca.github.io/rl/slides/00-rl_welcome.html
[slides-m1-3]: https://bss-osca.github.io/rl/slides/01-welcome_r_part.html
[slides-m4-5]: https://bss-osca.github.io/rl/slides/02-programming.html
[slides-m6-8]: https://bss-osca.github.io/rl/slides/03-transform.html
[slides-m9]: https://bss-osca.github.io/rl/slides/04-plot.html
[slides-m83]: https://bss-osca.github.io/rl/slides/05-joins.html
[sutton-notation]: https://bss-osca.github.io/rl/sutton-notation.pdf

[tidyverse-main-page]: https://www.tidyverse.org
[tidyverse-packages]: https://www.tidyverse.org/packages/
[tidyverse-core]: https://www.tidyverse.org/packages/#core-tidyverse
[tidyverse-ggplot2]: https://ggplot2.tidyverse.org/
[tidyverse-dplyr]: https://dplyr.tidyverse.org/
[tidyverse-tidyr]: https://tidyr.tidyverse.org/
[tidyverse-readr]: https://readr.tidyverse.org/
[tidyverse-purrr]: https://purrr.tidyverse.org/
[tidyverse-tibble]: https://tibble.tidyverse.org/
[tidyverse-stringr]: https://stringr.tidyverse.org/
[tidyverse-forcats]: https://forcats.tidyverse.org/
[tidyverse-readxl]: https://readxl.tidyverse.org
[tidyverse-googlesheets4]: https://googlesheets4.tidyverse.org/index.html
[tutorial-markdown]: https://commonmark.org/help/tutorial/
[tfa-course]: https://bss-osca.github.io/tfa/

[Udemy]: https://www.udemy.com/

[vba-yt-course1]: https://www.youtube.com/playlist?list=PLpOAvcoMay5S_hb2D7iKznLqJ8QG_pde0
[vba-course1-hello]: https://youtu.be/f42OniDWaIo

[vba-yt-course2]: https://www.youtube.com/playlist?list=PL3A6U40JUYCi4njVx59-vaUxYkG0yRO4m
[vba-course2-devel-tab]: https://youtu.be/awEOUaw9q58
[vba-course2-devel-editor]: https://youtu.be/awEOUaw9q58
[vba-course2-devel-project]: https://youtu.be/fp6PTbU7bXo
[vba-course2-devel-properties]: https://youtu.be/ks2QYKAd9Xw
[vba-course2-devel-hello]: https://youtu.be/EQ6tDWBc8G4

[video-install]: https://vimeo.com/415501284
[video-rstudio-intro]: https://vimeo.com/416391353
[video-packages]: https://vimeo.com/416743698
[video-projects]: https://vimeo.com/319318233
[video-r-intro-p1]: https://www.youtube.com/watch?v=vGY5i_J2c-c
[video-r-intro-p2]: https://www.youtube.com/watch?v=w8_XdYI3reU
[video-r-intro-p3]: https://www.youtube.com/watch?v=NuY6jY4qE7I
[video-subsetting]: https://www.youtube.com/watch?v=hWbgqzsQJF0&list=PLjTlxb-wKvXPqyY3FZDO8GqIaWuEDy-Od&index=10&t=0s
[video-datatypes]: https://www.youtube.com/watch?v=5AQM-yUX9zg&list=PLjTlxb-wKvXPqyY3FZDO8GqIaWuEDy-Od&index=10
[video-control-structures]: https://www.youtube.com/watch?v=s_h9ruNwI_0
[video-conditional-loops]: https://www.youtube.com/watch?v=2evtsnPaoDg
[video-functions]: https://www.youtube.com/watch?v=ffPeac3BigM
[video-tibble-vs-df]: https://www.youtube.com/watch?v=EBk6PnvE1R4
[video-dplyr]: https://www.youtube.com/watch?v=aywFompr1F4

[wiki-snake-case]: https://en.wikipedia.org/wiki/Snake_case
[wiki-camel-case]: https://en.wikipedia.org/wiki/Camel_case
[wiki-interpreted]: https://en.wikipedia.org/wiki/Interpreted_language
[wiki-literate-programming]: https://en.wikipedia.org/wiki/Literate_programming
[wiki-csv]: https://en.wikipedia.org/wiki/Comma-separated_values
[wiki-json]: https://en.wikipedia.org/wiki/JSON


