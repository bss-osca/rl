--- 
editor_options: 
  chunk_output_type: console
---


# About the course notes {#sec-intro}

```{r}
#| include: false
#| eval: false
quarto::quarto_preview("book")
quarto::quarto_preview_stop()
```

```{r, code = readLines("setup.R"), cache = FALSE, include=FALSE}
```


```{r include = FALSE, cache=FALSE}
# library(devtools)
library(DT)
library(rmarkdown)
#tagList(html_dependency_font_awesome())
```


This site contains course notes for the course "Reinforcement Learning for Business" held at [Aarhus BSS][BSS]. It consists of a set of learning modules. The course is an elective course mainly for the [Operations and Supply Chain Analytics][osca-programme] and [Business Intelligence][bi-programme] programme and intended to give you an introduction to Reinforcement Learning (RL). You can expect the site to be updated while the course runs. The date listed above is the last time the site was updated.

```{r, echo=FALSE}
link_slide_file_text("01", "intro")
```


## Learning outcomes

By the end of this module, you are expected to:

* Understand the prerequisites and the goals for the course.
* Have downloaded the textbook.
* Know how the course is organized.
<!-- * Installed R and RStudio. -->
* Have annotated the online notes.

The learning outcomes relate to the [overall learning goals](#sec-lg-course) number 3, 5 and 6 of the course.

<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->


## Purpose of the course

The purpose of this course is to give an introduction and knowledge about reinforcement learning (RL).

RL may be seen as

* An approach of modelling sequential decision making problems.
* An approach for learning good decision making under uncertainty from experience.
* Mathematical models for learning-based decision making.
* Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.
* Estimating and finding near optimal decisions of a stochastic process with sequential decision making. 
* A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.

RL can also be seen as a way of modelling intuition. As humans, we often learn by trial and error. For instance, when playing a game, our strategy is based on the game rules and what we have experienced works based on previous plays. In a RL setting, the system has specific states, actions and reward structure, that is, the rules of the game, and it is up to the agent how to solve the game, i.e. find actions that maximize the reward. Typically, the agent starts with totally random trials and finishes with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, RL is an effective way to find good actions. 

A classic RL example is the bandit problem: You are in a casino and want to choose one of many slot machines (one-armed bandits) in each round. However, you do not know the distribution of the payouts of the machines. In the beginning, you will probably just try out machines (exploration) and then, after some learning, you will prefer the best ones (exploitation). Now the problem arises that if you use a slot machine frequently, you will not be able to gain information about the others and may not even find the best machine (exploration-exploitation dilemma). RL focuses on finding a balance between exploration of uncharted territory and exploitation of current knowledge.

The course starts by giving a general overview over RL and introducing bandit problems. Next, the mathematical framework of Markov decision processes (MDPs) is given as a classical formalization of sequential decision making. In this case, actions influence not just immediate rewards, but also subsequent situations, or states, and therefore also future rewards. An MDP assumes that the dynamics of the underlying process and the reward structure are known explicitly by the decision maker. In the last part of the course, we go beyond the case of decision making in known environments and study RL methods for stochastic control. 


## Learning goals of the course {#sec-lg-course}

After having participated in the course, you must, in addition to achieving general academic skills, demonstrate: 

Knowledge of

   1.	RL for Bandit problems 
   2.	Markov decision processes and ways to optimize them
   3.	the exploration vs exploitation challenge in RL and approaches for addressing this challenge 
   4.	the role of policy evaluation with stochastic approximation in the context of RL

Skills to

   5.	define the key features of RL that distinguishes it from other machine learning techniques
   6.	discuss fundamental concepts in RL
   7.	describe the mathematical framework of Markov decision processes
   8.	formulate and solve Markov and semi-Markov decision processes for realistic problems with finite state space under different objectives
   9.	apply fundamental techniques, results and concepts of RL on selected RL problems.
   10.	given an application problem, decide if it should be formulated as a RL problem and define it formally (in terms of the state space, action space, dynamics and reward model)

Competences to

   11. identify areas where RL are valuable
   12. select and apply the appropriate RL model for a given business problem
   13. interpret and communicate the results from RL
   

## Reinforcement learning textbook

The course uses the free textbook *Reinforcement Learning: An Introduction* by @Sutton18. The book is essential reading for anyone wishing to learn the theory and practice of modern Reinforcement learning. Read the weekly readings before the lecture to understand the material better, and perform better in the course.

Sutton and Barto's book is the most cited publication in RL research, and is responsible for making RL accessible to people around the world. The new edition, released in 2018, offers improved notation and explanations, additional algorithms, advanced topics, and many new examples; and it's totally free. Just follow the citation link to download it. 


## Course organization

Each week considers a learning module. A learning module is related to either a chapter in the textbook, tutorials, cases or exercises. The learning path would typical be 

- Before lectures: Read the chapter in the textbook or tutorial.
- Lectures (at campus).
- After lectures: Module Exercises (in groups).

Lectures will not cover all the curriculum but focus on the main parts. In some weeks tutorials or cases are given and we focus on a specific RL problem.

Lecture notes also contains an appendix with different modules that may be helpful for you.
   



## Programming software

We use [Python] as programming software and it is assumed that you are familiar with using Python. [Python] is a programming language and free software environment. 

During this course we are going to use [Google Colab][Colab] which is a hosted [Jupyter] notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. That is, [Colab] runs in your browser and you do not have to install anything on your computer. With a [Jupyter] notebook you can weave you code together with text.

That said, you may need to run Python on your laptop for larger tasks. You can run Python from a terminal but in general you use an IDE (integrated development environment) such as [Positron], [PyCharm] or [VSCode] for running Python and to saving your work.  

It is assumed as a prerequisite that you know how to use Python. If you need a brush-up on your Python programming skills then have a look at Module @sec-python-setup. 

Even though Python will be used in the course, algorithms can also be coded in R if you prefer to it.

## Acknowledgements and license {- #ack} 

Materials are taken from various places:

* The notes are based on @Sutton18.
* Some notes are adopted from [Scott Jeen](https://github.com/enjeeneer/sutton_and_barto), 
[Bryn Elesedy](https://github.com/brynhayder/reinforcement_learning_an_introduction) and [Peter Goldsborough](https://github.com/peter-can-write/david-silver-rl-notes).
* Some slides are inspired by the [RL specialization](https://www.coursera.org/specializations/reinforcement-learning) at Coursera.
* Some exercises are taken from @Sutton18 and modified slightly. 

I would like to thank all for their inspiration. 

This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).



## Exercises (do before class) {#sec-intro-ex}

`r strExercises`

### Exercise - How to annotate {#sec-intro-ex-annotate}

The online course notes can be annotated using [hypothes.is]. You can create both private and public annotations. Collaborative annotation helps people connect to each other and what they’re reading, even when they’re keeping their distance. You may also use public notes to help indicate spell errors, unclear content etc. in the notes. 

<!-- <img src="img/annotate.png" align="right" alt="Hypothes.is icons" title="Hypothes.is icons" style = "float: right; margin: 0 0 0 15px;"> -->

   1) Sign-up at [hypothes.is]. If you are using Chrome you may also install the [Chrome extension](https://chrome.google.com/webstore/detail/hypothesis-web-pdf-annota/bjfhmglciegochdpefhhlphglcehbmek).
   2) Go back to this page and login in the upper right corner (there should be some icons e.g. `<`).
   3) Select some text and try to annotate it using both a private and public annotation (you may delete it again afterwards). 
   4) Go to the [slides for this module](https://bss-osca.github.io/rl/slides/01_intro-slides.html#6) and try to annotate the page with a private comment. 


### Exercise - Colab {#sec-intro-ex-colab}

During this course we are going to use [Google Colab][Colab] which is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. That is, [Colab] runs in your browser and you do not have to install anything on your computer.

To be familiar with Colab do the following:

   1) If you do not have a Google account [create one](https://accounts.google.com/). Note if you have a gMail then you already have a Google account.
   2) Open and do this [tutorial][colab-01-intro-colab].
   
   

```{r links, child="links.md"}
```
