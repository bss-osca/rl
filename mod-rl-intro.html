<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 1 An introduction to RL | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 1 An introduction to RL | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bss-osca.github.io/rl/" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 1 An introduction to RL | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html"/>
<link rel="next" href="r-setup.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.21/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});
</script>
<script src="https://hypothes.is/embed.js" async></script>
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li><a href="index.html#mod-intro">About the course notes<span></span></a>
<ul>
<li><a href="index.html#learning-outcomes">Learning outcomes<span></span></a></li>
<li><a href="index.html#purpose-of-the-course">Purpose of the course<span></span></a></li>
<li><a href="index.html#learning-goals-of-the-course">Learning goals of the course<span></span></a></li>
<li><a href="index.html#reinforcement-learning-textbook">Reinforcement learning textbook<span></span></a></li>
<li><a href="index.html#course-organization">Course organization<span></span></a></li>
<li><a href="index.html#programming-software">Programming software<span></span></a></li>
<li><a href="index.html#ack">Acknowledgements and license<span></span></a></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)<span></span></a></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.11</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-vba-hello"><i class="fa fa-check"></i><b>1.11.1</b> Exercise - Hello<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="r-setup.html"><a href="r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R<span></span></a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups<span></span></a>
<ul>
<li><a href="groups.html#joint-r-project-structure">Joint R project structure<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code<span></span></a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes<span></span></a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help<span></span></a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals<span></span></a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon<span></span></a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-rl-intro" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Module 1</span> An introduction to RL<a href="mod-rl-intro.html#mod-rl-intro" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This module gives a short introduction to Reinforcement learning.</p>
<div id="mod-rl-intro-lo" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Learning outcomes<a href="mod-rl-intro.html#mod-rl-intro-lo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<!-- * Describe what VBA is. -->
<!-- * Setup Excel for VBA. -->
<!-- * Know how the macro recorder works. -->
<!-- * Make your first program. -->
<!-- * Have an overview over what VBA can do. -->
<!-- * Recorded you first macro using the macro recorder -->
<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Textbook readings<a href="mod-rl-intro.html#textbook-readings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 1 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module.</p>
</div>
<div id="what-is-reinforcement-learning" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> What is reinforcement learning<a href="mod-rl-intro.html#what-is-reinforcement-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>RL can be seen as</p>
<ul>
<li>An approach of modelling sequential decision making problems.</li>
<li>An approach for learning good decision making under uncertainty from experience.</li>
<li>Mathematical models for learning-based decision making.</li>
<li>Trying to optimize decisions in a sequential decision model. That is, making a good sequence of decisions.</li>
<li>Estimating and finding near optimal decisions of a stochastic process with sequential decision making.</li>
<li>A model where given a state of a system, the agent wants to take actions to maximize future reward. Often the agent does not know the underlying setting and, thus, is bound to learn from experience.</li>
</ul>
<p>Sequential decision problems are problems where you take decisions/actions over time. As an agent, you base your decision on the current state of the system (a state is a function of the information/data available). At the next time-step, the system have moved (stochastically) to the next stage. Here new information may be available and you receive a reward and take a new action. Examples of sequential decision problems are (with possible actions):</p>
<ul>
<li>Playing backgammon (how to move the checkers).</li>
<li><a href="https://arxiv.org/pdf/1807.00412.pdf">Driving a car</a> (left, right, forward, back, break, stop, …).</li>
<li>How to <a href="https://medium.com/ibm-data-ai/reinforcement-learning-the-business-use-case-part-2-c175740999">invest/maintain a portfolio of stocks</a> (buy, sell, amount).<br />
</li>
<li><a href="https://www.youtube.com/watch?v=pxWkg2N0l9c">Control an inventory</a> (wait, buy, amount).</li>
<li>Vehicle routing (routes).</li>
<li>Maintain a spare-part (wait, maintain).</li>
<li><a href="https://arxiv.org/pdf/2103.14295.pdf">Robot operations</a> (sort, move, …)</li>
<li><a href="http://dx.doi.org/10.1016/j.ejor.2019.01.050">Dairy cow treatment/replacement</a> (treat, replace, …)</li>
<li>Recommender systems e.g. <a href="https://scale.com/blog/Netflix-Recommendation-Personalization-TransformX-Scale-AI-Insights">Netflix recommendations</a> (videos)</li>
</ul>
<p>Since RL involves a scalar reward signal, the goal is to choose actions such that the total reward is maximized. Note actions have an impact on the future and may have long term consequences. As such, you cannot simply choose the action that maximize the current reward. It may, in fact, be better to sacrifice immediate reward to gain more long term reward.</p>
<p>RL can be seen as a way of modelling intuition. An RL model has specific states, actions and reward structure and our goal as an agent is to find good decisions/actions that maximize the total reward. The agent learn using, for instance:</p>
<ul>
<li>totally random trials (in the start),</li>
<li>sophisticated tactics and superhuman skills (in the end).</li>
</ul>
<p>That is, as the agent learn, the reward estimate of a given action becomes better.</p>
<p>As humans, we often learn by trial and error too:</p>
<ul>
<li>Learning to walk (by falling/pain).</li>
<li>Learning to play (strategy is based on the game rules and what we have experienced works based on previous plays).</li>
</ul>
<p>This can also be seen as learning the reward of our actions.</p>
</div>
<div id="rl-and-business-analytics" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> RL and Business Analytics<a href="mod-rl-intro.html#rl-and-business-analytics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="https://en.wikipedia.org/wiki/Business_analytics">Business Analytics</a> (BA) (or just <a href="http://connect.informs.org/analytics/home">Analytics</a>) refers to the scientific process of transforming data into insight for making better decisions in business. BA can both be seen as the complete decision making process for solving a business problem or as a set of methodologies that enable the creation of business value. As a process it can be characterized by descriptive, predictive, and prescriptive model building using “big” data sources.</p>
<p><strong>Descriptive Analytics</strong>: A set of technologies and processes that use data to understand and analyze business performance. Descriptive analytics are the most commonly used and most well understood type of analytics. Descriptive analytics categorizes, characterizes, consolidates, and classifies data. Examples are standard reporting and dashboards (KPIs, what happened or is happening now?) and ad-hoc reporting (how many/often?). Descriptive analytics often serves as a first step in the successful application of predictive or prescriptive analytics.</p>
<p><strong>Predictive Analytics</strong>: The use of data and statistical techniques to make predictions about future outputs/outcomes, identify patterns or opportunities for business performance. Examples of techniques are data mining (what data is correlated with other data?), pattern recognition and alerts (when should I take action to correct/adjust a spare part?), Monte-Carlo simulation (what could happen?), neural networks (which customer group are best?) and forecasting (what if these trends continue?).</p>
<p><strong>Prescriptive Analytics</strong>: The use of optimization and other decision modelling techniques using the results of descriptive and predictive analytics to suggest decision options with the goal of improving business performance. Prescriptive analytics attempt to quantify the effect of future decisions in order to advise on possible outcomes before the decisions are actually made. Prescriptive analytics predicts not only what will happen, but also why it will happen and provides recommendations regarding actions that will take advantage of the predictions. Prescriptive analytics are relatively complex to administer, and most companies are not yet using it in their daily course of business. However, when implemented correctly, it can have a huge impact on business performance and how businesses make decisions. Examples on prescriptive analytics are optimization in production planning and scheduling, inventory management, the supply chain and transportation planning. Since reinforcement learning focus optimizing decisions it is Prescriptive Analytics.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:analytics"></span>
<img src="img/analytics_plot9.png" alt="Business Analytics and competive advantage."  />
<p class="caption">
Figure 1.1: Business Analytics and competive advantage.
</p>
</div>
<p>Companies who use BA focus on fact-based management to drive decision making and treats data and information as a strategic asset that is shared within the company. This enterprise approach generates a companywide respect for applying descriptive, predictive and prescriptive analytics in areas such as supply chain, marketing and human resources. Focusing on BA gives a company a competive advantage (see Figure <a href="mod-rl-intro.html#fig:analytics">1.1</a>).</p>
<p><strong>BA and related areas</strong>: In the past <em>Business Intelligence</em> traditionally focuses on querying, reporting, online analytical processing, i.e. descriptive analytics. However, a more modern definition of Business Intelligence is the union of descriptive and predictive analytics. <em>Operations Research</em> or <em>Management Science</em> deals with the application of advanced analytical methods to help make better decisions and can hence be seen as prescriptive analytics. However, traditionally it has been taking a more theoretical approach and focusing on problem-driven research while BA takes a more data-driven approach. <em>Logistics</em> is a cross-functional area focusing on the effective and efficient flows of goods and services, and the related flows of information and cash. <em>Supply Chain Management</em> adds a process-oriented and cross-company perspective. Both can be seen as prescriptive analytics with a more problem-driven research focus. Advanced Analytics is often used as a classification of both predictive and prescriptive analytics. <em>Data science</em> is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured and can be seen as Business analytics applied to a wider range of data.</p>
</div>
<div id="rl-in-different-research-deciplines" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> RL in different research deciplines<a href="mod-rl-intro.html#rl-in-different-research-deciplines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>RL is used in many research fields using different names:</p>
<ul>
<li>RL (most used) originated from computer science and AI.</li>
<li><em>Approximate dynamic programming (ADP)</em> is mostly used within operations research.</li>
<li><em>Neuro-dynamic programming</em> (when states are represented using a neural network).</li>
<li>RL is closely related to <em>Markov decision processes</em> (a mathematical model for a sequential decision problem).</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="img/rl-names.png" alt="Adopted from @Silver15." width="70%" />
<p class="caption">
Figure 1.2: Adopted from <span class="citation">Silver (<a href="#ref-Silver15" role="doc-biblioref">2015</a>)</span>.
</p>
</div>
</div>
<div id="rl-and-machine-learning" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> RL and machine learning<a href="mod-rl-intro.html#rl-and-machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Different ways of learning:</p>
<ul>
<li><strong>Supervised learning:</strong> Given data <span class="math inline">\((x_i, y_i)\)</span> learn to predict <span class="math inline">\(y\)</span> from <span class="math inline">\(x\)</span>, i.e. find <span class="math inline">\(y \approx f(x)\)</span> (e.g. regression).</li>
<li><strong>Unsupervised learning:</strong> Given data <span class="math inline">\((x_i)\)</span> learn patterns using <span class="math inline">\(x\)</span>, i.e. find <span class="math inline">\(f(x)\)</span> (e.g. clustering).
<!-- * Often assume that data are independent and identically distributed (iid).  --></li>
<li><strong>RL:</strong> Given state <span class="math inline">\(x\)</span> you take an action and observe the reward <span class="math inline">\(r\)</span> and the new state <span class="math inline">\(x&#39;\)</span>.
<ul>
<li>There is no supervisor <span class="math inline">\(y\)</span>, only a reward signal <span class="math inline">\(r\)</span>.</li>
<li>Your goal is to find a policy that optimize the total reward function.</li>
</ul></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="img/rl-ml.png" alt="Adopted from @Silver15." width="70%" />
<p class="caption">
Figure 1.3: Adopted from <span class="citation">Silver (<a href="#ref-Silver15" role="doc-biblioref">2015</a>)</span>.
</p>
</div>
</div>
<div id="the-rl-data-stream" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> The RL data-stream<a href="mod-rl-intro.html#the-rl-data-stream" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>RL considers an agent in an environment:</p>
<ul>
<li>Agent: The one who takes the action (computer, robot, decision maker).</li>
<li>Environment: The system/world where observations and rewards are found.</li>
</ul>
<p>Data are revealed sequentially as you take actions <span class="math display">\[(O_0, A_0, R_1, O_1, A_1, R_2, O_2, \ldots).\]</span> At time <span class="math inline">\(t\)</span> the agent have been taken action <span class="math inline">\(A_{t-1}\)</span> and observed observation <span class="math inline">\(O_t\)</span> and reward <span class="math inline">\(R_t\)</span>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="rl_files/figure-html/unnamed-chunk-8-1.png" alt="Agent-environment representation." width="672" />
<p class="caption">
Figure 1.4: Agent-environment representation.
</p>
</div>
<p>This gives us the <em>history</em> at time <span class="math inline">\(t\)</span> is the sequence of observations, actions and rewards <span class="math display">\[H_t = (O_0, A_0, R_1, O_1, \ldots, A_{t-1}, R_t, O_t).\]</span></p>
</div>
<div id="states-actions-rewards-and-policies" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> States, actions, rewards and policies<a href="mod-rl-intro.html#states-actions-rewards-and-policies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The (agent) state <span class="math inline">\(S_t\)</span> is the information used to take the next action <span class="math inline">\(A_t\)</span>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="rl_files/figure-html/unnamed-chunk-9-1.png" alt="State and action." width="672" />
<p class="caption">
Figure 1.5: State and action.
</p>
</div>
<p>A state depends on the history, i.e. a state is a function of the history <span class="math inline">\(S_t = f(H_t)\)</span>. Different strategies for defining a state may be considered. Choosing <span class="math inline">\(S_t = H_t\)</span> is bad since the size of a state representation grows very fast. A better strategy is to just store the information needed for taking the next action. Moreover, it is good to have Markov states where given the present state the future is independent of the past. That is, the current state holds just as much information as the history, i.e. it holds all useful information of the history. Symbolically, we call a state <span class="math inline">\(S_t\)</span> Markov iff</p>
<p><span class="math display">\[\Pr[S_{t+1} | S_t] = \Pr[S_{t+1} | S_1,...,S_t].\]</span></p>
<p>That is, the probability of seeing some next state <span class="math inline">\(S_{t+1}\)</span> given the current state is exactly equal to the probability of that next state given the entire history of states. Note that we can always find some Markov state. Though the smaller the state, the more “valuable” it is. In the worst case, <span class="math inline">\(H_t\)</span> is Markov, since it represents all known information about itself.</p>
<p>The reward <span class="math inline">\(R_t\)</span> is a number representing the reward at time <span class="math inline">\(t\)</span> (negative if a cost). Examples of rewards are</p>
<ul>
<li>Playing backgammon (0 (when play), 1 (when win), -1 (when loose)).</li>
<li>How to invest/maintain a portfolio of stocks (the profit).<br />
</li>
<li>Control an inventory (inventory cost, lost sales cost).</li>
<li>Vehicle routing (transportation cost).</li>
</ul>
<p>The goal is to find a policy that maximize the total future reward. A <em>policy</em> is the agent’s behaviour and is a map from state to action, i.e. a function <span class="math display">\[a = \pi(s)\]</span> saying that given the agent is in state <span class="math inline">\(s\)</span> we choose action <span class="math inline">\(a\)</span>.</p>
<p>The total future reward is a currently not defined clearly. Let the <em>value function</em> denote the future reward in state <span class="math inline">\(s\)</span> and define it as the expected discounted future reward: <span class="math display">\[V_\pi(s) = \mathbb{E}_\pi(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S = s).\]</span> Note the value function is defined using a specific policy and the goal is to find a policy that maximize the total future reward in all possible states. The value of the discount factor is important:</p>
<ul>
<li>Discount factor <span class="math inline">\(\gamma=0\)</span>: Only care about present reward.</li>
<li>Discount factor <span class="math inline">\(\gamma=1\)</span>: Future reward is as beneficial as immediate reward. Can be used if the time-horizon is finite.</li>
<li>Discount factor <span class="math inline">\(\gamma&lt;1\)</span>: Rewards near to the present more beneficial. Note <span class="math inline">\(V(s)\)</span> will converge to a number even if the time-horizon is infinite.</li>
</ul>
<p>The policy that maximize the total future reward given state <span class="math inline">\(s\)</span> is: <span class="math display">\[\pi^* = \arg\max_{\pi\in\Pi}(V_\pi(s)).\]</span></p>
<!-- ## Model free vs Model based -->
</div>
<div id="exploitation-vs-exploration" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Exploitation vs Exploration<a href="mod-rl-intro.html#exploitation-vs-exploration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A key problem of reinforcement learning (in general) is the difference between exploration and exploitation. Should the agent sacrifice what is currently know as the best action to explore a (possibly) better opportunity, or should it just exploit its best possible policy? <em>Exploitation</em> takes the action assumed to be optimal with respect to the data observed so far. This, gives better predictions of the value function (given the current policy) but prevents the agent from discovering potential better decisions (a better policy). <em>Exploration</em> does not take the action that seems to be optimal. That is, the agent explore to find new states and update the value function for this state.</p>
<p>Examples in the exploration and exploitation dilemma are for instance movie recommendations: recommending the user’s best rated movie type (exploitation) or trying another movie type (exploration) or oil drilling: drilling at the best known location (exploitation) or trying a new location (exploration).</p>
</div>
<div id="rl-in-action-tic-tac-toe" class="section level2 hasAnchor" number="1.10">
<h2><span class="header-section-number">1.10</span> RL in action (Tic-Tac-Toe)<a href="mod-rl-intro.html#rl-in-action-tic-tac-toe" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="sec-rl-intro-ex" class="section level2 hasAnchor" number="1.11">
<h2><span class="header-section-number">1.11</span> Exercises<a href="mod-rl-intro.html#sec-rl-intro-ex" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Some of the solutions to each exercise can be seen by pressing the button at each question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="ex-vba-hello" class="section level3 hasAnchor" number="1.11.1">
<h3><span class="header-section-number">1.11.1</span> Exercise - Hello<a href="mod-rl-intro.html#ex-vba-hello" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Silver15" class="csl-entry">
Silver, David. 2015. <span>“Lectures on Reinforcement Learning.”</span> <a href="https://www.davidsilver.uk/teaching/">https://www.davidsilver.uk/teaching/</a>.
</div>
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r-setup.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/01_rl-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
