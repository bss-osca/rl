<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 6 Monte Carlo methods for prediction and control | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 6 Monte Carlo methods for prediction and control | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 6 Monte Carlo methods for prediction and control | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2025-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mod-dp.html"/>
<link rel="next" href="mod-td-pred.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the course notes</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#purpose-of-the-course"><i class="fa fa-check"></i>Purpose of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-goals-of-the-course"><i class="fa fa-check"></i>Learning goals of the course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reinforcement-learning-textbook"><i class="fa fa-check"></i>Reinforcement learning textbook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-organization"><i class="fa fa-check"></i>Course organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programming-software"><i class="fa fa-check"></i>Programming software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#ack"><i class="fa fa-check"></i>Acknowledgements and license</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex"><i class="fa fa-check"></i>Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-annotate"><i class="fa fa-check"></i>Exercise - How to annotate</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sec-intro-ex-templates"><i class="fa fa-check"></i>Exercise - Templates</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL</b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings</a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning</a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics</a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines</a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning</a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream</a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies</a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration</a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play</a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay</a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#summary"><i class="fa fa-check"></i><b>1.11</b> Summary</a></li>
<li class="chapter" data-level="1.12" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.12</b> Exercises</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.12.1</b> Exercise - Self-Play</a></li>
<li class="chapter" data-level="1.12.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-sym"><i class="fa fa-check"></i><b>1.12.2</b> Exercise - Symmetries</a></li>
<li class="chapter" data-level="1.12.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-greedy"><i class="fa fa-check"></i><b>1.12.3</b> Exercise - Greedy Play</a></li>
<li class="chapter" data-level="1.12.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-exploit"><i class="fa fa-check"></i><b>1.12.4</b> Exercise - Learning from Exploration</a></li>
<li class="chapter" data-level="1.12.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-other"><i class="fa fa-check"></i><b>1.12.5</b> Exercise - Other Improvements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods</b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings</a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem</a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action</a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-step-size"><i class="fa fa-check"></i><b>2.5</b> The role of the step-size</a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.6</b> Optimistic initial values</a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#upper-confidence-bound-action-selection"><i class="fa fa-check"></i><b>2.7</b> Upper-Confidence Bound Action Selection</a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#summary-1"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
<li class="chapter" data-level="2.9" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.9</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-adv"><i class="fa fa-check"></i><b>2.9.1</b> Exercise - Advertising</a></li>
<li class="chapter" data-level="2.9.2" data-path="mod-bandit.html"><a href="mod-bandit.html#ex-bandit-coin"><i class="fa fa-check"></i><b>2.9.2</b> Exercise - A coin game</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html"><i class="fa fa-check"></i><b>3</b> Markov decision processes (MDPs)</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#learning-outcomes-2"><i class="fa fa-check"></i><b>3.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="3.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#textbook-readings-2"><i class="fa fa-check"></i><b>3.2</b> Textbook readings</a></li>
<li class="chapter" data-level="3.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#an-mdp-as-a-model-for-the-agent-environment"><i class="fa fa-check"></i><b>3.3</b> An MDP as a model for the agent-environment</a></li>
<li class="chapter" data-level="3.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#rewards-and-the-objective-function-goal"><i class="fa fa-check"></i><b>3.4</b> Rewards and the objective function (goal)</a></li>
<li class="chapter" data-level="3.5" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#summary-2"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
<li class="chapter" data-level="3.6" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#sec-mdp-1-ex"><i class="fa fa-check"></i><b>3.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-seq"><i class="fa fa-check"></i><b>3.6.1</b> Exercise - Sequential decision problems</a></li>
<li class="chapter" data-level="3.6.2" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-exp-return"><i class="fa fa-check"></i><b>3.6.2</b> Exercise - Expected return</a></li>
<li class="chapter" data-level="3.6.3" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-gambler"><i class="fa fa-check"></i><b>3.6.3</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="3.6.4" data-path="mod-mdp-1.html"><a href="mod-mdp-1.html#ex-mdp-1-storage"><i class="fa fa-check"></i><b>3.6.4</b> Exercise - Factory storage</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html"><i class="fa fa-check"></i><b>4</b> Policies and value functions for MDPs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#learning-outcomes-3"><i class="fa fa-check"></i><b>4.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="4.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#textbook-readings-3"><i class="fa fa-check"></i><b>4.2</b> Textbook readings</a></li>
<li class="chapter" data-level="4.3" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#policies-and-value-functions"><i class="fa fa-check"></i><b>4.3</b> Policies and value functions</a></li>
<li class="chapter" data-level="4.4" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-opt"><i class="fa fa-check"></i><b>4.4</b> Optimal policies and value functions</a></li>
<li class="chapter" data-level="4.5" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#optimality-vs-approximation"><i class="fa fa-check"></i><b>4.5</b> Optimality vs approximation</a></li>
<li class="chapter" data-level="4.6" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#semi-mdps-non-fixed-time-length"><i class="fa fa-check"></i><b>4.6</b> Semi-MDPs (non-fixed time length)</a></li>
<li class="chapter" data-level="4.7" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#summary-3"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
<li class="chapter" data-level="4.8" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#sec-mdp-2-ex"><i class="fa fa-check"></i><b>4.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-policy"><i class="fa fa-check"></i><b>4.8.1</b> Exercise - Optimal policy</a></li>
<li class="chapter" data-level="4.8.2" data-path="mod-mdp-2.html"><a href="mod-mdp-2.html#ex-mdp-2-car"><i class="fa fa-check"></i><b>4.8.2</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mod-dp.html"><a href="mod-dp.html"><i class="fa fa-check"></i><b>5</b> Dynamic programming</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mod-dp.html"><a href="mod-dp.html#learning-outcomes-4"><i class="fa fa-check"></i><b>5.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="5.2" data-path="mod-dp.html"><a href="mod-dp.html#textbook-readings-4"><i class="fa fa-check"></i><b>5.2</b> Textbook readings</a></li>
<li class="chapter" data-level="5.3" data-path="mod-dp.html"><a href="mod-dp.html#sec-dp-pe"><i class="fa fa-check"></i><b>5.3</b> Policy evaluation</a></li>
<li class="chapter" data-level="5.4" data-path="mod-dp.html"><a href="mod-dp.html#policy-improvement"><i class="fa fa-check"></i><b>5.4</b> Policy Improvement</a></li>
<li class="chapter" data-level="5.5" data-path="mod-dp.html"><a href="mod-dp.html#policy-iteration"><i class="fa fa-check"></i><b>5.5</b> Policy Iteration</a></li>
<li class="chapter" data-level="5.6" data-path="mod-dp.html"><a href="mod-dp.html#value-iteration"><i class="fa fa-check"></i><b>5.6</b> Value Iteration</a></li>
<li class="chapter" data-level="5.7" data-path="mod-dp.html"><a href="mod-dp.html#generalized-policy-iteration"><i class="fa fa-check"></i><b>5.7</b> Generalized policy iteration</a></li>
<li class="chapter" data-level="5.8" data-path="mod-dp.html"><a href="mod-dp.html#exe-dp-storage"><i class="fa fa-check"></i><b>5.8</b> Example - Factory Storage</a></li>
<li class="chapter" data-level="5.9" data-path="mod-dp.html"><a href="mod-dp.html#summary-4"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
<li class="chapter" data-level="5.10" data-path="mod-dp.html"><a href="mod-dp.html#exercises"><i class="fa fa-check"></i><b>5.10</b> Exercises</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-gambler"><i class="fa fa-check"></i><b>5.10.1</b> Exercise - Gambler’s problem</a></li>
<li class="chapter" data-level="5.10.2" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-maintain"><i class="fa fa-check"></i><b>5.10.2</b> Exercise - Maintenance problem</a></li>
<li class="chapter" data-level="5.10.3" data-path="mod-dp.html"><a href="mod-dp.html#ex-dp-rental"><i class="fa fa-check"></i><b>5.10.3</b> Exercise - Car rental</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mod-mc.html"><a href="mod-mc.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo methods for prediction and control</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mod-mc.html"><a href="mod-mc.html#learning-outcomes-5"><i class="fa fa-check"></i><b>6.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="6.2" data-path="mod-mc.html"><a href="mod-mc.html#textbook-readings-5"><i class="fa fa-check"></i><b>6.2</b> Textbook readings</a></li>
<li class="chapter" data-level="6.3" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-evaluation"><i class="fa fa-check"></i><b>6.3</b> MC prediction (evaluation)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="mod-mc.html"><a href="mod-mc.html#mc-prediction-of-action-values"><i class="fa fa-check"></i><b>6.3.1</b> MC prediction of action-values</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="mod-mc.html"><a href="mod-mc.html#mc-control-improvement"><i class="fa fa-check"></i><b>6.4</b> MC control (improvement)</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="mod-mc.html"><a href="mod-mc.html#gpi-with-exploring-starts"><i class="fa fa-check"></i><b>6.4.1</b> GPI with exploring starts</a></li>
<li class="chapter" data-level="6.4.2" data-path="mod-mc.html"><a href="mod-mc.html#gpi-using-epsilon-soft-policies"><i class="fa fa-check"></i><b>6.4.2</b> GPI using <span class="math inline">\(\epsilon\)</span>-soft policies</a></li>
<li class="chapter" data-level="6.4.3" data-path="mod-mc.html"><a href="mod-mc.html#gpi-using-upper-confience-bound-action-selection"><i class="fa fa-check"></i><b>6.4.3</b> GPI using upper-confience bound action selection</a></li>
<li class="chapter" data-level="6.4.4" data-path="mod-mc.html"><a href="mod-mc.html#mc-seasonal"><i class="fa fa-check"></i><b>6.4.4</b> Example - Seasonal inventory and sales planning</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mod-mc.html"><a href="mod-mc.html#sec-mc-off-policy"><i class="fa fa-check"></i><b>6.5</b> Off-policy MC prediction</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="mod-mc.html"><a href="mod-mc.html#weighted-importance-sampling"><i class="fa fa-check"></i><b>6.5.1</b> Weighted importance sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="mod-mc.html"><a href="mod-mc.html#off-policy-control-improvement"><i class="fa fa-check"></i><b>6.6</b> Off-policy control (improvement)</a></li>
<li class="chapter" data-level="6.7" data-path="mod-mc.html"><a href="mod-mc.html#summary-5"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
<li class="chapter" data-level="6.8" data-path="mod-mc.html"><a href="mod-mc.html#exercises-1"><i class="fa fa-check"></i><b>6.8</b> Exercises</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="mod-mc.html"><a href="mod-mc.html#ex-mc-seasonal"><i class="fa fa-check"></i><b>6.8.1</b> Exercise - Seasonal inventory and sales planning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mod-td-pred.html"><a href="mod-td-pred.html"><i class="fa fa-check"></i><b>7</b> Temporal difference methods for prediction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#learning-outcomes-6"><i class="fa fa-check"></i><b>7.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="7.2" data-path="mod-td-pred.html"><a href="mod-td-pred.html#textbook-readings-6"><i class="fa fa-check"></i><b>7.2</b> Textbook readings</a></li>
<li class="chapter" data-level="7.3" data-path="mod-td-pred.html"><a href="mod-td-pred.html#what-is-td-learning"><i class="fa fa-check"></i><b>7.3</b> What is TD learning?</a></li>
<li class="chapter" data-level="7.4" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction"><i class="fa fa-check"></i><b>7.4</b> TD prediction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#td-prediction-for-action-values"><i class="fa fa-check"></i><b>7.4.1</b> TD prediction for action-values</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mod-td-pred.html"><a href="mod-td-pred.html#benefits-of-td-methods"><i class="fa fa-check"></i><b>7.5</b> Benefits of TD methods</a></li>
<li class="chapter" data-level="7.6" data-path="mod-td-pred.html"><a href="mod-td-pred.html#exercises-2"><i class="fa fa-check"></i><b>7.6</b> Exercises</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="mod-td-pred.html"><a href="mod-td-pred.html#ex-td-pred-random"><i class="fa fa-check"></i><b>7.6.1</b> Exercise - A randow walk</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mod-td-control.html"><a href="mod-td-control.html"><i class="fa fa-check"></i><b>8</b> Temporal difference methods for control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mod-td-control.html"><a href="mod-td-control.html#learning-outcomes-7"><i class="fa fa-check"></i><b>8.1</b> Learning outcomes</a></li>
<li class="chapter" data-level="8.2" data-path="mod-td-control.html"><a href="mod-td-control.html#textbook-readings-7"><i class="fa fa-check"></i><b>8.2</b> Textbook readings</a></li>
<li class="chapter" data-level="8.3" data-path="mod-td-control.html"><a href="mod-td-control.html#sarsa---on-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.3</b> SARSA - On-policy GPI using TD</a></li>
<li class="chapter" data-level="8.4" data-path="mod-td-control.html"><a href="mod-td-control.html#q-learning---off-policy-gpi-using-td"><i class="fa fa-check"></i><b>8.4</b> Q-learning - Off-policy GPI using TD</a></li>
<li class="chapter" data-level="8.5" data-path="mod-td-control.html"><a href="mod-td-control.html#expected-sarsa---gpi-using-td"><i class="fa fa-check"></i><b>8.5</b> Expected SARSA - GPI using TD</a></li>
<li class="chapter" data-level="8.6" data-path="mod-td-control.html"><a href="mod-td-control.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
<li class="chapter" data-level="8.7" data-path="mod-td-control.html"><a href="mod-td-control.html#exercises-3"><i class="fa fa-check"></i><b>8.7</b> Exercises</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="mod-td-control.html"><a href="mod-td-control.html#ex-td-control-storage"><i class="fa fa-check"></i><b>8.7.1</b> Exercise - Factory Storage</a></li>
<li class="chapter" data-level="8.7.2" data-path="mod-td-control.html"><a href="mod-td-control.html#ex-td-control-car"><i class="fa fa-check"></i><b>8.7.2</b> Exercise - Car Rental</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R</a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups</a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention</a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes</a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help</a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals</a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon</a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-mc" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Module 6</span> Monte Carlo methods for prediction and control<a href="mod-mc.html#mod-mc" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The term “Monte Carlo” (MC) is often used for an estimation method which involves a random component. MC methods of RL learn state and action values by sampling and averaging returns. MC do not use dynamics where we estimate the value in the current state using the value in the next state (like in dynamic programming). Instead the MC methods estimate the values by considering different <em>sample-paths</em> (state, action and reward realizations). Compared to a Markov decision process, MC methods are model-free since they not require full knowledge of the transition probabilities and rewards (a model of the environment) instead MC methods learn the value function directly from experience. Often though, the sample-path is generated using simulation, i.e. some knowledge about the environment is given, but it is only used to generate sample transitions. For instance, consider an MDP model for the game Blackjack. Here calculating all the transition probabilities may be tedious and error-prone in terms of coding and numerical precision. Instead we can simulate a game (a sample-path) and use the simulations to evaluate/predict the value function of a policy and then use control to find a good policy. That is, we still use a generalised policy iteration framework, but instead of computing the value function using the MDP model a priori, we learn it from experience.</p>
<p>MC methods can be used for processes with episodes, i.e. where there is a terminal state. This reduces the length of the sample-path and the value of the states visited on the path can be updated based on the reward received.</p>
<div id="learning-outcomes-5" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Learning outcomes<a href="mod-mc.html#learning-outcomes-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<ul>
<li>Identify the difference between model-based and model-free RL.</li>
<li>Identify problems that can be solved using Monte-Carlo methods.</li>
<li>Describe how MC methods can be used to estimate value functions from sample data.</li>
<li>Do MC prediction to estimate the value function for a given policy.</li>
<li>Explain why it is important to maintain exploration in MC algorithms.</li>
<li>Do policy improvement (control) using MC in a generalized policy improvement algorithm.</li>
<li>Compare different ways of exploring the state-action space.</li>
<li>Argue why off-policy learning can help deal with the exploration problem.</li>
<li>Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.</li>
<li>Use importance sampling in off-policy learning to predict the value-function of a target policy.</li>
<li>Explain how to modify the MC prediction and improvement algorithm for off-policy learning.</li>
</ul>
<p>The learning outcomes relate to the <a href="mod-lg-course.html#mod-lg-course">overall learning goals</a> number 3, 4, 9 and 12 of the course.</p>
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings-5" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Textbook readings<a href="mod-mc.html#textbook-readings-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 5-5.7 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18">2018</a>)</span>. Read it before continuing this module. A summary of the book notation can be seen <a href="https://bss-osca.github.io/rl/misc/sutton-notation.pdf">here</a>.</p>
<div>
Slides for this module can be seen
<a href="https://bss-osca.github.io/rl/slides/06_mc-slides.html" target="_blank">here.</a>
You do not have to look at them before the lecture!
</div>
</div>
<div id="mc-prediction-evaluation" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> MC prediction (evaluation)<a href="mod-mc.html#mc-prediction-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a policy <span class="math inline">\(\pi\)</span>, we want to estimate the state-value function. Recall that the state value function is
<span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s].
\]</span>
where the return is
<span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}
\]</span>
Now given policy <span class="math inline">\(\pi\)</span> and a sample-path (episode) <span class="math inline">\(S_0, A_0, R_1, S_1, A_1, \ldots, S_{T-1}, A_{T-1}, R_T\)</span> ending in the terminal state at time <span class="math inline">\(T\)</span>, we can calculate the realized return for each state in the sample-path. Each time we have a new sample-path a new realized return for the states is given and the average for the returns in a state is an estimate of the state-value. With enough observations, the sample average converges to the true state-value under the policy <span class="math inline">\(\pi\)</span>.</p>
<p>Given a policy <span class="math inline">\(\pi\)</span> and a set of sample-paths, there are two ways to estimate the state values <span class="math inline">\(v_\pi(s)\)</span>:</p>
<ul>
<li>First visit MC: average returns from first visit to state <span class="math inline">\(s\)</span>.</li>
<li>Every visit MC: average returns following every visit to state <span class="math inline">\(s\)</span>.</li>
</ul>
<p>First visit MC generates iid estimates of <span class="math inline">\(v_\pi(s)\)</span> with finite variance, so the sequence of estimates converges to the expected value by the law of large numbers as the number of observations grow. Every visit MC does not generate independent estimates, but still converges.</p>
<p>An algorithm for first visit MC is given in Figure <a href="mod-mc.html#fig:mc-prediction-alg">6.1</a>. The state-value estimate is stored in a vector <span class="math inline">\(V\)</span> and the returns for each state in a list. Given a sample-path we add the return to the states on the path by scanning the path backwards and updating <span class="math inline">\(G\)</span>. Note since the algorithm considers first visit MC, a check of occurrence of the state earlier in the path done. If this check is dropped, we have a every visit MC algorithm instead. Moreover, the computation needed to update the state-value does not depend on the size of the process/MDP but only of the length of the sample-path.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mc-prediction-alg"></span>
<img src="img/mc-prediction.png" alt="MC policy prediction [@Sutton18]."  />
<p class="caption">
Figure 6.1: MC policy prediction <span class="citation">(<a href="#ref-Sutton18">Sutton and Barto 2018</a>)</span>.
</p>
</div>
<p>The algorithm maintains a list of all returns for each state which may require a lot of memory. Instead as incremental update of <span class="math inline">\(V\)</span> can be done. Adapting Eq. <a href="mod-bandit.html#eq:avg">(2.1)</a>, we have that the sample average can be updated using:</p>
<p><span class="math display">\[
  V(s) \leftarrow V(s) + \frac{1}{n} \left[G - V(s)\right].
\]</span>
where <span class="math inline">\(n\)</span> denote the number of realized returns found for state <span class="math inline">\(s\)</span> and <span class="math inline">\(G\)</span> the current realized return. The state-value vector must be initialized to zero and a vector counting the number of returns found for each state must be stored.</p>
<!-- ### Blackjack - MC prediction -->
<div id="mc-prediction-of-action-values" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> MC prediction of action-values<a href="mod-mc.html#mc-prediction-of-action-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With a model of the environment we only need to estimate the state-value function, since it is easy to determine the policy from the state-values using the Bellman optimality equations <a href="mod-mdp-2.html#eq:bell-opt-state-policy">(4.3)</a>. However, if we do not know the expected reward and transition probabilities state values are not enough. In that case, it is useful to estimate action-values since the optimal policy can be found using <span class="math inline">\(q_*\)</span> (see Eq. <a href="mod-mdp-2.html#eq:bell-opt-state-policy">(4.3)</a>). To find <span class="math inline">\(q_*\)</span>, we first need to predict action-values for a policy <span class="math inline">\(\pi\)</span>. This is essentially the same as for state-values, only we now talk about state-action pairs being visited, i.e. taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> instead.</p>
<p>If <span class="math inline">\(\pi\)</span> is deterministic, then we will only estimate the values of actions that <span class="math inline">\(\pi\)</span> dictates. Therefore some exploration are needed in order to have estimates for all action-values. Two possibilities are:</p>
<ol style="list-style-type: decimal">
<li>Make <span class="math inline">\(\pi\)</span> stochastic, e.g. <span class="math inline">\(\varepsilon\)</span>-soft that that have non-zero probability of selecting each state-action pair.</li>
<li>Use <em>exploring starts</em>, which specifies that ever state-action pair has non-zero probability of being selected as the starting state of an sample-path.</li>
</ol>
</div>
</div>
<div id="mc-control-improvement" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> MC control (improvement)<a href="mod-mc.html#mc-control-improvement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are now ready to formulate a generalized policy iteration (GPI) algorithm using MC to predict the action-values <span class="math inline">\(q(s,a)\)</span>. Policy improvement is done by selecting the next policy greedy with respect to the action-value function:
<span class="math display">\[
    \pi(s) = \arg\max_a q(s, a).
\]</span>
That is, we generate a sequence of policies and action-value functions
<span class="math display">\[\pi_0 \xrightarrow[]{E} q_{\pi_0} \xrightarrow[]{I} \pi_1 \xrightarrow[]{E} q_{\pi_1} \xrightarrow[]{I} \pi_2 \xrightarrow[]{E} q_{\pi_2} \xrightarrow[]{I} \ldots \xrightarrow[]{I} \pi_* \xrightarrow[]{E} q_{*}.\]</span>
Hence the policy improvement theorem applies for all <span class="math inline">\(s \in \mathcal{S}\)</span>:</p>
<p><span class="math display">\[\begin{align}
    q_{\pi_k}(s, a=\pi_{k+1}(s)) &amp;= q_{\pi_k}(s, \arg\max_a q_{\pi_k}(s, a)) \\
                    &amp;= \max_a q_{\pi_k}(s, a) \\
                    &amp;\geq q_{\pi_k}(s, \pi_k(s))\\
                    &amp;= v_{\pi_k}(s)
\end{align}\]</span></p>
<p>That is, <span class="math inline">\(\pi_{k+1}\)</span> is better than <span class="math inline">\(\pi_k\)</span> or optimal.</p>
<p>It is important to understand the major difference between model-based GPI (remember that a model means the transition probability matrix and reward distribution are known) and model-free GPI. We cannot simply use a 100% greedy strategy all the time, since all our action-values are estimates. As such, we now need to introduce an element of exploration into our algorithm to estimate the action-values. For convergence to the optimal policy a model-free GPI algorithm must satisfy:</p>
<ol style="list-style-type: decimal">
<li><em>Infinite exploration</em>: all state-action <span class="math inline">\((s,a)\)</span> pairs should be explored infinitely many times as the number of iterations go to infinity (in the limit), i.e. as the number of iterations <span class="math inline">\(k\)</span> goes to infinity the number of visits <span class="math inline">\(n_k\)</span> does too <span class="math display">\[\lim_{k\rightarrow\infty} n_k(s, a) = \infty.\]</span></li>
<li><em>Greedy in the limit</em>: while we maintain infinite exploration, we do eventually need to converge to the optimal policy:
<span class="math display">\[\lim_{k\rightarrow\infty} \pi_k(a|s) = 1 \text{ for } a = \arg\max_a q(s, a).\]</span></li>
</ol>
<div id="gpi-with-exploring-starts" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> GPI with exploring starts<a href="mod-mc.html#gpi-with-exploring-starts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An algorithm using exploring starts and first visit MC is given in Figure <a href="mod-mc.html#fig:mc-gpi-es-alg">6.2</a>. It satisfies the convergence properties and and incremental implementation can be used to update <span class="math inline">\(Q\)</span>. Note that to predict the action-values for a policy, we in general need a large number of sample-paths. However, much like we did with value iteration, we do not need to fully evaluate the value function for a given policy. Instead we can merely move the value toward the correct value and then switch to policy improvement thereafter. To stop the algorithm from having infinitely many sample-paths we may stop the algorithm once the <span class="math inline">\(q_{\pi_k}\)</span> stop moving within a certain error.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mc-gpi-es-alg"></span>
<img src="img/mc-gpi-es.png" alt="GPI using MC policy prediction with exploring starts [@Sutton18]."  />
<p class="caption">
Figure 6.2: GPI using MC policy prediction with exploring starts <span class="citation">(<a href="#ref-Sutton18">Sutton and Barto 2018</a>)</span>.
</p>
</div>
<!-- ### Blackjack - MC control -->
</div>
<div id="gpi-using-epsilon-soft-policies" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> GPI using <span class="math inline">\(\epsilon\)</span>-soft policies<a href="mod-mc.html#gpi-using-epsilon-soft-policies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note by using exploring starts in Algorithm <a href="mod-mc.html#fig:mc-gpi-es-alg">6.2</a>, the ‘infinite exploration’ convergence assumption is satisfied. However exploring starts may be hard to use in practice. Another approach to ensure infinite exploration is to use a soft policy, i.e. assign a non-zero probability to each possible action in a state. An on-policy algorithm using <span class="math inline">\(\epsilon\)</span>-greedy policies is given in Figure <a href="mod-mc.html#fig:mc-gpi-on-policy-alg">6.3</a>. Here we put probability <span class="math inline">\(1 - \varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|}\)</span> on the maximal action and <span class="math inline">\(\frac{\varepsilon}{|\mathcal{A}(s)|}\)</span> on each of the others. Note using <span class="math inline">\(\epsilon\)</span>-greedy policy selection will improve the current policy; otherwise we have found best policy amongst the <span class="math inline">\(\epsilon\)</span>-soft policies. If we want to find the optimal policy we have to ensure the ‘greedy in the limit’ convergence assumption. This can be done by decreasing <span class="math inline">\(\epsilon\)</span> as the number of iterations increase (e.g. <span class="math inline">\(\epsilon = 1/k\)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mc-gpi-on-policy-alg"></span>
<img src="img/mc-gpi-on-policy.png" alt="On-policy GPI using MC policy prediction [@Sutton18]."  />
<p class="caption">
Figure 6.3: On-policy GPI using MC policy prediction <span class="citation">(<a href="#ref-Sutton18">Sutton and Barto 2018</a>)</span>.
</p>
</div>
<p>An incremental approach for updating <span class="math inline">\(Q\)</span> can be used by storing the number of times <span class="math inline">\(n_a\)</span>, action <span class="math inline">\(a\)</span> has been visited in state <span class="math inline">\(s\)</span> and then update <span class="math inline">\(Q(s,a)\)</span> using <span class="math display">\[Q_{n+1} = Q_n + \frac{1}{n_a}(G-Q_n),\]</span>
where <span class="math inline">\(Q_n\)</span> denote the previous value.</p>
<p>Finally, the algorithm in Figure <a href="mod-mc.html#fig:mc-gpi-on-policy-alg">6.3</a> do not mention how to find the start state of an episode. In general all states that we want to approximate must be used as start state.</p>
</div>
<div id="gpi-using-upper-confience-bound-action-selection" class="section level3 hasAnchor" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> GPI using upper-confience bound action selection<a href="mod-mc.html#gpi-using-upper-confience-bound-action-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>GPI using exploring starts or <span class="math inline">\(\epsilon\)</span>-soft policies may be slow. Often speed-ups can be done by using e.g. upper-confidence bounds (UCB) for action selection. Recall from Module <a href="mod-bandit.html#mod-bandit">2</a> that UCB select actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. That is, the next action <span class="math inline">\(a&#39;\)</span> given a state <span class="math inline">\(s\)</span> is selected using:
<span class="math display">\[
    a&#39; = \arg\max_a \left(Q(s,a) + c\sqrt{\frac{\ln n_s}{n_a}}\right),
\]</span>
where <span class="math inline">\(n_s\)</span> denote the number of times state <span class="math inline">\(s\)</span> has been visited and <span class="math inline">\(n_a\)</span> denote the number of times action <span class="math inline">\(a\)</span> has been visited (both numbers must be stored). The parameter <span class="math inline">\(c&gt;0\)</span> controls the degree of exploration. Higher <span class="math inline">\(c\)</span> results in more weight on the uncertainty. However, one problem with UCB is that it is hard to decide on which value of <span class="math inline">\(c\)</span> to use in advance.<br />
<!-- Preliminary testing showed that using UCB instead of $\epsilon$-greedy policies worked better.  --></p>
</div>
<div id="mc-seasonal" class="section level3 hasAnchor" number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Example - Seasonal inventory and sales planning<a href="mod-mc.html#mc-seasonal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the following example we try to implement an algorithm that uses generalized policy iteration with every-visit estimation using epsilon-greedy action selection.</p>
<p>We consider seasonal product such as garden furnitures. Assume that the maximum inventory level is <span class="math inline">\(Q\)</span> items, i.e. we can buy at most <span class="math inline">\(Q\)</span> items at the start of the season for a price of $14. The product can be sold for at most <span class="math inline">\(T\)</span> weeks and at the end of the period (week <span class="math inline">\(T\)</span>), the remaining inventory is sold to an outlet store for $5 per item.</p>
<p>The demand depends on the sales price which based on historic observations is assumed in the interval <span class="math inline">\([10,25].\)</span> In general a higher sales price result in a lower demand. Moreover, in the first half part of the season the demand is on average 10% higher given a fixed sales price compared to the last half part of the season. Historic observed demands can be seen in Figure <a href="mod-mc.html#fig:demand">6.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:demand"></span>
<img src="06_mc_files/figure-html/demand-1.png" alt="Observed demands given price (scaled based on number of observations)." width="672" />
<p class="caption">
Figure 6.4: Observed demands given price (scaled based on number of observations).
</p>
</div>
<p>Let <span class="math inline">\(s = (q,t)\)</span> denote the state of the system in the start of a week, where <span class="math inline">\(q\)</span> is the inventory and <span class="math inline">\(t\)</span> the week number. Then the state space is <span class="math display">\[\mathcal{S} = \{ s = (q,t) | 1 \leq q \leq Q, 1 \leq t \leq T \} \cup \{ 0 \},\]</span>
where state <span class="math inline">\(s = 0\)</span> denote the terminal state (inventory empty).
Let us limit us to actions <span class="math display">\[\mathcal{A}(q,t) = \{ 10,15,20,25 \}, \mathcal{A}(0) = \{ d \}, \]</span> where action <span class="math inline">\(a\)</span> denote the price and <span class="math inline">\(d\)</span> denote the dummy action with deterministic transition to state <span class="math inline">\(0\)</span>.</p>
<p>The inventory dynamics for transitions not to the terminal state are <span class="math display">\[t&#39; = t + 1,\]</span> <span class="math display">\[q&#39; = q - min(q, D),\]</span>
where <span class="math inline">\(D\)</span> denote the demand. Moreover, if <span class="math inline">\(t = T\)</span> or <span class="math inline">\(q&#39; = 0\)</span>, then a transition to the terminal state happens.</p>
<p>For <span class="math inline">\(t=1\)</span> the reward of an state <span class="math inline">\((q,t)\)</span> is sales price times the number of sold items minus the purchase cost. For <span class="math inline">\(1&lt;t&lt;T\)</span> the reward is sales price times the number of sold (we assume an inventory cost of zero), while for <span class="math inline">\(t=T\)</span> the reward is the scrap price times the inventory.</p>
<p>Let us define a R6 class representing the environment for this problem</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="mod-mc.html#cb52-1" tabindex="-1"></a><span class="fu">library</span>(R6)</span>
<span id="cb52-2"><a href="mod-mc.html#cb52-2" tabindex="-1"></a><span class="fu">library</span>(hash)</span>
<span id="cb52-3"><a href="mod-mc.html#cb52-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb52-4"><a href="mod-mc.html#cb52-4" tabindex="-1"></a></span>
<span id="cb52-5"><a href="mod-mc.html#cb52-5" tabindex="-1"></a><span class="co">#&#39; R6 Class representing the RL environment for the problem</span></span>
<span id="cb52-6"><a href="mod-mc.html#cb52-6" tabindex="-1"></a>RLEnvSeasonal <span class="ot">&lt;-</span> <span class="fu">R6Class</span>(<span class="st">&quot;RLEnvSeasonal&quot;</span>,</span>
<span id="cb52-7"><a href="mod-mc.html#cb52-7" tabindex="-1"></a>   <span class="at">public =</span> <span class="fu">list</span>(</span>
<span id="cb52-8"><a href="mod-mc.html#cb52-8" tabindex="-1"></a>      </span>
<span id="cb52-9"><a href="mod-mc.html#cb52-9" tabindex="-1"></a>      <span class="co">#&#39; @field maxInv Max inventory level.</span></span>
<span id="cb52-10"><a href="mod-mc.html#cb52-10" tabindex="-1"></a>      <span class="at">maxInv =</span> <span class="cn">NA</span>,  </span>
<span id="cb52-11"><a href="mod-mc.html#cb52-11" tabindex="-1"></a>      </span>
<span id="cb52-12"><a href="mod-mc.html#cb52-12" tabindex="-1"></a>      <span class="co">#&#39; @field maxT Max number of weeks to sell product.</span></span>
<span id="cb52-13"><a href="mod-mc.html#cb52-13" tabindex="-1"></a>      <span class="at">maxT =</span> <span class="cn">NA</span>,</span>
<span id="cb52-14"><a href="mod-mc.html#cb52-14" tabindex="-1"></a>      </span>
<span id="cb52-15"><a href="mod-mc.html#cb52-15" tabindex="-1"></a>      <span class="co">#&#39; @field scrapPrice Scrap price per item in week maxT.</span></span>
<span id="cb52-16"><a href="mod-mc.html#cb52-16" tabindex="-1"></a>      <span class="at">scrapPrice =</span> <span class="cn">NA</span>,</span>
<span id="cb52-17"><a href="mod-mc.html#cb52-17" tabindex="-1"></a>      </span>
<span id="cb52-18"><a href="mod-mc.html#cb52-18" tabindex="-1"></a>      <span class="co">#&#39; @field purchasePrice Purchase price per item.</span></span>
<span id="cb52-19"><a href="mod-mc.html#cb52-19" tabindex="-1"></a>      <span class="at">purchasePrice =</span> <span class="cn">NA</span>,</span>
<span id="cb52-20"><a href="mod-mc.html#cb52-20" tabindex="-1"></a>      </span>
<span id="cb52-21"><a href="mod-mc.html#cb52-21" tabindex="-1"></a>      <span class="co">#&#39; @field prices Possible sales prices per item.</span></span>
<span id="cb52-22"><a href="mod-mc.html#cb52-22" tabindex="-1"></a>      <span class="at">prices =</span> <span class="cn">NA</span>,</span>
<span id="cb52-23"><a href="mod-mc.html#cb52-23" tabindex="-1"></a></span>
<span id="cb52-24"><a href="mod-mc.html#cb52-24" tabindex="-1"></a>      <span class="co">#&#39; @description Create an object (when call new).</span></span>
<span id="cb52-25"><a href="mod-mc.html#cb52-25" tabindex="-1"></a>      <span class="co">#&#39; @param maxInv Max inventory level.</span></span>
<span id="cb52-26"><a href="mod-mc.html#cb52-26" tabindex="-1"></a>      <span class="co">#&#39; @param maxT Max number of weeks to sell product.</span></span>
<span id="cb52-27"><a href="mod-mc.html#cb52-27" tabindex="-1"></a>      <span class="co">#&#39; @param scrapPrice Scrap price per item in week maxT.</span></span>
<span id="cb52-28"><a href="mod-mc.html#cb52-28" tabindex="-1"></a>      <span class="co">#&#39; @param purchasePrice Purchase price per item.</span></span>
<span id="cb52-29"><a href="mod-mc.html#cb52-29" tabindex="-1"></a>      <span class="co">#&#39; @return The new object.</span></span>
<span id="cb52-30"><a href="mod-mc.html#cb52-30" tabindex="-1"></a>      <span class="at">initialize =</span> <span class="cf">function</span>(maxInv, maxT, scrapPrice, purchasePrice, prices) {</span>
<span id="cb52-31"><a href="mod-mc.html#cb52-31" tabindex="-1"></a>         self<span class="sc">$</span>maxInv <span class="ot">&lt;-</span> maxInv </span>
<span id="cb52-32"><a href="mod-mc.html#cb52-32" tabindex="-1"></a>         self<span class="sc">$</span>maxT <span class="ot">&lt;-</span> maxT</span>
<span id="cb52-33"><a href="mod-mc.html#cb52-33" tabindex="-1"></a>         self<span class="sc">$</span>scrapPrice <span class="ot">&lt;-</span> scrapPrice</span>
<span id="cb52-34"><a href="mod-mc.html#cb52-34" tabindex="-1"></a>         self<span class="sc">$</span>purchasePrice <span class="ot">&lt;-</span> purchasePrice</span>
<span id="cb52-35"><a href="mod-mc.html#cb52-35" tabindex="-1"></a>         self<span class="sc">$</span>prices <span class="ot">&lt;-</span> prices</span>
<span id="cb52-36"><a href="mod-mc.html#cb52-36" tabindex="-1"></a>      },</span>
<span id="cb52-37"><a href="mod-mc.html#cb52-37" tabindex="-1"></a>      </span>
<span id="cb52-38"><a href="mod-mc.html#cb52-38" tabindex="-1"></a>      </span>
<span id="cb52-39"><a href="mod-mc.html#cb52-39" tabindex="-1"></a>      <span class="co">#&#39; @description Return all states (keys).</span></span>
<span id="cb52-40"><a href="mod-mc.html#cb52-40" tabindex="-1"></a>      <span class="at">getStates =</span> <span class="cf">function</span>() {</span>
<span id="cb52-41"><a href="mod-mc.html#cb52-41" tabindex="-1"></a>         states <span class="ot">&lt;-</span> <span class="fu">expand_grid</span>(<span class="at">l =</span> <span class="dv">1</span><span class="sc">:</span>env<span class="sc">$</span>maxInv, <span class="at">t =</span> <span class="dv">1</span><span class="sc">:</span>env<span class="sc">$</span>maxT) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">s =</span> <span class="fu">str_c</span>(l, <span class="st">&quot;,&quot;</span>, t)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(s)</span>
<span id="cb52-42"><a href="mod-mc.html#cb52-42" tabindex="-1"></a>         states <span class="ot">&lt;-</span> <span class="fu">c</span>(states, <span class="st">&quot;0&quot;</span>)</span>
<span id="cb52-43"><a href="mod-mc.html#cb52-43" tabindex="-1"></a>         <span class="fu">return</span>(states)</span>
<span id="cb52-44"><a href="mod-mc.html#cb52-44" tabindex="-1"></a>      },</span>
<span id="cb52-45"><a href="mod-mc.html#cb52-45" tabindex="-1"></a>      </span>
<span id="cb52-46"><a href="mod-mc.html#cb52-46" tabindex="-1"></a>      <span class="co">#&#39; @description Return all actions (keys) for a state.</span></span>
<span id="cb52-47"><a href="mod-mc.html#cb52-47" tabindex="-1"></a>      <span class="co">#&#39; @param s State considered.</span></span>
<span id="cb52-48"><a href="mod-mc.html#cb52-48" tabindex="-1"></a>      <span class="at">getActions =</span> <span class="cf">function</span>(s) {</span>
<span id="cb52-49"><a href="mod-mc.html#cb52-49" tabindex="-1"></a>         <span class="cf">if</span> (s <span class="sc">==</span> <span class="st">&quot;0&quot;</span>) <span class="fu">return</span>(<span class="st">&quot;dummy&quot;</span>)</span>
<span id="cb52-50"><a href="mod-mc.html#cb52-50" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">str_split</span>(s, <span class="st">&quot;,&quot;</span>, <span class="at">simplify =</span> T)</span>
<span id="cb52-51"><a href="mod-mc.html#cb52-51" tabindex="-1"></a>         t <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">2</span>])</span>
<span id="cb52-52"><a href="mod-mc.html#cb52-52" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">1</span>])</span>
<span id="cb52-53"><a href="mod-mc.html#cb52-53" tabindex="-1"></a>         <span class="cf">if</span> (t <span class="sc">==</span> self<span class="sc">$</span>maxT) <span class="fu">return</span>(<span class="fu">str_c</span>(self<span class="sc">$</span>scrapPrice))</span>
<span id="cb52-54"><a href="mod-mc.html#cb52-54" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">str_c</span>(self<span class="sc">$</span>prices))</span>
<span id="cb52-55"><a href="mod-mc.html#cb52-55" tabindex="-1"></a>      },</span>
<span id="cb52-56"><a href="mod-mc.html#cb52-56" tabindex="-1"></a>      </span>
<span id="cb52-57"><a href="mod-mc.html#cb52-57" tabindex="-1"></a>      <span class="co">#&#39; @description Stochastic demand sample. </span></span>
<span id="cb52-58"><a href="mod-mc.html#cb52-58" tabindex="-1"></a>      <span class="co">#&#39; @param price Sales price.</span></span>
<span id="cb52-59"><a href="mod-mc.html#cb52-59" tabindex="-1"></a>      <span class="co">#&#39; @param t Week.</span></span>
<span id="cb52-60"><a href="mod-mc.html#cb52-60" tabindex="-1"></a>      <span class="at">getDemand =</span> <span class="cf">function</span>(price, t) {</span>
<span id="cb52-61"><a href="mod-mc.html#cb52-61" tabindex="-1"></a>         l1 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">20</span>, <span class="dv">12</span>,<span class="dv">12</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> T)  <span class="co"># points (price, demand) for first line</span></span>
<span id="cb52-62"><a href="mod-mc.html#cb52-62" tabindex="-1"></a>         l2 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">12</span>,<span class="dv">12</span>, <span class="dv">15</span>,<span class="dv">10</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> T)  </span>
<span id="cb52-63"><a href="mod-mc.html#cb52-63" tabindex="-1"></a>         l3 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">15</span>,<span class="dv">10</span>), <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">byrow =</span> T)  </span>
<span id="cb52-64"><a href="mod-mc.html#cb52-64" tabindex="-1"></a>         </span>
<span id="cb52-65"><a href="mod-mc.html#cb52-65" tabindex="-1"></a>         <span class="cf">if</span> (price <span class="sc">&lt;=</span> <span class="fu">max</span>(l1[<span class="dv">1</span>,<span class="dv">1</span>],l1[<span class="dv">2</span>,<span class="dv">1</span>])) {</span>
<span id="cb52-66"><a href="mod-mc.html#cb52-66" tabindex="-1"></a>            a <span class="ot">&lt;-</span> (l1[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">-</span>l1[<span class="dv">2</span>,<span class="dv">2</span>])<span class="sc">/</span>(l1[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">-</span>l1[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb52-67"><a href="mod-mc.html#cb52-67" tabindex="-1"></a>            b <span class="ot">&lt;-</span> l1[<span class="dv">1</span>,<span class="dv">2</span>] <span class="sc">-</span> a <span class="sc">*</span> l1[<span class="dv">1</span>,<span class="dv">1</span>] </span>
<span id="cb52-68"><a href="mod-mc.html#cb52-68" tabindex="-1"></a>            d <span class="ot">&lt;-</span> a <span class="sc">*</span> price <span class="sc">+</span> b </span>
<span id="cb52-69"><a href="mod-mc.html#cb52-69" tabindex="-1"></a>            dS <span class="ot">&lt;-</span> d <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="fl">0.75</span>, <span class="fl">1.25</span>)</span>
<span id="cb52-70"><a href="mod-mc.html#cb52-70" tabindex="-1"></a>         } <span class="cf">else</span> <span class="cf">if</span> (price <span class="sc">&gt;=</span> <span class="fu">min</span>(l2[<span class="dv">1</span>,<span class="dv">1</span>],l2[<span class="dv">2</span>,<span class="dv">1</span>]) <span class="sc">&amp;</span> price <span class="sc">&lt;=</span> <span class="fu">max</span>(l2[<span class="dv">1</span>,<span class="dv">1</span>],l2[<span class="dv">2</span>,<span class="dv">1</span>])) {</span>
<span id="cb52-71"><a href="mod-mc.html#cb52-71" tabindex="-1"></a>            a <span class="ot">&lt;-</span> (l2[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">-</span>l2[<span class="dv">2</span>,<span class="dv">2</span>])<span class="sc">/</span>(l2[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">-</span>l2[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb52-72"><a href="mod-mc.html#cb52-72" tabindex="-1"></a>            b <span class="ot">&lt;-</span> l2[<span class="dv">1</span>,<span class="dv">2</span>] <span class="sc">-</span> a <span class="sc">*</span> l2[<span class="dv">1</span>,<span class="dv">1</span>] </span>
<span id="cb52-73"><a href="mod-mc.html#cb52-73" tabindex="-1"></a>            d <span class="ot">&lt;-</span> a <span class="sc">*</span> price <span class="sc">+</span> b </span>
<span id="cb52-74"><a href="mod-mc.html#cb52-74" tabindex="-1"></a>            dS <span class="ot">&lt;-</span> d <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="fl">0.75</span>, <span class="fl">1.25</span>)</span>
<span id="cb52-75"><a href="mod-mc.html#cb52-75" tabindex="-1"></a>         } <span class="cf">else</span> <span class="cf">if</span> (price <span class="sc">&gt;=</span> l3[<span class="dv">1</span>,<span class="dv">1</span>]) {</span>
<span id="cb52-76"><a href="mod-mc.html#cb52-76" tabindex="-1"></a>            d <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">4</span><span class="sc">*</span><span class="fu">log</span>(price <span class="sc">-</span> l3[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">+</span> l3[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb52-77"><a href="mod-mc.html#cb52-77" tabindex="-1"></a>            dS <span class="ot">&lt;-</span> d <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb52-78"><a href="mod-mc.html#cb52-78" tabindex="-1"></a>         }</span>
<span id="cb52-79"><a href="mod-mc.html#cb52-79" tabindex="-1"></a>         <span class="cf">if</span> (t <span class="sc">&lt;=</span> self<span class="sc">$</span>maxT<span class="sc">/</span><span class="dv">2</span>) {</span>
<span id="cb52-80"><a href="mod-mc.html#cb52-80" tabindex="-1"></a>            dS <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">1.2</span>) <span class="sc">*</span> dS</span>
<span id="cb52-81"><a href="mod-mc.html#cb52-81" tabindex="-1"></a>         }</span>
<span id="cb52-82"><a href="mod-mc.html#cb52-82" tabindex="-1"></a>         <span class="co"># if (stochastic) return(round(dS)) else return(round(d))</span></span>
<span id="cb52-83"><a href="mod-mc.html#cb52-83" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">round</span>(dS))</span>
<span id="cb52-84"><a href="mod-mc.html#cb52-84" tabindex="-1"></a>      },</span>
<span id="cb52-85"><a href="mod-mc.html#cb52-85" tabindex="-1"></a>      </span>
<span id="cb52-86"><a href="mod-mc.html#cb52-86" tabindex="-1"></a>      <span class="co">#&#39; @description Returns the simulated transition probabilities and reward. </span></span>
<span id="cb52-87"><a href="mod-mc.html#cb52-87" tabindex="-1"></a>      <span class="at">getTransPrR =</span> <span class="cf">function</span>(s, a, <span class="at">runs =</span> <span class="dv">10000</span>) {</span>
<span id="cb52-88"><a href="mod-mc.html#cb52-88" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">str_split</span>(s, <span class="st">&quot;,&quot;</span>, <span class="at">simplify =</span> T)</span>
<span id="cb52-89"><a href="mod-mc.html#cb52-89" tabindex="-1"></a>         t <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">2</span>])</span>
<span id="cb52-90"><a href="mod-mc.html#cb52-90" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">1</span>])</span>
<span id="cb52-91"><a href="mod-mc.html#cb52-91" tabindex="-1"></a>         <span class="cf">if</span> (q <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb52-92"><a href="mod-mc.html#cb52-92" tabindex="-1"></a>            <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">pr =</span> <span class="fu">c</span>(<span class="st">&quot;0&quot;</span> <span class="ot">=</span> <span class="dv">1</span>), <span class="at">r =</span> <span class="dv">0</span>))</span>
<span id="cb52-93"><a href="mod-mc.html#cb52-93" tabindex="-1"></a>         }</span>
<span id="cb52-94"><a href="mod-mc.html#cb52-94" tabindex="-1"></a>         <span class="cf">if</span> (t <span class="sc">==</span> env<span class="sc">$</span>maxT) {</span>
<span id="cb52-95"><a href="mod-mc.html#cb52-95" tabindex="-1"></a>            <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">pr =</span> <span class="fu">c</span>(<span class="st">&quot;0&quot;</span> <span class="ot">=</span> <span class="dv">1</span>), <span class="at">r =</span> env<span class="sc">$</span>scrapPrice <span class="sc">*</span> q))</span>
<span id="cb52-96"><a href="mod-mc.html#cb52-96" tabindex="-1"></a>         }</span>
<span id="cb52-97"><a href="mod-mc.html#cb52-97" tabindex="-1"></a>         a <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(a)</span>
<span id="cb52-98"><a href="mod-mc.html#cb52-98" tabindex="-1"></a>         df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">sim =</span> <span class="dv">1</span><span class="sc">:</span>runs) <span class="sc">%&gt;%</span> </span>
<span id="cb52-99"><a href="mod-mc.html#cb52-99" tabindex="-1"></a>            <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb52-100"><a href="mod-mc.html#cb52-100" tabindex="-1"></a>            <span class="fu">mutate</span>(<span class="at">demand =</span> self<span class="sc">$</span><span class="fu">getDemand</span>(a, t)) <span class="sc">%&gt;%</span> </span>
<span id="cb52-101"><a href="mod-mc.html#cb52-101" tabindex="-1"></a>            <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb52-102"><a href="mod-mc.html#cb52-102" tabindex="-1"></a>            <span class="fu">mutate</span>(<span class="at">qN =</span> q <span class="sc">-</span> <span class="fu">pmin</span>(q, demand), <span class="at">reward =</span> a <span class="sc">*</span> <span class="fu">pmin</span>(q, demand))</span>
<span id="cb52-103"><a href="mod-mc.html#cb52-103" tabindex="-1"></a>         reward <span class="ot">&lt;-</span> <span class="fu">mean</span>(df<span class="sc">$</span>reward)</span>
<span id="cb52-104"><a href="mod-mc.html#cb52-104" tabindex="-1"></a>         <span class="cf">if</span> (t <span class="sc">==</span> <span class="dv">1</span>) reward <span class="ot">&lt;-</span> reward <span class="sc">-</span> q <span class="sc">*</span> self<span class="sc">$</span>purchasePrice</span>
<span id="cb52-105"><a href="mod-mc.html#cb52-105" tabindex="-1"></a>         df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb52-106"><a href="mod-mc.html#cb52-106" tabindex="-1"></a>            <span class="fu">count</span>(qN) <span class="sc">%&gt;%</span> </span>
<span id="cb52-107"><a href="mod-mc.html#cb52-107" tabindex="-1"></a>            <span class="fu">mutate</span>(<span class="at">pr =</span> n<span class="sc">/</span><span class="fu">sum</span>(n), <span class="at">qN =</span> <span class="fu">if_else</span>(qN <span class="sc">==</span> <span class="dv">0</span>, <span class="st">&quot;0&quot;</span>, <span class="fu">str_c</span>(qN, <span class="st">&quot;,&quot;</span>, t<span class="sc">+</span><span class="dv">1</span>)))</span>
<span id="cb52-108"><a href="mod-mc.html#cb52-108" tabindex="-1"></a>         pr <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">pull</span>(pr, <span class="at">name =</span> qN)</span>
<span id="cb52-109"><a href="mod-mc.html#cb52-109" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">pr =</span> pr, <span class="at">r =</span> reward))</span>
<span id="cb52-110"><a href="mod-mc.html#cb52-110" tabindex="-1"></a>      },</span>
<span id="cb52-111"><a href="mod-mc.html#cb52-111" tabindex="-1"></a>      </span>
<span id="cb52-112"><a href="mod-mc.html#cb52-112" tabindex="-1"></a>      <span class="co">#&#39; @description Returns an episode as a tibble with cols s, a, r (last col the terminal reward).</span></span>
<span id="cb52-113"><a href="mod-mc.html#cb52-113" tabindex="-1"></a>      <span class="co">#&#39; @param agent The agent that holds the model with current policy stored in pi. </span></span>
<span id="cb52-114"><a href="mod-mc.html#cb52-114" tabindex="-1"></a>      <span class="at">getEpisodePi =</span> <span class="cf">function</span>(agent, startState) {</span>
<span id="cb52-115"><a href="mod-mc.html#cb52-115" tabindex="-1"></a>         <span class="cf">if</span> (startState <span class="sc">==</span> <span class="st">&quot;0&quot;</span>) {</span>
<span id="cb52-116"><a href="mod-mc.html#cb52-116" tabindex="-1"></a>            <span class="fu">return</span>(tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="sc">!!!</span><span class="fu">c</span>(<span class="st">&quot;s&quot;</span>, <span class="st">&quot;a&quot;</span>, <span class="st">&quot;r&quot;</span>), <span class="at">.rows =</span> <span class="dv">0</span>))  <span class="co"># empty tibble</span></span>
<span id="cb52-117"><a href="mod-mc.html#cb52-117" tabindex="-1"></a>         }</span>
<span id="cb52-118"><a href="mod-mc.html#cb52-118" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">str_split</span>(startState, <span class="st">&quot;,&quot;</span>, <span class="at">simplify =</span> T)</span>
<span id="cb52-119"><a href="mod-mc.html#cb52-119" tabindex="-1"></a>         t <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">2</span>])</span>
<span id="cb52-120"><a href="mod-mc.html#cb52-120" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">1</span>])</span>
<span id="cb52-121"><a href="mod-mc.html#cb52-121" tabindex="-1"></a>         df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">s =</span> <span class="fu">rep</span>(<span class="cn">NA_character_</span>, <span class="dv">100</span> <span class="sc">*</span> self<span class="sc">$</span>maxInv), <span class="at">a =</span> <span class="cn">NA_character_</span>, <span class="at">r =</span> <span class="cn">NA_real_</span>)  <span class="co"># a tibble with more rows than used</span></span>
<span id="cb52-122"><a href="mod-mc.html#cb52-122" tabindex="-1"></a>         <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)) {</span>
<span id="cb52-123"><a href="mod-mc.html#cb52-123" tabindex="-1"></a>            <span class="cf">if</span> (q <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb52-124"><a href="mod-mc.html#cb52-124" tabindex="-1"></a>               df <span class="ot">&lt;-</span> df[<span class="dv">1</span><span class="sc">:</span>(i<span class="dv">-1</span>),]  <span class="co"># remove unused rows</span></span>
<span id="cb52-125"><a href="mod-mc.html#cb52-125" tabindex="-1"></a>               <span class="cf">break</span>  <span class="co"># have reached terminal state</span></span>
<span id="cb52-126"><a href="mod-mc.html#cb52-126" tabindex="-1"></a>            }</span>
<span id="cb52-127"><a href="mod-mc.html#cb52-127" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">str_c</span>(q, <span class="st">&quot;,&quot;</span>, t)</span>
<span id="cb52-128"><a href="mod-mc.html#cb52-128" tabindex="-1"></a>            a <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionPi</span>(s)</span>
<span id="cb52-129"><a href="mod-mc.html#cb52-129" tabindex="-1"></a>            <span class="cf">if</span> (t <span class="sc">==</span> self<span class="sc">$</span>maxT) { <span class="co"># sell remaining</span></span>
<span id="cb52-130"><a href="mod-mc.html#cb52-130" tabindex="-1"></a>               r <span class="ot">&lt;-</span> self<span class="sc">$</span>scrapPrice <span class="sc">*</span> q</span>
<span id="cb52-131"><a href="mod-mc.html#cb52-131" tabindex="-1"></a>               q <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># new q value</span></span>
<span id="cb52-132"><a href="mod-mc.html#cb52-132" tabindex="-1"></a>            } <span class="cf">else</span> {</span>
<span id="cb52-133"><a href="mod-mc.html#cb52-133" tabindex="-1"></a>               price <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(a)</span>
<span id="cb52-134"><a href="mod-mc.html#cb52-134" tabindex="-1"></a>               d <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getDemand</span>(price, t)</span>
<span id="cb52-135"><a href="mod-mc.html#cb52-135" tabindex="-1"></a>               <span class="cf">if</span> (t <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb52-136"><a href="mod-mc.html#cb52-136" tabindex="-1"></a>                  r <span class="ot">&lt;-</span> price <span class="sc">*</span> <span class="fu">min</span>(q, d) <span class="sc">-</span> q <span class="sc">*</span> self<span class="sc">$</span>purchasePrice</span>
<span id="cb52-137"><a href="mod-mc.html#cb52-137" tabindex="-1"></a>               } <span class="cf">else</span> {</span>
<span id="cb52-138"><a href="mod-mc.html#cb52-138" tabindex="-1"></a>                  r <span class="ot">&lt;-</span> price <span class="sc">*</span> <span class="fu">min</span>(q, d)</span>
<span id="cb52-139"><a href="mod-mc.html#cb52-139" tabindex="-1"></a>               }</span>
<span id="cb52-140"><a href="mod-mc.html#cb52-140" tabindex="-1"></a>               q <span class="ot">&lt;-</span> q <span class="sc">-</span> <span class="fu">min</span>(q, d)  <span class="co"># new q value</span></span>
<span id="cb52-141"><a href="mod-mc.html#cb52-141" tabindex="-1"></a>               t <span class="ot">&lt;-</span> t <span class="sc">+</span> <span class="dv">1</span> <span class="co"># new t value</span></span>
<span id="cb52-142"><a href="mod-mc.html#cb52-142" tabindex="-1"></a>            }</span>
<span id="cb52-143"><a href="mod-mc.html#cb52-143" tabindex="-1"></a>            df<span class="sc">$</span>s[i] <span class="ot">&lt;-</span> s</span>
<span id="cb52-144"><a href="mod-mc.html#cb52-144" tabindex="-1"></a>            df<span class="sc">$</span>a[i] <span class="ot">&lt;-</span> a</span>
<span id="cb52-145"><a href="mod-mc.html#cb52-145" tabindex="-1"></a>            df<span class="sc">$</span>r[i] <span class="ot">&lt;-</span> r</span>
<span id="cb52-146"><a href="mod-mc.html#cb52-146" tabindex="-1"></a>         }</span>
<span id="cb52-147"><a href="mod-mc.html#cb52-147" tabindex="-1"></a>         <span class="fu">return</span>(df)</span>
<span id="cb52-148"><a href="mod-mc.html#cb52-148" tabindex="-1"></a>      },</span>
<span id="cb52-149"><a href="mod-mc.html#cb52-149" tabindex="-1"></a>      </span>
<span id="cb52-150"><a href="mod-mc.html#cb52-150" tabindex="-1"></a>      <span class="co">#&#39; @description Returns an episode as a tibble with cols s, a, r (last col the terminal reward).</span></span>
<span id="cb52-151"><a href="mod-mc.html#cb52-151" tabindex="-1"></a>      <span class="co">#&#39; @param agent The agent that holds the model with current policy. </span></span>
<span id="cb52-152"><a href="mod-mc.html#cb52-152" tabindex="-1"></a>      <span class="at">getEpisode =</span> <span class="cf">function</span>(agent, startState, <span class="at">coeff =</span> <span class="dv">1</span>, <span class="at">eps =</span> <span class="fl">0.1</span>) {</span>
<span id="cb52-153"><a href="mod-mc.html#cb52-153" tabindex="-1"></a>         <span class="cf">if</span> (startState <span class="sc">==</span> <span class="st">&quot;0&quot;</span>) {</span>
<span id="cb52-154"><a href="mod-mc.html#cb52-154" tabindex="-1"></a>            <span class="fu">return</span>(tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="sc">!!!</span><span class="fu">c</span>(<span class="st">&quot;s&quot;</span>, <span class="st">&quot;a&quot;</span>, <span class="st">&quot;r&quot;</span>), <span class="at">.rows =</span> <span class="dv">0</span>))  <span class="co"># empty tibble</span></span>
<span id="cb52-155"><a href="mod-mc.html#cb52-155" tabindex="-1"></a>         }</span>
<span id="cb52-156"><a href="mod-mc.html#cb52-156" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">str_split</span>(startState, <span class="st">&quot;,&quot;</span>, <span class="at">simplify =</span> T)</span>
<span id="cb52-157"><a href="mod-mc.html#cb52-157" tabindex="-1"></a>         t <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">2</span>])</span>
<span id="cb52-158"><a href="mod-mc.html#cb52-158" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(q[<span class="dv">1</span>])</span>
<span id="cb52-159"><a href="mod-mc.html#cb52-159" tabindex="-1"></a>         df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">s =</span> <span class="fu">rep</span>(<span class="cn">NA_character_</span>, <span class="dv">100</span> <span class="sc">*</span> self<span class="sc">$</span>maxInv), <span class="at">a =</span> <span class="cn">NA_character_</span>, <span class="at">r =</span> <span class="cn">NA_real_</span>)  <span class="co"># a tibble with more rows than used</span></span>
<span id="cb52-160"><a href="mod-mc.html#cb52-160" tabindex="-1"></a>         <span class="co"># q &lt;- sample(1:self$maxInv, 1)  # pick start inventory random</span></span>
<span id="cb52-161"><a href="mod-mc.html#cb52-161" tabindex="-1"></a>         <span class="co"># q &lt;- 1</span></span>
<span id="cb52-162"><a href="mod-mc.html#cb52-162" tabindex="-1"></a>         <span class="co"># t &lt;- 1</span></span>
<span id="cb52-163"><a href="mod-mc.html#cb52-163" tabindex="-1"></a>         <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)) {</span>
<span id="cb52-164"><a href="mod-mc.html#cb52-164" tabindex="-1"></a>            <span class="cf">if</span> (q <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb52-165"><a href="mod-mc.html#cb52-165" tabindex="-1"></a>               df <span class="ot">&lt;-</span> df[<span class="dv">1</span><span class="sc">:</span>(i<span class="dv">-1</span>),]  <span class="co"># remove unused rows</span></span>
<span id="cb52-166"><a href="mod-mc.html#cb52-166" tabindex="-1"></a>               <span class="cf">break</span>  <span class="co"># have reached terminal state</span></span>
<span id="cb52-167"><a href="mod-mc.html#cb52-167" tabindex="-1"></a>            }</span>
<span id="cb52-168"><a href="mod-mc.html#cb52-168" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">str_c</span>(q, <span class="st">&quot;,&quot;</span>, t)</span>
<span id="cb52-169"><a href="mod-mc.html#cb52-169" tabindex="-1"></a>            <span class="cf">if</span> (t <span class="sc">==</span> self<span class="sc">$</span>maxT) { <span class="co"># sell remaining</span></span>
<span id="cb52-170"><a href="mod-mc.html#cb52-170" tabindex="-1"></a>               a <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionUCB</span>(s, coeff)</span>
<span id="cb52-171"><a href="mod-mc.html#cb52-171" tabindex="-1"></a>               r <span class="ot">&lt;-</span> self<span class="sc">$</span>scrapPrice <span class="sc">*</span> q</span>
<span id="cb52-172"><a href="mod-mc.html#cb52-172" tabindex="-1"></a>               q <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># new q value</span></span>
<span id="cb52-173"><a href="mod-mc.html#cb52-173" tabindex="-1"></a>            } <span class="cf">else</span> {</span>
<span id="cb52-174"><a href="mod-mc.html#cb52-174" tabindex="-1"></a>               <span class="co"># actions &lt;- names(agent$model[[s]]$pi)</span></span>
<span id="cb52-175"><a href="mod-mc.html#cb52-175" tabindex="-1"></a>               <span class="co"># a &lt;- actions[sample(1:length(actions), 1, prob = agent$model[[s]]$pi)]</span></span>
<span id="cb52-176"><a href="mod-mc.html#cb52-176" tabindex="-1"></a>               <span class="co"># a &lt;- agent$getActionUCB(s, coeff)</span></span>
<span id="cb52-177"><a href="mod-mc.html#cb52-177" tabindex="-1"></a>               <span class="co"># epsN &lt;- eps * (1/(agent$model[[s]]$n+1))^0.28</span></span>
<span id="cb52-178"><a href="mod-mc.html#cb52-178" tabindex="-1"></a>               epsN <span class="ot">&lt;-</span> eps</span>
<span id="cb52-179"><a href="mod-mc.html#cb52-179" tabindex="-1"></a>               a <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionEG</span>(s, epsN)</span>
<span id="cb52-180"><a href="mod-mc.html#cb52-180" tabindex="-1"></a>               <span class="co"># cat(a, &quot; &quot;)</span></span>
<span id="cb52-181"><a href="mod-mc.html#cb52-181" tabindex="-1"></a>               price <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(a)</span>
<span id="cb52-182"><a href="mod-mc.html#cb52-182" tabindex="-1"></a>               d <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getDemand</span>(price, t)</span>
<span id="cb52-183"><a href="mod-mc.html#cb52-183" tabindex="-1"></a>               <span class="cf">if</span> (t <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb52-184"><a href="mod-mc.html#cb52-184" tabindex="-1"></a>                  r <span class="ot">&lt;-</span> price <span class="sc">*</span> <span class="fu">min</span>(q, d) <span class="sc">-</span> q <span class="sc">*</span> self<span class="sc">$</span>purchasePrice</span>
<span id="cb52-185"><a href="mod-mc.html#cb52-185" tabindex="-1"></a>               } <span class="cf">else</span> {</span>
<span id="cb52-186"><a href="mod-mc.html#cb52-186" tabindex="-1"></a>                  r <span class="ot">&lt;-</span> price <span class="sc">*</span> <span class="fu">min</span>(q, d)</span>
<span id="cb52-187"><a href="mod-mc.html#cb52-187" tabindex="-1"></a>               }</span>
<span id="cb52-188"><a href="mod-mc.html#cb52-188" tabindex="-1"></a>               q <span class="ot">&lt;-</span> q <span class="sc">-</span> <span class="fu">min</span>(q, d)  <span class="co"># new q value</span></span>
<span id="cb52-189"><a href="mod-mc.html#cb52-189" tabindex="-1"></a>               t <span class="ot">&lt;-</span> t <span class="sc">+</span> <span class="dv">1</span> <span class="co"># new t value</span></span>
<span id="cb52-190"><a href="mod-mc.html#cb52-190" tabindex="-1"></a>            }</span>
<span id="cb52-191"><a href="mod-mc.html#cb52-191" tabindex="-1"></a>            df<span class="sc">$</span>s[i] <span class="ot">&lt;-</span> s</span>
<span id="cb52-192"><a href="mod-mc.html#cb52-192" tabindex="-1"></a>            df<span class="sc">$</span>a[i] <span class="ot">&lt;-</span> a</span>
<span id="cb52-193"><a href="mod-mc.html#cb52-193" tabindex="-1"></a>            df<span class="sc">$</span>r[i] <span class="ot">&lt;-</span> r</span>
<span id="cb52-194"><a href="mod-mc.html#cb52-194" tabindex="-1"></a>         }</span>
<span id="cb52-195"><a href="mod-mc.html#cb52-195" tabindex="-1"></a>         <span class="fu">return</span>(df)</span>
<span id="cb52-196"><a href="mod-mc.html#cb52-196" tabindex="-1"></a>      }</span>
<span id="cb52-197"><a href="mod-mc.html#cb52-197" tabindex="-1"></a>   )</span>
<span id="cb52-198"><a href="mod-mc.html#cb52-198" tabindex="-1"></a>)</span></code></pre></div>
<p>Note that we define methods for getting the state and actions, an episode and the demand. Moreover, for this problem we many also use simulation to get the transition probabilities and the expected reward of a state-action pair (see method <code>getTransPrR</code>). That is, we may solve the problem using an MDP first and compare. Let us define the environment:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="mod-mc.html#cb53-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">876</span>)</span>
<span id="cb53-2"><a href="mod-mc.html#cb53-2" tabindex="-1"></a>prices <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>)</span>
<span id="cb53-3"><a href="mod-mc.html#cb53-3" tabindex="-1"></a>env <span class="ot">&lt;-</span> RLEnvSeasonal<span class="sc">$</span><span class="fu">new</span>(<span class="at">maxInv =</span> <span class="dv">100</span>, <span class="at">maxT =</span> <span class="dv">15</span>, <span class="at">scrapPrice =</span> <span class="dv">5</span>, <span class="at">purchasePrice =</span> <span class="dv">14</span>, prices)</span></code></pre></div>
<p>First let us try to find the optimal policy using an MDP:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="mod-mc.html#cb54-1" tabindex="-1"></a><span class="do">## Build the MDP</span></span>
<span id="cb54-2"><a href="mod-mc.html#cb54-2" tabindex="-1"></a>mdp <span class="ot">&lt;-</span> MDPClass<span class="sc">$</span><span class="fu">new</span>()                           <span class="co"># initialize mdp object</span></span>
<span id="cb54-3"><a href="mod-mc.html#cb54-3" tabindex="-1"></a>mdp<span class="sc">$</span><span class="fu">addStateSpace</span>(env<span class="sc">$</span><span class="fu">getStates</span>())              <span class="co"># add states</span></span>
<span id="cb54-4"><a href="mod-mc.html#cb54-4" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> mdp<span class="sc">$</span><span class="fu">getStateKeys</span>()) {                 <span class="co"># add actions</span></span>
<span id="cb54-5"><a href="mod-mc.html#cb54-5" tabindex="-1"></a>   mdp<span class="sc">$</span><span class="fu">addActionSpace</span>(s, env<span class="sc">$</span><span class="fu">getActions</span>(s))</span>
<span id="cb54-6"><a href="mod-mc.html#cb54-6" tabindex="-1"></a>} </span>
<span id="cb54-7"><a href="mod-mc.html#cb54-7" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> mdp<span class="sc">$</span><span class="fu">getStateKeys</span>()) {                 <span class="co"># add trans pr and reward (this will take some time!)</span></span>
<span id="cb54-8"><a href="mod-mc.html#cb54-8" tabindex="-1"></a>   <span class="cf">for</span> (a <span class="cf">in</span> mdp<span class="sc">$</span><span class="fu">getActionKeys</span>(s)) {</span>
<span id="cb54-9"><a href="mod-mc.html#cb54-9" tabindex="-1"></a>      lst <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getTransPrR</span>(s, a, <span class="at">runs =</span> <span class="dv">1000</span>)</span>
<span id="cb54-10"><a href="mod-mc.html#cb54-10" tabindex="-1"></a>      mdp<span class="sc">$</span><span class="fu">addAction</span>(s, a, <span class="at">r =</span> lst<span class="sc">$</span>r, <span class="at">pr =</span> lst<span class="sc">$</span>pr)</span>
<span id="cb54-11"><a href="mod-mc.html#cb54-11" tabindex="-1"></a>   }</span>
<span id="cb54-12"><a href="mod-mc.html#cb54-12" tabindex="-1"></a>}</span>
<span id="cb54-13"><a href="mod-mc.html#cb54-13" tabindex="-1"></a><span class="do">## Solve the MDP</span></span>
<span id="cb54-14"><a href="mod-mc.html#cb54-14" tabindex="-1"></a>mdp<span class="sc">$</span><span class="fu">policyIte</span>(<span class="at">gamma =</span> <span class="dv">1</span>)</span>
<span id="cb54-15"><a href="mod-mc.html#cb54-15" tabindex="-1"></a><span class="co"># mdp$valueIte(gamma = 1)</span></span>
<span id="cb54-16"><a href="mod-mc.html#cb54-16" tabindex="-1"></a>dfMDP <span class="ot">&lt;-</span> <span class="fu">left_join</span>(mdp<span class="sc">$</span><span class="fu">getStateValues</span>(), mdp<span class="sc">$</span><span class="fu">getPolicy</span>(), <span class="at">by =</span> <span class="st">&quot;state&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb54-17"><a href="mod-mc.html#cb54-17" tabindex="-1"></a>   <span class="fu">select</span>(<span class="sc">-</span>pr) <span class="sc">%&gt;%</span> </span>
<span id="cb54-18"><a href="mod-mc.html#cb54-18" tabindex="-1"></a>   <span class="fu">separate</span>(state, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;inv&quot;</span>, <span class="st">&quot;t&quot;</span>), <span class="at">remove =</span> F, <span class="at">convert =</span> T)</span></code></pre></div>
<p>Let us plot the optimal policy (see Figure <a href="mod-mc.html#fig:seasonal-mdp-plot">6.5</a>):</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="mod-mc.html#cb55-1" tabindex="-1"></a>dfMDP <span class="sc">%&gt;%</span> <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(t)) <span class="sc">%&gt;%</span> </span>
<span id="cb55-2"><a href="mod-mc.html#cb55-2" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> t, <span class="at">y =</span> inv, <span class="at">col =</span> action)) <span class="sc">+</span></span>
<span id="cb55-3"><a href="mod-mc.html#cb55-3" tabindex="-1"></a>   <span class="fu">geom_point</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:seasonal-mdp-plot"></span>
<img src="06_mc_files/figure-html/seasonal-mdp-plot-1.png" alt="Optimal policy for the MDP." width="672" />
<p class="caption">
Figure 6.5: Optimal policy for the MDP.
</p>
</div>
<p>Note in general we change the price based on a diagonal line running from upper-left to lower-right. Some time two prices oscillate given a time due to state-values close to each other.</p>
<p>The best inventory level to order can be found by searching among the state-values at time one.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="mod-mc.html#cb56-1" tabindex="-1"></a>res <span class="ot">&lt;-</span> dfMDP <span class="sc">%&gt;%</span> </span>
<span id="cb56-2"><a href="mod-mc.html#cb56-2" tabindex="-1"></a>   <span class="fu">filter</span>(t <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb56-3"><a href="mod-mc.html#cb56-3" tabindex="-1"></a>   <span class="fu">arrange</span>(<span class="fu">desc</span>(v)) <span class="sc">%&gt;%</span> </span>
<span id="cb56-4"><a href="mod-mc.html#cb56-4" tabindex="-1"></a>   <span class="fu">print</span>()</span>
<span id="cb56-5"><a href="mod-mc.html#cb56-5" tabindex="-1"></a><span class="co">#&gt; # A tibble: 100 × 5</span></span>
<span id="cb56-6"><a href="mod-mc.html#cb56-6" tabindex="-1"></a><span class="co">#&gt;    state   inv     t     v action</span></span>
<span id="cb56-7"><a href="mod-mc.html#cb56-7" tabindex="-1"></a><span class="co">#&gt;    &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; </span></span>
<span id="cb56-8"><a href="mod-mc.html#cb56-8" tabindex="-1"></a><span class="co">#&gt;  1 64,1     64     1  358. 20    </span></span>
<span id="cb56-9"><a href="mod-mc.html#cb56-9" tabindex="-1"></a><span class="co">#&gt;  2 62,1     62     1  358. 20    </span></span>
<span id="cb56-10"><a href="mod-mc.html#cb56-10" tabindex="-1"></a><span class="co">#&gt;  3 63,1     63     1  358. 20    </span></span>
<span id="cb56-11"><a href="mod-mc.html#cb56-11" tabindex="-1"></a><span class="co">#&gt;  4 61,1     61     1  357. 20    </span></span>
<span id="cb56-12"><a href="mod-mc.html#cb56-12" tabindex="-1"></a><span class="co">#&gt;  5 65,1     65     1  357. 20    </span></span>
<span id="cb56-13"><a href="mod-mc.html#cb56-13" tabindex="-1"></a><span class="co">#&gt;  6 66,1     66     1  355. 20    </span></span>
<span id="cb56-14"><a href="mod-mc.html#cb56-14" tabindex="-1"></a><span class="co">#&gt;  7 60,1     60     1  355. 20    </span></span>
<span id="cb56-15"><a href="mod-mc.html#cb56-15" tabindex="-1"></a><span class="co">#&gt;  8 67,1     67     1  353. 20    </span></span>
<span id="cb56-16"><a href="mod-mc.html#cb56-16" tabindex="-1"></a><span class="co">#&gt;  9 59,1     59     1  352. 20    </span></span>
<span id="cb56-17"><a href="mod-mc.html#cb56-17" tabindex="-1"></a><span class="co">#&gt; 10 68,1     68     1  351. 20    </span></span>
<span id="cb56-18"><a href="mod-mc.html#cb56-18" tabindex="-1"></a><span class="co">#&gt; # ℹ 90 more rows</span></span></code></pre></div>
<p>This is approx around 64 items. If ordering 100 items we would loose:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="mod-mc.html#cb57-1" tabindex="-1"></a>res<span class="sc">$</span>v[<span class="dv">1</span>] <span class="sc">-</span> dfMDP <span class="sc">%&gt;%</span> </span>
<span id="cb57-2"><a href="mod-mc.html#cb57-2" tabindex="-1"></a>   <span class="fu">filter</span>(state <span class="sc">==</span> <span class="st">&quot;100,1&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(v)</span>
<span id="cb57-3"><a href="mod-mc.html#cb57-3" tabindex="-1"></a><span class="co">#&gt; 64,1.20 </span></span>
<span id="cb57-4"><a href="mod-mc.html#cb57-4" tabindex="-1"></a><span class="co">#&gt;    91.3</span></span></code></pre></div>
<p>Let us now try to use RL and MC to approximate the best policy. First, we define a generic RL agent:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="mod-mc.html#cb58-1" tabindex="-1"></a><span class="fu">library</span>(R6)</span>
<span id="cb58-2"><a href="mod-mc.html#cb58-2" tabindex="-1"></a><span class="fu">library</span>(hash)</span>
<span id="cb58-3"><a href="mod-mc.html#cb58-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb58-4"><a href="mod-mc.html#cb58-4" tabindex="-1"></a></span>
<span id="cb58-5"><a href="mod-mc.html#cb58-5" tabindex="-1"></a><span class="do">## Generic RL agent for tabular data (R6 class)</span></span>
<span id="cb58-6"><a href="mod-mc.html#cb58-6" tabindex="-1"></a>RLAgent <span class="ot">&lt;-</span> <span class="fu">R6Class</span>(<span class="st">&quot;RLAgent&quot;</span>,</span>
<span id="cb58-7"><a href="mod-mc.html#cb58-7" tabindex="-1"></a>   <span class="at">public =</span> <span class="fu">list</span>(</span>
<span id="cb58-8"><a href="mod-mc.html#cb58-8" tabindex="-1"></a>      <span class="co">#&#39; @field model The model is used to represent the information we have. The</span></span>
<span id="cb58-9"><a href="mod-mc.html#cb58-9" tabindex="-1"></a>      <span class="co">#&#39; model is represented using a hash list for the states. Each states contains </span></span>
<span id="cb58-10"><a href="mod-mc.html#cb58-10" tabindex="-1"></a>      <span class="co">#&#39;    - A list with `actions` (a hash #&#39; list with actions).</span></span>
<span id="cb58-11"><a href="mod-mc.html#cb58-11" tabindex="-1"></a>      <span class="co">#&#39;    - `pi` (a named vector with policy pr (only positive values).</span></span>
<span id="cb58-12"><a href="mod-mc.html#cb58-12" tabindex="-1"></a>      <span class="co">#&#39;    - `piG` the greedy action (a string). </span></span>
<span id="cb58-13"><a href="mod-mc.html#cb58-13" tabindex="-1"></a>      <span class="co">#&#39;    - `n` a visit counter</span></span>
<span id="cb58-14"><a href="mod-mc.html#cb58-14" tabindex="-1"></a>      <span class="co">#&#39; The `actions` hash list contains </span></span>
<span id="cb58-15"><a href="mod-mc.html#cb58-15" tabindex="-1"></a>      <span class="co">#&#39;    - The action-values `q`.</span></span>
<span id="cb58-16"><a href="mod-mc.html#cb58-16" tabindex="-1"></a>      <span class="co">#&#39;    - `n` a visit counter.</span></span>
<span id="cb58-17"><a href="mod-mc.html#cb58-17" tabindex="-1"></a>      <span class="at">model =</span> <span class="cn">NULL</span>,  </span>
<span id="cb58-18"><a href="mod-mc.html#cb58-18" tabindex="-1"></a>      </span>
<span id="cb58-19"><a href="mod-mc.html#cb58-19" tabindex="-1"></a>      <span class="co">#&#39; @description Create an object (when call new).</span></span>
<span id="cb58-20"><a href="mod-mc.html#cb58-20" tabindex="-1"></a>      <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb58-21"><a href="mod-mc.html#cb58-21" tabindex="-1"></a>         self<span class="sc">$</span>model <span class="ot">&lt;-</span> <span class="fu">hash</span>()</span>
<span id="cb58-22"><a href="mod-mc.html#cb58-22" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-23"><a href="mod-mc.html#cb58-23" tabindex="-1"></a>      },</span>
<span id="cb58-24"><a href="mod-mc.html#cb58-24" tabindex="-1"></a></span>
<span id="cb58-25"><a href="mod-mc.html#cb58-25" tabindex="-1"></a>      <span class="co">#&#39; @description Add state and action to the hash (only if not already added)</span></span>
<span id="cb58-26"><a href="mod-mc.html#cb58-26" tabindex="-1"></a>      <span class="co">#&#39; @param s State key/string.</span></span>
<span id="cb58-27"><a href="mod-mc.html#cb58-27" tabindex="-1"></a>      <span class="co">#&#39; @param a Action key/string.</span></span>
<span id="cb58-28"><a href="mod-mc.html#cb58-28" tabindex="-1"></a>      <span class="at">addStateAction =</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb58-29"><a href="mod-mc.html#cb58-29" tabindex="-1"></a>         <span class="cf">if</span> (<span class="sc">!</span><span class="fu">has.key</span>(s, self<span class="sc">$</span>model)) <span class="fu">addStates</span>(s)</span>
<span id="cb58-30"><a href="mod-mc.html#cb58-30" tabindex="-1"></a>         <span class="cf">if</span> (<span class="sc">!</span><span class="fu">has.key</span>(a, self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)) self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]] <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">q =</span> <span class="dv">0</span>, <span class="at">n =</span> <span class="dv">0</span>)</span>
<span id="cb58-31"><a href="mod-mc.html#cb58-31" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-32"><a href="mod-mc.html#cb58-32" tabindex="-1"></a>      },</span>
<span id="cb58-33"><a href="mod-mc.html#cb58-33" tabindex="-1"></a>      </span>
<span id="cb58-34"><a href="mod-mc.html#cb58-34" tabindex="-1"></a>      <span class="co">#&#39; @description Add the states (keys) and define void policy and empty action hash. </span></span>
<span id="cb58-35"><a href="mod-mc.html#cb58-35" tabindex="-1"></a>      <span class="co">#&#39; @param states A vector of states (converted to strings).</span></span>
<span id="cb58-36"><a href="mod-mc.html#cb58-36" tabindex="-1"></a>      <span class="at">addStates =</span> <span class="cf">function</span>(states) {</span>
<span id="cb58-37"><a href="mod-mc.html#cb58-37" tabindex="-1"></a>         keys <span class="ot">&lt;-</span> <span class="fu">make.keys</span>(states)</span>
<span id="cb58-38"><a href="mod-mc.html#cb58-38" tabindex="-1"></a>         self<span class="sc">$</span>model[keys] <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">pi =</span> <span class="cn">NA</span>)   <span class="co"># don&#39;t use pi = NULL since then won&#39;t be defined </span></span>
<span id="cb58-39"><a href="mod-mc.html#cb58-39" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> keys) {</span>
<span id="cb58-40"><a href="mod-mc.html#cb58-40" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb58-41"><a href="mod-mc.html#cb58-41" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions <span class="ot">&lt;-</span> <span class="fu">hash</span>()</span>
<span id="cb58-42"><a href="mod-mc.html#cb58-42" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="ot">&lt;-</span> <span class="dv">0</span>  <span class="co"># counter visited</span></span>
<span id="cb58-43"><a href="mod-mc.html#cb58-43" tabindex="-1"></a>         }</span>
<span id="cb58-44"><a href="mod-mc.html#cb58-44" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-45"><a href="mod-mc.html#cb58-45" tabindex="-1"></a>      },</span>
<span id="cb58-46"><a href="mod-mc.html#cb58-46" tabindex="-1"></a>      </span>
<span id="cb58-47"><a href="mod-mc.html#cb58-47" tabindex="-1"></a>      <span class="co">#&#39; @description Add the actions to a state</span></span>
<span id="cb58-48"><a href="mod-mc.html#cb58-48" tabindex="-1"></a>      <span class="co">#&#39; @param s State (key).</span></span>
<span id="cb58-49"><a href="mod-mc.html#cb58-49" tabindex="-1"></a>      <span class="co">#&#39; @param actions A vector of actions (converted to strings).</span></span>
<span id="cb58-50"><a href="mod-mc.html#cb58-50" tabindex="-1"></a>      <span class="at">addActions =</span> <span class="cf">function</span>(s, actions) {</span>
<span id="cb58-51"><a href="mod-mc.html#cb58-51" tabindex="-1"></a>         keys <span class="ot">&lt;-</span> <span class="fu">make.keys</span>(actions)</span>
<span id="cb58-52"><a href="mod-mc.html#cb58-52" tabindex="-1"></a>         <span class="cf">for</span> (a <span class="cf">in</span> keys) {</span>
<span id="cb58-53"><a href="mod-mc.html#cb58-53" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">addStateAction</span>(s, a)</span>
<span id="cb58-54"><a href="mod-mc.html#cb58-54" tabindex="-1"></a>         }</span>
<span id="cb58-55"><a href="mod-mc.html#cb58-55" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-56"><a href="mod-mc.html#cb58-56" tabindex="-1"></a>      },</span>
<span id="cb58-57"><a href="mod-mc.html#cb58-57" tabindex="-1"></a>      </span>
<span id="cb58-58"><a href="mod-mc.html#cb58-58" tabindex="-1"></a>      <span class="co">#&#39; @description Add states and actions to the hash with initial values. If already exists nothing happens. </span></span>
<span id="cb58-59"><a href="mod-mc.html#cb58-59" tabindex="-1"></a>      <span class="co">#&#39; @param df A tibble with string columns `s` (states) and `a` (actions).</span></span>
<span id="cb58-60"><a href="mod-mc.html#cb58-60" tabindex="-1"></a>      <span class="at">addStatesAndActions =</span> <span class="cf">function</span>(df) {</span>
<span id="cb58-61"><a href="mod-mc.html#cb58-61" tabindex="-1"></a>         <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)) {</span>
<span id="cb58-62"><a href="mod-mc.html#cb58-62" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">addStateAction</span>(df<span class="sc">$</span>s[i], df<span class="sc">$</span>a[i])</span>
<span id="cb58-63"><a href="mod-mc.html#cb58-63" tabindex="-1"></a>         }</span>
<span id="cb58-64"><a href="mod-mc.html#cb58-64" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-65"><a href="mod-mc.html#cb58-65" tabindex="-1"></a>      },</span>
<span id="cb58-66"><a href="mod-mc.html#cb58-66" tabindex="-1"></a>      </span>
<span id="cb58-67"><a href="mod-mc.html#cb58-67" tabindex="-1"></a>      <span class="co">#&#39; @description Set the action-values for all actions.</span></span>
<span id="cb58-68"><a href="mod-mc.html#cb58-68" tabindex="-1"></a>      <span class="co">#&#39; @param value The value.</span></span>
<span id="cb58-69"><a href="mod-mc.html#cb58-69" tabindex="-1"></a>      <span class="at">setActionValue =</span> <span class="cf">function</span>(<span class="at">value =</span> <span class="dv">0</span>) {</span>
<span id="cb58-70"><a href="mod-mc.html#cb58-70" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)) {</span>
<span id="cb58-71"><a href="mod-mc.html#cb58-71" tabindex="-1"></a>            <span class="cf">for</span> (a <span class="cf">in</span> <span class="fu">keys</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)) {</span>
<span id="cb58-72"><a href="mod-mc.html#cb58-72" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">=</span> value</span>
<span id="cb58-73"><a href="mod-mc.html#cb58-73" tabindex="-1"></a>            }</span>
<span id="cb58-74"><a href="mod-mc.html#cb58-74" tabindex="-1"></a>         }</span>
<span id="cb58-75"><a href="mod-mc.html#cb58-75" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-76"><a href="mod-mc.html#cb58-76" tabindex="-1"></a>      },</span>
<span id="cb58-77"><a href="mod-mc.html#cb58-77" tabindex="-1"></a>      </span>
<span id="cb58-78"><a href="mod-mc.html#cb58-78" tabindex="-1"></a>      <span class="co">#&#39; @description Set the state-value of states</span></span>
<span id="cb58-79"><a href="mod-mc.html#cb58-79" tabindex="-1"></a>      <span class="co">#&#39; @param states A vector of states.</span></span>
<span id="cb58-80"><a href="mod-mc.html#cb58-80" tabindex="-1"></a>      <span class="co">#&#39; @param value The value.</span></span>
<span id="cb58-81"><a href="mod-mc.html#cb58-81" tabindex="-1"></a>      <span class="at">setStateValue =</span> <span class="cf">function</span>(<span class="at">states =</span> <span class="fu">keys</span>(self<span class="sc">$</span>model), <span class="at">value =</span> <span class="dv">0</span>) {</span>
<span id="cb58-82"><a href="mod-mc.html#cb58-82" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-83"><a href="mod-mc.html#cb58-83" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v <span class="ot">&lt;-</span> value</span>
<span id="cb58-84"><a href="mod-mc.html#cb58-84" tabindex="-1"></a>         }</span>
<span id="cb58-85"><a href="mod-mc.html#cb58-85" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-86"><a href="mod-mc.html#cb58-86" tabindex="-1"></a>      },</span>
<span id="cb58-87"><a href="mod-mc.html#cb58-87" tabindex="-1"></a>      </span>
<span id="cb58-88"><a href="mod-mc.html#cb58-88" tabindex="-1"></a>      <span class="co">#&#39; @description Set the action visit counter values for all actions.</span></span>
<span id="cb58-89"><a href="mod-mc.html#cb58-89" tabindex="-1"></a>      <span class="co">#&#39; @param ctrValue Counter value.</span></span>
<span id="cb58-90"><a href="mod-mc.html#cb58-90" tabindex="-1"></a>      <span class="at">setActionCtrValue =</span> <span class="cf">function</span>(<span class="at">ctrValue =</span> <span class="dv">0</span>) {</span>
<span id="cb58-91"><a href="mod-mc.html#cb58-91" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)) {</span>
<span id="cb58-92"><a href="mod-mc.html#cb58-92" tabindex="-1"></a>            <span class="cf">for</span> (a <span class="cf">in</span> <span class="fu">keys</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)) {</span>
<span id="cb58-93"><a href="mod-mc.html#cb58-93" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="ot">=</span> ctrValue</span>
<span id="cb58-94"><a href="mod-mc.html#cb58-94" tabindex="-1"></a>            }</span>
<span id="cb58-95"><a href="mod-mc.html#cb58-95" tabindex="-1"></a>         }</span>
<span id="cb58-96"><a href="mod-mc.html#cb58-96" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-97"><a href="mod-mc.html#cb58-97" tabindex="-1"></a>      },</span>
<span id="cb58-98"><a href="mod-mc.html#cb58-98" tabindex="-1"></a>      </span>
<span id="cb58-99"><a href="mod-mc.html#cb58-99" tabindex="-1"></a>      <span class="co">#&#39; @description Set the action-values for a single action (including the counter values).</span></span>
<span id="cb58-100"><a href="mod-mc.html#cb58-100" tabindex="-1"></a>      <span class="co">#&#39; @param value The value.</span></span>
<span id="cb58-101"><a href="mod-mc.html#cb58-101" tabindex="-1"></a>      <span class="co">#&#39; @param ctrValue Counter value.</span></span>
<span id="cb58-102"><a href="mod-mc.html#cb58-102" tabindex="-1"></a>      <span class="at">setActionValueSingle =</span> <span class="cf">function</span>(<span class="at">value =</span> <span class="dv">0</span>, <span class="at">ctrValue =</span> <span class="dv">0</span>, s, a) {</span>
<span id="cb58-103"><a href="mod-mc.html#cb58-103" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">=</span> value</span>
<span id="cb58-104"><a href="mod-mc.html#cb58-104" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="ot">=</span> ctrValue</span>
<span id="cb58-105"><a href="mod-mc.html#cb58-105" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-106"><a href="mod-mc.html#cb58-106" tabindex="-1"></a>      },</span>
<span id="cb58-107"><a href="mod-mc.html#cb58-107" tabindex="-1"></a>      </span>
<span id="cb58-108"><a href="mod-mc.html#cb58-108" tabindex="-1"></a>      <span class="co">#&#39; @description Set the policy to a random epsilon-greedy policy.</span></span>
<span id="cb58-109"><a href="mod-mc.html#cb58-109" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used in epsilon-greedy policy.</span></span>
<span id="cb58-110"><a href="mod-mc.html#cb58-110" tabindex="-1"></a>      <span class="at">setRandomEpsGreedyPolicy =</span> <span class="cf">function</span>(eps) {</span>
<span id="cb58-111"><a href="mod-mc.html#cb58-111" tabindex="-1"></a>         states <span class="ot">&lt;-</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)</span>
<span id="cb58-112"><a href="mod-mc.html#cb58-112" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-113"><a href="mod-mc.html#cb58-113" tabindex="-1"></a>            actions <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)</span>
<span id="cb58-114"><a href="mod-mc.html#cb58-114" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi <span class="ot">&lt;-</span> <span class="fu">rep</span>(eps<span class="sc">/</span><span class="fu">length</span>(actions), <span class="fu">length</span>(actions))</span>
<span id="cb58-115"><a href="mod-mc.html#cb58-115" tabindex="-1"></a>            <span class="fu">names</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi) <span class="ot">&lt;-</span> actions</span>
<span id="cb58-116"><a href="mod-mc.html#cb58-116" tabindex="-1"></a>            piG <span class="ot">&lt;-</span> <span class="fu">sample</span>(self<span class="sc">$</span><span class="fu">getActionKeys</span>(s), <span class="dv">1</span>)</span>
<span id="cb58-117"><a href="mod-mc.html#cb58-117" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi[piG] <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi[piG] <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> eps</span>
<span id="cb58-118"><a href="mod-mc.html#cb58-118" tabindex="-1"></a>         }</span>
<span id="cb58-119"><a href="mod-mc.html#cb58-119" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-120"><a href="mod-mc.html#cb58-120" tabindex="-1"></a>      },</span>
<span id="cb58-121"><a href="mod-mc.html#cb58-121" tabindex="-1"></a>      </span>
<span id="cb58-122"><a href="mod-mc.html#cb58-122" tabindex="-1"></a>      <span class="co">#&#39; @description Set the policy to the optimal epsilon-greedy policy </span></span>
<span id="cb58-123"><a href="mod-mc.html#cb58-123" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used in epsilon-greedy policy.</span></span>
<span id="cb58-124"><a href="mod-mc.html#cb58-124" tabindex="-1"></a>      <span class="co">#&#39; @param states States under consideration.</span></span>
<span id="cb58-125"><a href="mod-mc.html#cb58-125" tabindex="-1"></a>      <span class="at">setEpsGreedyPolicy =</span> <span class="cf">function</span>(eps, states) {</span>
<span id="cb58-126"><a href="mod-mc.html#cb58-126" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-127"><a href="mod-mc.html#cb58-127" tabindex="-1"></a>            actions <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)</span>
<span id="cb58-128"><a href="mod-mc.html#cb58-128" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi <span class="ot">&lt;-</span> <span class="fu">rep</span>(eps<span class="sc">/</span><span class="fu">length</span>(actions), <span class="fu">length</span>(actions))</span>
<span id="cb58-129"><a href="mod-mc.html#cb58-129" tabindex="-1"></a>            <span class="fu">names</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi) <span class="ot">&lt;-</span> actions</span>
<span id="cb58-130"><a href="mod-mc.html#cb58-130" tabindex="-1"></a>            idx <span class="ot">&lt;-</span> nnet<span class="sc">::</span><span class="fu">which.is.max</span>(<span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,]))  <span class="co"># choose among max values at random</span></span>
<span id="cb58-131"><a href="mod-mc.html#cb58-131" tabindex="-1"></a>            <span class="co"># idx &lt;- which.max(unlist(values(self$model[[s]]$actions)[&quot;q&quot;,]))  # choose first max </span></span>
<span id="cb58-132"><a href="mod-mc.html#cb58-132" tabindex="-1"></a>            <span class="co"># self$model[[s]]$piG &lt;- actions[idx]</span></span>
<span id="cb58-133"><a href="mod-mc.html#cb58-133" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi[idx] <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi[idx] <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> eps</span>
<span id="cb58-134"><a href="mod-mc.html#cb58-134" tabindex="-1"></a>         }</span>
<span id="cb58-135"><a href="mod-mc.html#cb58-135" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-136"><a href="mod-mc.html#cb58-136" tabindex="-1"></a>      },</span>
<span id="cb58-137"><a href="mod-mc.html#cb58-137" tabindex="-1"></a>      </span>
<span id="cb58-138"><a href="mod-mc.html#cb58-138" tabindex="-1"></a>      <span class="co">#&#39; @description Set the greedy policy based on action-values. </span></span>
<span id="cb58-139"><a href="mod-mc.html#cb58-139" tabindex="-1"></a>      <span class="co">#&#39; @param states States under consideration.</span></span>
<span id="cb58-140"><a href="mod-mc.html#cb58-140" tabindex="-1"></a>      <span class="at">setGreedyPolicy =</span> <span class="cf">function</span>(<span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>()) {</span>
<span id="cb58-141"><a href="mod-mc.html#cb58-141" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-142"><a href="mod-mc.html#cb58-142" tabindex="-1"></a>            pi <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb58-143"><a href="mod-mc.html#cb58-143" tabindex="-1"></a>            actions <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)</span>
<span id="cb58-144"><a href="mod-mc.html#cb58-144" tabindex="-1"></a>            <span class="co"># idx &lt;- nnet::which.is.max(unlist(values(self$model[[s]]$actions)[&quot;q&quot;,]))  # choose among max values at random</span></span>
<span id="cb58-145"><a href="mod-mc.html#cb58-145" tabindex="-1"></a>            idx <span class="ot">&lt;-</span> <span class="fu">which.max</span>(<span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,]))  <span class="co"># choose first max</span></span>
<span id="cb58-146"><a href="mod-mc.html#cb58-146" tabindex="-1"></a>            <span class="fu">names</span>(pi) <span class="ot">&lt;-</span> actions[idx]</span>
<span id="cb58-147"><a href="mod-mc.html#cb58-147" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi <span class="ot">&lt;-</span> pi</span>
<span id="cb58-148"><a href="mod-mc.html#cb58-148" tabindex="-1"></a>         }</span>
<span id="cb58-149"><a href="mod-mc.html#cb58-149" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-150"><a href="mod-mc.html#cb58-150" tabindex="-1"></a>      },</span>
<span id="cb58-151"><a href="mod-mc.html#cb58-151" tabindex="-1"></a>      </span>
<span id="cb58-152"><a href="mod-mc.html#cb58-152" tabindex="-1"></a>      <span class="co">#&#39; @description Set the policy to the named vector pi for a set of states</span></span>
<span id="cb58-153"><a href="mod-mc.html#cb58-153" tabindex="-1"></a>      <span class="co">#&#39; @param states States under consideration.</span></span>
<span id="cb58-154"><a href="mod-mc.html#cb58-154" tabindex="-1"></a>      <span class="co">#&#39; @param pi A named vector with policy pr (only psitive values).</span></span>
<span id="cb58-155"><a href="mod-mc.html#cb58-155" tabindex="-1"></a>      <span class="at">setPolicy =</span> <span class="cf">function</span>(states, pi) {</span>
<span id="cb58-156"><a href="mod-mc.html#cb58-156" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-157"><a href="mod-mc.html#cb58-157" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi <span class="ot">&lt;-</span> pi</span>
<span id="cb58-158"><a href="mod-mc.html#cb58-158" tabindex="-1"></a>         }</span>
<span id="cb58-159"><a href="mod-mc.html#cb58-159" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-160"><a href="mod-mc.html#cb58-160" tabindex="-1"></a>      },</span>
<span id="cb58-161"><a href="mod-mc.html#cb58-161" tabindex="-1"></a>      </span>
<span id="cb58-162"><a href="mod-mc.html#cb58-162" tabindex="-1"></a>      <span class="co">#&#39; @description Set the state visit counter values for all states.</span></span>
<span id="cb58-163"><a href="mod-mc.html#cb58-163" tabindex="-1"></a>      <span class="co">#&#39; @param ctrValue Counter value.</span></span>
<span id="cb58-164"><a href="mod-mc.html#cb58-164" tabindex="-1"></a>      <span class="at">setStateCtrValue =</span> <span class="cf">function</span>(<span class="at">ctrValue =</span> <span class="dv">0</span>) {</span>
<span id="cb58-165"><a href="mod-mc.html#cb58-165" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)) {</span>
<span id="cb58-166"><a href="mod-mc.html#cb58-166" tabindex="-1"></a>            self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="ot">=</span> ctrValue</span>
<span id="cb58-167"><a href="mod-mc.html#cb58-167" tabindex="-1"></a>         }</span>
<span id="cb58-168"><a href="mod-mc.html#cb58-168" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-169"><a href="mod-mc.html#cb58-169" tabindex="-1"></a>      },</span>
<span id="cb58-170"><a href="mod-mc.html#cb58-170" tabindex="-1"></a>      </span>
<span id="cb58-171"><a href="mod-mc.html#cb58-171" tabindex="-1"></a>      <span class="co">#&#39; @description Return the state keys</span></span>
<span id="cb58-172"><a href="mod-mc.html#cb58-172" tabindex="-1"></a>      <span class="at">getStateKeys =</span> <span class="cf">function</span>() {</span>
<span id="cb58-173"><a href="mod-mc.html#cb58-173" tabindex="-1"></a>         <span class="fu">keys</span>(self<span class="sc">$</span>model)</span>
<span id="cb58-174"><a href="mod-mc.html#cb58-174" tabindex="-1"></a>      },</span>
<span id="cb58-175"><a href="mod-mc.html#cb58-175" tabindex="-1"></a>      </span>
<span id="cb58-176"><a href="mod-mc.html#cb58-176" tabindex="-1"></a>      <span class="co">#&#39; @description Return the state-value for a state and policy using the q/action-values </span></span>
<span id="cb58-177"><a href="mod-mc.html#cb58-177" tabindex="-1"></a>      <span class="co">#&#39; @param s A state.</span></span>
<span id="cb58-178"><a href="mod-mc.html#cb58-178" tabindex="-1"></a>      <span class="at">getStateValueQ =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-179"><a href="mod-mc.html#cb58-179" tabindex="-1"></a>         pi <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi</span>
<span id="cb58-180"><a href="mod-mc.html#cb58-180" tabindex="-1"></a>         <span class="co"># print(pi)</span></span>
<span id="cb58-181"><a href="mod-mc.html#cb58-181" tabindex="-1"></a>         val <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb58-182"><a href="mod-mc.html#cb58-182" tabindex="-1"></a>         <span class="cf">for</span> (a <span class="cf">in</span> <span class="fu">names</span>(pi)) {</span>
<span id="cb58-183"><a href="mod-mc.html#cb58-183" tabindex="-1"></a>            val <span class="ot">&lt;-</span> val <span class="sc">+</span> pi[a] <span class="sc">*</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q</span>
<span id="cb58-184"><a href="mod-mc.html#cb58-184" tabindex="-1"></a>            <span class="co"># print(self$model[[s]]$actions[[a]]$q)</span></span>
<span id="cb58-185"><a href="mod-mc.html#cb58-185" tabindex="-1"></a>         }</span>
<span id="cb58-186"><a href="mod-mc.html#cb58-186" tabindex="-1"></a>         <span class="co"># print(val)</span></span>
<span id="cb58-187"><a href="mod-mc.html#cb58-187" tabindex="-1"></a>         <span class="fu">return</span>(val)</span>
<span id="cb58-188"><a href="mod-mc.html#cb58-188" tabindex="-1"></a>      },</span>
<span id="cb58-189"><a href="mod-mc.html#cb58-189" tabindex="-1"></a>      </span>
<span id="cb58-190"><a href="mod-mc.html#cb58-190" tabindex="-1"></a>      <span class="co">#&#39; @description Return the state-values as a tibble</span></span>
<span id="cb58-191"><a href="mod-mc.html#cb58-191" tabindex="-1"></a>      <span class="co">#&#39; @param s A vector of state keys.</span></span>
<span id="cb58-192"><a href="mod-mc.html#cb58-192" tabindex="-1"></a>      <span class="at">getStateValues =</span> <span class="cf">function</span>(<span class="at">s =</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)) {</span>
<span id="cb58-193"><a href="mod-mc.html#cb58-193" tabindex="-1"></a>         <span class="fu">tibble</span>(<span class="at">state =</span> s) <span class="sc">%&gt;%</span> <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">v =</span> self<span class="sc">$</span>model[[state]]<span class="sc">$</span>v) </span>
<span id="cb58-194"><a href="mod-mc.html#cb58-194" tabindex="-1"></a>      },</span>
<span id="cb58-195"><a href="mod-mc.html#cb58-195" tabindex="-1"></a>      </span>
<span id="cb58-196"><a href="mod-mc.html#cb58-196" tabindex="-1"></a>      <span class="co">#&#39; @description Return the action keys</span></span>
<span id="cb58-197"><a href="mod-mc.html#cb58-197" tabindex="-1"></a>      <span class="co">#&#39; @param s The state considered.</span></span>
<span id="cb58-198"><a href="mod-mc.html#cb58-198" tabindex="-1"></a>      <span class="at">getActionKeys =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-199"><a href="mod-mc.html#cb58-199" tabindex="-1"></a>         <span class="fu">keys</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions) </span>
<span id="cb58-200"><a href="mod-mc.html#cb58-200" tabindex="-1"></a>      },</span>
<span id="cb58-201"><a href="mod-mc.html#cb58-201" tabindex="-1"></a>      </span>
<span id="cb58-202"><a href="mod-mc.html#cb58-202" tabindex="-1"></a>      <span class="co">#&#39; @description Return information about actions stored in a state</span></span>
<span id="cb58-203"><a href="mod-mc.html#cb58-203" tabindex="-1"></a>      <span class="co">#&#39; @param s The state considered.</span></span>
<span id="cb58-204"><a href="mod-mc.html#cb58-204" tabindex="-1"></a>      <span class="at">getActionInfo =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-205"><a href="mod-mc.html#cb58-205" tabindex="-1"></a>         <span class="fu">as.list</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions) </span>
<span id="cb58-206"><a href="mod-mc.html#cb58-206" tabindex="-1"></a>      },</span>
<span id="cb58-207"><a href="mod-mc.html#cb58-207" tabindex="-1"></a>      </span>
<span id="cb58-208"><a href="mod-mc.html#cb58-208" tabindex="-1"></a>      <span class="co">#&#39; @description Return the current policy as a tibble</span></span>
<span id="cb58-209"><a href="mod-mc.html#cb58-209" tabindex="-1"></a>      <span class="at">getPolicy =</span> <span class="cf">function</span>(<span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>()) {</span>
<span id="cb58-210"><a href="mod-mc.html#cb58-210" tabindex="-1"></a>         <span class="fu">map_dfr</span>(states, <span class="at">.f =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-211"><a href="mod-mc.html#cb58-211" tabindex="-1"></a>               <span class="fu">list</span>(<span class="at">state =</span> s, <span class="at">action =</span> <span class="fu">names</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi), <span class="at">pr =</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi)</span>
<span id="cb58-212"><a href="mod-mc.html#cb58-212" tabindex="-1"></a>            })</span>
<span id="cb58-213"><a href="mod-mc.html#cb58-213" tabindex="-1"></a>      },</span>
<span id="cb58-214"><a href="mod-mc.html#cb58-214" tabindex="-1"></a>      </span>
<span id="cb58-215"><a href="mod-mc.html#cb58-215" tabindex="-1"></a>      <span class="co">#&#39; @description Returns all action-values in a matrix (cols: actions, rows: states)</span></span>
<span id="cb58-216"><a href="mod-mc.html#cb58-216" tabindex="-1"></a>      <span class="at">getStateActionQMat =</span> <span class="cf">function</span>() {</span>
<span id="cb58-217"><a href="mod-mc.html#cb58-217" tabindex="-1"></a>         states <span class="ot">&lt;-</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)</span>
<span id="cb58-218"><a href="mod-mc.html#cb58-218" tabindex="-1"></a>         actions <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">unlist</span>(<span class="fu">sapply</span>(states, <span class="cf">function</span>(s) self<span class="sc">$</span><span class="fu">getActionKeys</span>(s))))</span>
<span id="cb58-219"><a href="mod-mc.html#cb58-219" tabindex="-1"></a>         m <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">length</span>(states), <span class="at">ncol =</span> <span class="fu">length</span>(actions))</span>
<span id="cb58-220"><a href="mod-mc.html#cb58-220" tabindex="-1"></a>         <span class="fu">colnames</span>(m) <span class="ot">&lt;-</span> actions</span>
<span id="cb58-221"><a href="mod-mc.html#cb58-221" tabindex="-1"></a>         <span class="fu">rownames</span>(m) <span class="ot">&lt;-</span> states</span>
<span id="cb58-222"><a href="mod-mc.html#cb58-222" tabindex="-1"></a>         <span class="cf">for</span> (s <span class="cf">in</span> states) {</span>
<span id="cb58-223"><a href="mod-mc.html#cb58-223" tabindex="-1"></a>            <span class="cf">for</span> (a <span class="cf">in</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)) {</span>
<span id="cb58-224"><a href="mod-mc.html#cb58-224" tabindex="-1"></a>               m[s, a] <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q</span>
<span id="cb58-225"><a href="mod-mc.html#cb58-225" tabindex="-1"></a>            }</span>
<span id="cb58-226"><a href="mod-mc.html#cb58-226" tabindex="-1"></a>         }</span>
<span id="cb58-227"><a href="mod-mc.html#cb58-227" tabindex="-1"></a>         <span class="fu">return</span>(m)</span>
<span id="cb58-228"><a href="mod-mc.html#cb58-228" tabindex="-1"></a>      },</span>
<span id="cb58-229"><a href="mod-mc.html#cb58-229" tabindex="-1"></a>      </span>
<span id="cb58-230"><a href="mod-mc.html#cb58-230" tabindex="-1"></a>      <span class="co">#&#39; @description Return the action-values as a tibble</span></span>
<span id="cb58-231"><a href="mod-mc.html#cb58-231" tabindex="-1"></a>      <span class="co">#&#39; @param states A vector of state keys.</span></span>
<span id="cb58-232"><a href="mod-mc.html#cb58-232" tabindex="-1"></a>      <span class="at">getActionValues =</span> <span class="cf">function</span>(<span class="at">states =</span> <span class="fu">keys</span>(self<span class="sc">$</span>model)) {</span>
<span id="cb58-233"><a href="mod-mc.html#cb58-233" tabindex="-1"></a>         <span class="fu">map_dfr</span>(states, <span class="at">.f =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-234"><a href="mod-mc.html#cb58-234" tabindex="-1"></a>               <span class="fu">list</span>(<span class="at">state =</span> s, <span class="at">action =</span> <span class="fu">keys</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions), <span class="at">q =</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,]), <span class="at">n =</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;n&quot;</span>,]))</span>
<span id="cb58-235"><a href="mod-mc.html#cb58-235" tabindex="-1"></a>            })</span>
<span id="cb58-236"><a href="mod-mc.html#cb58-236" tabindex="-1"></a>      },</span>
<span id="cb58-237"><a href="mod-mc.html#cb58-237" tabindex="-1"></a>      </span>
<span id="cb58-238"><a href="mod-mc.html#cb58-238" tabindex="-1"></a>      <span class="co">#&#39; @description Select next action using upper-confidence bound. Also update the visit counters for both state and selected action.</span></span>
<span id="cb58-239"><a href="mod-mc.html#cb58-239" tabindex="-1"></a>      <span class="co">#&#39; @return Action.</span></span>
<span id="cb58-240"><a href="mod-mc.html#cb58-240" tabindex="-1"></a>      <span class="at">getActionUCB =</span> <span class="cf">function</span>(s, <span class="at">coeff =</span> <span class="dv">1</span>) { </span>
<span id="cb58-241"><a href="mod-mc.html#cb58-241" tabindex="-1"></a>         actions <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)</span>
<span id="cb58-242"><a href="mod-mc.html#cb58-242" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># visit s</span></span>
<span id="cb58-243"><a href="mod-mc.html#cb58-243" tabindex="-1"></a>         qV <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,])</span>
<span id="cb58-244"><a href="mod-mc.html#cb58-244" tabindex="-1"></a>         nA <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;n&quot;</span>,])</span>
<span id="cb58-245"><a href="mod-mc.html#cb58-245" tabindex="-1"></a>         nS <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n</span>
<span id="cb58-246"><a href="mod-mc.html#cb58-246" tabindex="-1"></a>         val <span class="ot">&lt;-</span> qV <span class="sc">+</span> coeff <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="fu">log</span>(nS <span class="sc">+</span> <span class="fl">0.0001</span>)<span class="sc">/</span>nA)</span>
<span id="cb58-247"><a href="mod-mc.html#cb58-247" tabindex="-1"></a>         idx <span class="ot">&lt;-</span> <span class="fu">which.max</span>(val)</span>
<span id="cb58-248"><a href="mod-mc.html#cb58-248" tabindex="-1"></a>         a <span class="ot">&lt;-</span> actions[idx]</span>
<span id="cb58-249"><a href="mod-mc.html#cb58-249" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># note there is a risk here if use every-visit for an episode then will update more than once implying slower convergence. </span></span>
<span id="cb58-250"><a href="mod-mc.html#cb58-250" tabindex="-1"></a>         <span class="fu">return</span>(a)</span>
<span id="cb58-251"><a href="mod-mc.html#cb58-251" tabindex="-1"></a>      },</span>
<span id="cb58-252"><a href="mod-mc.html#cb58-252" tabindex="-1"></a>      </span>
<span id="cb58-253"><a href="mod-mc.html#cb58-253" tabindex="-1"></a>      <span class="co">#&#39; @description Select next action using epsilon-greedy policy based on action-values. Also update the visit counters for both state and selected action.</span></span>
<span id="cb58-254"><a href="mod-mc.html#cb58-254" tabindex="-1"></a>      <span class="co">#&#39; @return Action.</span></span>
<span id="cb58-255"><a href="mod-mc.html#cb58-255" tabindex="-1"></a>      <span class="at">getActionEG =</span> <span class="cf">function</span>(s, eps) {</span>
<span id="cb58-256"><a href="mod-mc.html#cb58-256" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># visit s</span></span>
<span id="cb58-257"><a href="mod-mc.html#cb58-257" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,])</span>
<span id="cb58-258"><a href="mod-mc.html#cb58-258" tabindex="-1"></a>         actions <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionKeys</span>(s)</span>
<span id="cb58-259"><a href="mod-mc.html#cb58-259" tabindex="-1"></a>         pi <span class="ot">&lt;-</span> <span class="fu">rep</span>(eps<span class="sc">/</span><span class="fu">length</span>(q), <span class="fu">length</span>(q))</span>
<span id="cb58-260"><a href="mod-mc.html#cb58-260" tabindex="-1"></a>         idx <span class="ot">&lt;-</span> nnet<span class="sc">::</span><span class="fu">which.is.max</span>(q)  <span class="co"># choose among max values at random</span></span>
<span id="cb58-261"><a href="mod-mc.html#cb58-261" tabindex="-1"></a>         <span class="co"># idx &lt;- which.max(unlist(values(self$model[[s]]$actions)[&quot;q&quot;,]))  # choose first max </span></span>
<span id="cb58-262"><a href="mod-mc.html#cb58-262" tabindex="-1"></a>         pi[idx] <span class="ot">&lt;-</span> pi[idx] <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> eps</span>
<span id="cb58-263"><a href="mod-mc.html#cb58-263" tabindex="-1"></a>         a <span class="ot">&lt;-</span> actions[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(actions), <span class="dv">1</span>, <span class="at">prob =</span> pi)]</span>
<span id="cb58-264"><a href="mod-mc.html#cb58-264" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># note there is a risk here if use every-visit for an episode then will update more than once implying slower convergence. </span></span>
<span id="cb58-265"><a href="mod-mc.html#cb58-265" tabindex="-1"></a>         <span class="fu">return</span>(a)</span>
<span id="cb58-266"><a href="mod-mc.html#cb58-266" tabindex="-1"></a>      },</span>
<span id="cb58-267"><a href="mod-mc.html#cb58-267" tabindex="-1"></a>      </span>
<span id="cb58-268"><a href="mod-mc.html#cb58-268" tabindex="-1"></a>      <span class="co">#&#39; @description Find maximum action value in a state.</span></span>
<span id="cb58-269"><a href="mod-mc.html#cb58-269" tabindex="-1"></a>      <span class="co">#&#39; @return Value.</span></span>
<span id="cb58-270"><a href="mod-mc.html#cb58-270" tabindex="-1"></a>      <span class="at">getMaxActionValue =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-271"><a href="mod-mc.html#cb58-271" tabindex="-1"></a>         q <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">values</span>(self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions)[<span class="st">&quot;q&quot;</span>,])</span>
<span id="cb58-272"><a href="mod-mc.html#cb58-272" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">max</span>(q))</span>
<span id="cb58-273"><a href="mod-mc.html#cb58-273" tabindex="-1"></a>      },</span>
<span id="cb58-274"><a href="mod-mc.html#cb58-274" tabindex="-1"></a>      </span>
<span id="cb58-275"><a href="mod-mc.html#cb58-275" tabindex="-1"></a>      <span class="co">#&#39; @description Find expected action value in a state based on current policy</span></span>
<span id="cb58-276"><a href="mod-mc.html#cb58-276" tabindex="-1"></a>      <span class="co">#&#39; @return Value.</span></span>
<span id="cb58-277"><a href="mod-mc.html#cb58-277" tabindex="-1"></a>      <span class="at">getExpActionValue =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-278"><a href="mod-mc.html#cb58-278" tabindex="-1"></a>         pi <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi</span>
<span id="cb58-279"><a href="mod-mc.html#cb58-279" tabindex="-1"></a>         a <span class="ot">&lt;-</span> <span class="fu">names</span>(pi)</span>
<span id="cb58-280"><a href="mod-mc.html#cb58-280" tabindex="-1"></a>         <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(pi)) {</span>
<span id="cb58-281"><a href="mod-mc.html#cb58-281" tabindex="-1"></a>            pi[i] <span class="ot">&lt;-</span> pi[i] <span class="sc">*</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a[i]]]<span class="sc">$</span>q</span>
<span id="cb58-282"><a href="mod-mc.html#cb58-282" tabindex="-1"></a>         }</span>
<span id="cb58-283"><a href="mod-mc.html#cb58-283" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">sum</span>(pi))</span>
<span id="cb58-284"><a href="mod-mc.html#cb58-284" tabindex="-1"></a>      },</span>
<span id="cb58-285"><a href="mod-mc.html#cb58-285" tabindex="-1"></a>      </span>
<span id="cb58-286"><a href="mod-mc.html#cb58-286" tabindex="-1"></a>      <span class="co">#&#39; @description Return and action sampled from the current policy pi. Also update the visit counters for both state and selected action.</span></span>
<span id="cb58-287"><a href="mod-mc.html#cb58-287" tabindex="-1"></a>      <span class="co">#&#39; @param s The state considered.</span></span>
<span id="cb58-288"><a href="mod-mc.html#cb58-288" tabindex="-1"></a>      <span class="at">getActionPi =</span> <span class="cf">function</span>(s) {</span>
<span id="cb58-289"><a href="mod-mc.html#cb58-289" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># visit s</span></span>
<span id="cb58-290"><a href="mod-mc.html#cb58-290" tabindex="-1"></a>         pi <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>pi</span>
<span id="cb58-291"><a href="mod-mc.html#cb58-291" tabindex="-1"></a>         actions <span class="ot">&lt;-</span> <span class="fu">names</span>(pi)</span>
<span id="cb58-292"><a href="mod-mc.html#cb58-292" tabindex="-1"></a>         a <span class="ot">&lt;-</span> <span class="fu">sample</span>(actions, <span class="dv">1</span>, <span class="at">prob =</span> pi)</span>
<span id="cb58-293"><a href="mod-mc.html#cb58-293" tabindex="-1"></a>         self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n <span class="sc">+</span> <span class="dv">1</span>  <span class="co"># note there is a risk here if use every-visit for an episode then will update more than once implying slower convergence. </span></span>
<span id="cb58-294"><a href="mod-mc.html#cb58-294" tabindex="-1"></a>         <span class="fu">return</span>(a)</span>
<span id="cb58-295"><a href="mod-mc.html#cb58-295" tabindex="-1"></a>      },</span>
<span id="cb58-296"><a href="mod-mc.html#cb58-296" tabindex="-1"></a>      </span>
<span id="cb58-297"><a href="mod-mc.html#cb58-297" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb58-298"><a href="mod-mc.html#cb58-298" tabindex="-1"></a><span class="co">#       getActionPi = function(s) {</span></span>
<span id="cb58-299"><a href="mod-mc.html#cb58-299" tabindex="-1"></a><span class="co">#          pi &lt;- self$model[[s]]$pi</span></span>
<span id="cb58-300"><a href="mod-mc.html#cb58-300" tabindex="-1"></a><span class="co">#          return(sample(names(pi), 1, prob = pi))</span></span>
<span id="cb58-301"><a href="mod-mc.html#cb58-301" tabindex="-1"></a><span class="co">#       },</span></span>
<span id="cb58-302"><a href="mod-mc.html#cb58-302" tabindex="-1"></a>      </span>
<span id="cb58-303"><a href="mod-mc.html#cb58-303" tabindex="-1"></a>      <span class="co">#&#39; @description Policy evaluation using TD(0)</span></span>
<span id="cb58-304"><a href="mod-mc.html#cb58-304" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getTimeStepData(s,a)` that return a list with elements `r` (reward) and `sN` (next state). </span></span>
<span id="cb58-305"><a href="mod-mc.html#cb58-305" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-306"><a href="mod-mc.html#cb58-306" tabindex="-1"></a>      <span class="co">#&#39; @param alpha Step-size (use a fixed step-size).</span></span>
<span id="cb58-307"><a href="mod-mc.html#cb58-307" tabindex="-1"></a>      <span class="co">#&#39; @param maxE Maximum number of episodes generated.  </span></span>
<span id="cb58-308"><a href="mod-mc.html#cb58-308" tabindex="-1"></a>      <span class="co">#&#39; @param maxEL Maximum episode length.</span></span>
<span id="cb58-309"><a href="mod-mc.html#cb58-309" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all state-values to 0.</span></span>
<span id="cb58-310"><a href="mod-mc.html#cb58-310" tabindex="-1"></a>      <span class="co">#&#39; @param states Possible start states of each episode (one picked at random).</span></span>
<span id="cb58-311"><a href="mod-mc.html#cb58-311" tabindex="-1"></a>      <span class="co">#&#39; @param ... Further arguments passed to `getEpisode` e.g the coefficient used for upper-confidence bound action selection. </span></span>
<span id="cb58-312"><a href="mod-mc.html#cb58-312" tabindex="-1"></a>      <span class="at">policyEvalTD0 =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">maxE =</span> <span class="dv">1000</span>, <span class="at">maxEL =</span> <span class="dv">10000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>()) {</span>
<span id="cb58-313"><a href="mod-mc.html#cb58-313" tabindex="-1"></a>         <span class="cf">if</span> (reset) self<span class="sc">$</span><span class="fu">setStateValue</span>(self<span class="sc">$</span><span class="fu">getStateKeys</span>())      <span class="co"># set to 0</span></span>
<span id="cb58-314"><a href="mod-mc.html#cb58-314" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxE) {</span>
<span id="cb58-315"><a href="mod-mc.html#cb58-315" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">sample</span>(states, <span class="dv">1</span>)  <span class="co"># pick start state among states</span></span>
<span id="cb58-316"><a href="mod-mc.html#cb58-316" tabindex="-1"></a>            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxEL) {  <span class="co"># for episode with ss as start (max length 100000)</span></span>
<span id="cb58-317"><a href="mod-mc.html#cb58-317" tabindex="-1"></a>               a <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionPi</span>(s)</span>
<span id="cb58-318"><a href="mod-mc.html#cb58-318" tabindex="-1"></a>               dat <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getTimeStepData</span>(s,a)  <span class="co"># get next state and reward</span></span>
<span id="cb58-319"><a href="mod-mc.html#cb58-319" tabindex="-1"></a>               r <span class="ot">&lt;-</span> dat<span class="sc">$</span>r</span>
<span id="cb58-320"><a href="mod-mc.html#cb58-320" tabindex="-1"></a>               sN <span class="ot">&lt;-</span> dat<span class="sc">$</span>sN</span>
<span id="cb58-321"><a href="mod-mc.html#cb58-321" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">is.na</span>(sN) <span class="sc">|</span> <span class="fu">is.na</span>(a)) <span class="cf">break</span>  <span class="co"># start generating new episode</span></span>
<span id="cb58-322"><a href="mod-mc.html#cb58-322" tabindex="-1"></a>               oldV <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v</span>
<span id="cb58-323"><a href="mod-mc.html#cb58-323" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v <span class="ot">&lt;-</span> oldV <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> self<span class="sc">$</span>model[[sN]]<span class="sc">$</span>v <span class="sc">-</span> oldV)</span>
<span id="cb58-324"><a href="mod-mc.html#cb58-324" tabindex="-1"></a>               s <span class="ot">&lt;-</span> sN</span>
<span id="cb58-325"><a href="mod-mc.html#cb58-325" tabindex="-1"></a>            }</span>
<span id="cb58-326"><a href="mod-mc.html#cb58-326" tabindex="-1"></a>            <span class="cf">if</span> (i <span class="sc">==</span> maxEL) <span class="cf">break</span></span>
<span id="cb58-327"><a href="mod-mc.html#cb58-327" tabindex="-1"></a>         }</span>
<span id="cb58-328"><a href="mod-mc.html#cb58-328" tabindex="-1"></a>      },</span>
<span id="cb58-329"><a href="mod-mc.html#cb58-329" tabindex="-1"></a>      </span>
<span id="cb58-330"><a href="mod-mc.html#cb58-330" tabindex="-1"></a>      </span>
<span id="cb58-331"><a href="mod-mc.html#cb58-331" tabindex="-1"></a>      <span class="co">#&#39; @description Policy evaluation using every-visit Monte Carlo sampling.  </span></span>
<span id="cb58-332"><a href="mod-mc.html#cb58-332" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getEpisode(agent, s, coeff)` that return an episode as a tibble with </span></span>
<span id="cb58-333"><a href="mod-mc.html#cb58-333" tabindex="-1"></a>      <span class="co">#&#39;    cols s, a, r (last col the terminal reward). This method also must update the visit counters if needed! This is also </span></span>
<span id="cb58-334"><a href="mod-mc.html#cb58-334" tabindex="-1"></a>      <span class="co">#&#39;    the method that decides which action selection method is used. </span></span>
<span id="cb58-335"><a href="mod-mc.html#cb58-335" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-336"><a href="mod-mc.html#cb58-336" tabindex="-1"></a>      <span class="co">#&#39; @param theta Threshold parameter.</span></span>
<span id="cb58-337"><a href="mod-mc.html#cb58-337" tabindex="-1"></a>      <span class="co">#&#39; @param minIte Minimum number of iterations for each start state (all `states` are used a start state in one iteration).</span></span>
<span id="cb58-338"><a href="mod-mc.html#cb58-338" tabindex="-1"></a>      <span class="co">#&#39; @param maxIte Maximum number of iterations for each start state (all `states` are used a start state in one iteration).</span></span>
<span id="cb58-339"><a href="mod-mc.html#cb58-339" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all state-values to 0.</span></span>
<span id="cb58-340"><a href="mod-mc.html#cb58-340" tabindex="-1"></a>      <span class="co">#&#39; @param states Start states in the episodes, which all are visited using a for loop.</span></span>
<span id="cb58-341"><a href="mod-mc.html#cb58-341" tabindex="-1"></a>      <span class="co">#&#39; @param verbose If true then print info for each episode.</span></span>
<span id="cb58-342"><a href="mod-mc.html#cb58-342" tabindex="-1"></a>      <span class="at">policyEvalMC =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">theta =</span> <span class="fl">0.1</span>, <span class="at">minIte =</span> <span class="dv">100</span>, <span class="at">maxIte =</span> <span class="dv">1000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>(), <span class="at">verbose =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb58-343"><a href="mod-mc.html#cb58-343" tabindex="-1"></a>         <span class="cf">if</span> (reset) {</span>
<span id="cb58-344"><a href="mod-mc.html#cb58-344" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setStateValue</span>()      <span class="co"># set to 0</span></span>
<span id="cb58-345"><a href="mod-mc.html#cb58-345" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionCtrValue</span>()   <span class="co"># reset counter</span></span>
<span id="cb58-346"><a href="mod-mc.html#cb58-346" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setStateCtrValue</span>()    <span class="co"># reset counter</span></span>
<span id="cb58-347"><a href="mod-mc.html#cb58-347" tabindex="-1"></a>         }</span>
<span id="cb58-348"><a href="mod-mc.html#cb58-348" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxIte) {</span>
<span id="cb58-349"><a href="mod-mc.html#cb58-349" tabindex="-1"></a>            delta <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb58-350"><a href="mod-mc.html#cb58-350" tabindex="-1"></a>            <span class="cf">for</span> (ss <span class="cf">in</span> states) {  <span class="co"># for episode with s as start</span></span>
<span id="cb58-351"><a href="mod-mc.html#cb58-351" tabindex="-1"></a>               df <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getEpisodePi</span>(self, ss)  <span class="co"># an episode stored in a tibble with cols s, a, r (last col the terminal reward)</span></span>
<span id="cb58-352"><a href="mod-mc.html#cb58-352" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">nrow</span>(df) <span class="sc">==</span> <span class="dv">0</span>) <span class="cf">next</span></span>
<span id="cb58-353"><a href="mod-mc.html#cb58-353" tabindex="-1"></a>               df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">nS =</span> <span class="cn">NA</span>, <span class="at">g =</span> <span class="cn">NA</span>, <span class="at">oldV =</span> <span class="cn">NA</span>, <span class="at">v =</span> <span class="cn">NA</span>)</span>
<span id="cb58-354"><a href="mod-mc.html#cb58-354" tabindex="-1"></a>               gain <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb58-355"><a href="mod-mc.html#cb58-355" tabindex="-1"></a>               <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">nrow</span>(df)<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb58-356"><a href="mod-mc.html#cb58-356" tabindex="-1"></a>                  s <span class="ot">&lt;-</span> df<span class="sc">$</span>s[i]</span>
<span id="cb58-357"><a href="mod-mc.html#cb58-357" tabindex="-1"></a>                  a <span class="ot">&lt;-</span> df<span class="sc">$</span>a[i]</span>
<span id="cb58-358"><a href="mod-mc.html#cb58-358" tabindex="-1"></a>                  gain <span class="ot">&lt;-</span> df<span class="sc">$</span>r[i] <span class="sc">+</span> gamma <span class="sc">*</span> gain</span>
<span id="cb58-359"><a href="mod-mc.html#cb58-359" tabindex="-1"></a>                  ctr <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n</span>
<span id="cb58-360"><a href="mod-mc.html#cb58-360" tabindex="-1"></a>                  oldV <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v</span>
<span id="cb58-361"><a href="mod-mc.html#cb58-361" tabindex="-1"></a>                  stepSize <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>ctr)</span>
<span id="cb58-362"><a href="mod-mc.html#cb58-362" tabindex="-1"></a>                  self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v <span class="ot">&lt;-</span> oldV <span class="sc">+</span> stepSize <span class="sc">*</span> (gain <span class="sc">-</span> oldV)</span>
<span id="cb58-363"><a href="mod-mc.html#cb58-363" tabindex="-1"></a>                  newV <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>v</span>
<span id="cb58-364"><a href="mod-mc.html#cb58-364" tabindex="-1"></a>                  delta <span class="ot">&lt;-</span> <span class="fu">max</span>(delta, <span class="fu">abs</span>(oldV <span class="sc">-</span> newV))</span>
<span id="cb58-365"><a href="mod-mc.html#cb58-365" tabindex="-1"></a>                  <span class="cf">if</span> (verbose) df<span class="sc">$</span>g[i] <span class="ot">&lt;-</span> gain; df<span class="sc">$</span>nS[i] <span class="ot">&lt;-</span> ctr; df<span class="sc">$</span>oldV[i] <span class="ot">&lt;-</span> oldV; df<span class="sc">$</span>v[i] <span class="ot">&lt;-</span> newV</span>
<span id="cb58-366"><a href="mod-mc.html#cb58-366" tabindex="-1"></a>               }</span>
<span id="cb58-367"><a href="mod-mc.html#cb58-367" tabindex="-1"></a>               <span class="cf">if</span> (verbose) <span class="fu">print</span>(df)</span>
<span id="cb58-368"><a href="mod-mc.html#cb58-368" tabindex="-1"></a>            }</span>
<span id="cb58-369"><a href="mod-mc.html#cb58-369" tabindex="-1"></a>            <span class="cf">if</span> (delta <span class="sc">&lt;</span> theta <span class="sc">&amp;</span> ite <span class="sc">&gt;=</span> minIte) <span class="cf">break</span></span>
<span id="cb58-370"><a href="mod-mc.html#cb58-370" tabindex="-1"></a>         }</span>
<span id="cb58-371"><a href="mod-mc.html#cb58-371" tabindex="-1"></a>         <span class="cf">if</span> (ite <span class="sc">==</span> maxIte) <span class="fu">warning</span>(<span class="st">&quot;Polcy eval algorithm stopped at max iterations allowed: &quot;</span>, maxIte)</span>
<span id="cb58-372"><a href="mod-mc.html#cb58-372" tabindex="-1"></a>         <span class="fu">message</span>(<span class="fu">str_c</span>(<span class="st">&quot;Policy eval algorihm finished in &quot;</span>, ite, <span class="st">&quot; iterations.&quot;</span>))</span>
<span id="cb58-373"><a href="mod-mc.html#cb58-373" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-374"><a href="mod-mc.html#cb58-374" tabindex="-1"></a>      },</span>
<span id="cb58-375"><a href="mod-mc.html#cb58-375" tabindex="-1"></a>      </span>
<span id="cb58-376"><a href="mod-mc.html#cb58-376" tabindex="-1"></a>      <span class="co">#&#39; @description Generalized policy iteration using on policy every-visit Monte Carlo sampling.  </span></span>
<span id="cb58-377"><a href="mod-mc.html#cb58-377" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getEpisode(agent, s, coeff)` that return an episode as a tibble with </span></span>
<span id="cb58-378"><a href="mod-mc.html#cb58-378" tabindex="-1"></a>      <span class="co">#&#39;    cols s, a, r (last col the terminal reward). This method also must update the visit counters if needed! This is also </span></span>
<span id="cb58-379"><a href="mod-mc.html#cb58-379" tabindex="-1"></a>      <span class="co">#&#39;    the method that decides which action selection method is used. </span></span>
<span id="cb58-380"><a href="mod-mc.html#cb58-380" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-381"><a href="mod-mc.html#cb58-381" tabindex="-1"></a>      <span class="co">#&#39; @param theta Threshold parameter.</span></span>
<span id="cb58-382"><a href="mod-mc.html#cb58-382" tabindex="-1"></a>      <span class="co">#&#39; @param minIte Minimum number of iterations for each start state (all `states` are used a start state in one iteration).</span></span>
<span id="cb58-383"><a href="mod-mc.html#cb58-383" tabindex="-1"></a>      <span class="co">#&#39; @param maxIte Maximum number of iterations for each start state (all `states` are used a start state in one iteration).</span></span>
<span id="cb58-384"><a href="mod-mc.html#cb58-384" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all action-values to 0.</span></span>
<span id="cb58-385"><a href="mod-mc.html#cb58-385" tabindex="-1"></a>      <span class="co">#&#39; @param states Start states in the episodes, which all are visited using a for loop.</span></span>
<span id="cb58-386"><a href="mod-mc.html#cb58-386" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used for the epsilon-greedy policy.</span></span>
<span id="cb58-387"><a href="mod-mc.html#cb58-387" tabindex="-1"></a>      <span class="co">#&#39; @param verbose If true then print info for each episode.</span></span>
<span id="cb58-388"><a href="mod-mc.html#cb58-388" tabindex="-1"></a>      <span class="at">gpiOnPolicyMC =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">theta =</span> <span class="fl">0.1</span>, <span class="at">minIte =</span> <span class="dv">100</span>, <span class="at">maxIte =</span> <span class="dv">1000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>(), <span class="at">eps =</span> <span class="fl">0.1</span>, <span class="at">verbose =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb58-389"><a href="mod-mc.html#cb58-389" tabindex="-1"></a>         <span class="cf">if</span> (reset) {</span>
<span id="cb58-390"><a href="mod-mc.html#cb58-390" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionValue</span>()      <span class="co"># set to 0</span></span>
<span id="cb58-391"><a href="mod-mc.html#cb58-391" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionCtrValue</span>()   <span class="co"># reset counter</span></span>
<span id="cb58-392"><a href="mod-mc.html#cb58-392" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setStateCtrValue</span>()    <span class="co"># reset counter</span></span>
<span id="cb58-393"><a href="mod-mc.html#cb58-393" tabindex="-1"></a>         }</span>
<span id="cb58-394"><a href="mod-mc.html#cb58-394" tabindex="-1"></a>         <span class="co"># self$setRandomEpsGreedyPolicy(epsilon)</span></span>
<span id="cb58-395"><a href="mod-mc.html#cb58-395" tabindex="-1"></a>         self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, self<span class="sc">$</span><span class="fu">getStateKeys</span>())</span>
<span id="cb58-396"><a href="mod-mc.html#cb58-396" tabindex="-1"></a>         <span class="co"># self$setGreedyPolicy()</span></span>
<span id="cb58-397"><a href="mod-mc.html#cb58-397" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxIte) {</span>
<span id="cb58-398"><a href="mod-mc.html#cb58-398" tabindex="-1"></a>            delta <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb58-399"><a href="mod-mc.html#cb58-399" tabindex="-1"></a>            <span class="co"># stable &lt;- TRUE</span></span>
<span id="cb58-400"><a href="mod-mc.html#cb58-400" tabindex="-1"></a>            <span class="cf">for</span> (ss <span class="cf">in</span> states) {  <span class="co"># for episode with s as start</span></span>
<span id="cb58-401"><a href="mod-mc.html#cb58-401" tabindex="-1"></a>               df <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getEpisode</span>(self, ss, eps)  <span class="co"># an episode stored in a tibble with cols s, a, r (last col the terminal reward)</span></span>
<span id="cb58-402"><a href="mod-mc.html#cb58-402" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">nrow</span>(df) <span class="sc">==</span> <span class="dv">0</span>) <span class="cf">next</span></span>
<span id="cb58-403"><a href="mod-mc.html#cb58-403" tabindex="-1"></a>               df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">nA =</span> <span class="cn">NA</span>, <span class="at">nS =</span> <span class="cn">NA</span>, <span class="at">oldQ =</span> <span class="cn">NA</span>, <span class="at">q =</span> <span class="cn">NA</span>, <span class="at">g =</span> <span class="cn">NA</span>, <span class="at">oldV =</span> <span class="cn">NA</span>, <span class="at">v =</span> <span class="cn">NA</span>)</span>
<span id="cb58-404"><a href="mod-mc.html#cb58-404" tabindex="-1"></a>               gain <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb58-405"><a href="mod-mc.html#cb58-405" tabindex="-1"></a>               <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">nrow</span>(df)<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb58-406"><a href="mod-mc.html#cb58-406" tabindex="-1"></a>                  s <span class="ot">&lt;-</span> df<span class="sc">$</span>s[i]</span>
<span id="cb58-407"><a href="mod-mc.html#cb58-407" tabindex="-1"></a>                  a <span class="ot">&lt;-</span> df<span class="sc">$</span>a[i]</span>
<span id="cb58-408"><a href="mod-mc.html#cb58-408" tabindex="-1"></a>                  gain <span class="ot">&lt;-</span> df<span class="sc">$</span>r[i] <span class="sc">+</span> gamma <span class="sc">*</span> gain</span>
<span id="cb58-409"><a href="mod-mc.html#cb58-409" tabindex="-1"></a>                  ctr <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>n</span>
<span id="cb58-410"><a href="mod-mc.html#cb58-410" tabindex="-1"></a>                  oldQ <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q</span>
<span id="cb58-411"><a href="mod-mc.html#cb58-411" tabindex="-1"></a>                  oldV <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getStateValueQ</span>(s)</span>
<span id="cb58-412"><a href="mod-mc.html#cb58-412" tabindex="-1"></a>                  stepSize <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>ctr)<span class="sc">^</span><span class="fl">0.5</span></span>
<span id="cb58-413"><a href="mod-mc.html#cb58-413" tabindex="-1"></a>                  self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">&lt;-</span> oldQ <span class="sc">+</span> stepSize <span class="sc">*</span> (gain <span class="sc">-</span> oldQ)</span>
<span id="cb58-414"><a href="mod-mc.html#cb58-414" tabindex="-1"></a>                  <span class="co"># self$model[[s]]$actions[[a]]$q &lt;- oldQ + 0.1 * (gain - oldQ)</span></span>
<span id="cb58-415"><a href="mod-mc.html#cb58-415" tabindex="-1"></a>                  self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, s)</span>
<span id="cb58-416"><a href="mod-mc.html#cb58-416" tabindex="-1"></a>                  newV <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getStateValueQ</span>(s)</span>
<span id="cb58-417"><a href="mod-mc.html#cb58-417" tabindex="-1"></a>                  delta <span class="ot">&lt;-</span> <span class="fu">max</span>(delta, <span class="fu">abs</span>(oldV <span class="sc">-</span> newV))</span>
<span id="cb58-418"><a href="mod-mc.html#cb58-418" tabindex="-1"></a>                  <span class="cf">if</span> (verbose) df<span class="sc">$</span>oldQ[i] <span class="ot">&lt;-</span> oldQ; df<span class="sc">$</span>q[i] <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q; df<span class="sc">$</span>g[i] <span class="ot">&lt;-</span> gain; df<span class="sc">$</span>nA[i] <span class="ot">&lt;-</span> ctr; df<span class="sc">$</span>nS[i] <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>n; df<span class="sc">$</span>oldV[i] <span class="ot">&lt;-</span> oldV; df<span class="sc">$</span>v[i] <span class="ot">&lt;-</span> newV</span>
<span id="cb58-419"><a href="mod-mc.html#cb58-419" tabindex="-1"></a>               }</span>
<span id="cb58-420"><a href="mod-mc.html#cb58-420" tabindex="-1"></a>               <span class="cf">if</span> (verbose) <span class="fu">print</span>(df)</span>
<span id="cb58-421"><a href="mod-mc.html#cb58-421" tabindex="-1"></a>            }</span>
<span id="cb58-422"><a href="mod-mc.html#cb58-422" tabindex="-1"></a>            <span class="cf">if</span> (delta <span class="sc">&lt;</span> theta <span class="sc">&amp;</span> ite <span class="sc">&gt;=</span> minIte) <span class="cf">break</span></span>
<span id="cb58-423"><a href="mod-mc.html#cb58-423" tabindex="-1"></a>         }</span>
<span id="cb58-424"><a href="mod-mc.html#cb58-424" tabindex="-1"></a>         <span class="cf">if</span> (ite <span class="sc">==</span> maxIte) <span class="fu">warning</span>(<span class="st">&quot;GPI algorithm stopped at max iterations allowed: &quot;</span>, maxIte)</span>
<span id="cb58-425"><a href="mod-mc.html#cb58-425" tabindex="-1"></a>         <span class="fu">message</span>(<span class="fu">str_c</span>(<span class="st">&quot;GPI algorihm finished in &quot;</span>, ite, <span class="st">&quot; iterations.&quot;</span>))</span>
<span id="cb58-426"><a href="mod-mc.html#cb58-426" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-427"><a href="mod-mc.html#cb58-427" tabindex="-1"></a>      },</span>
<span id="cb58-428"><a href="mod-mc.html#cb58-428" tabindex="-1"></a>      </span>
<span id="cb58-429"><a href="mod-mc.html#cb58-429" tabindex="-1"></a>      <span class="co">#&#39; @description Generalized policy iteration using on policy SARSA.  </span></span>
<span id="cb58-430"><a href="mod-mc.html#cb58-430" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getTimeStepData(s,a)` that return a list with elements `r` (reward) and `sN` (next state). </span></span>
<span id="cb58-431"><a href="mod-mc.html#cb58-431" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-432"><a href="mod-mc.html#cb58-432" tabindex="-1"></a>      <span class="co">#&#39; @param maxE Maximum number of episodes generated.</span></span>
<span id="cb58-433"><a href="mod-mc.html#cb58-433" tabindex="-1"></a>      <span class="co">#&#39; @param maxEL Maximum length of episode. If model with continuing tasks use this to set the length of training.</span></span>
<span id="cb58-434"><a href="mod-mc.html#cb58-434" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all action-values to 0.</span></span>
<span id="cb58-435"><a href="mod-mc.html#cb58-435" tabindex="-1"></a>      <span class="co">#&#39; @param states Possible start states of an episode. One selected randomly.</span></span>
<span id="cb58-436"><a href="mod-mc.html#cb58-436" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used for the epsilon-greedy policy.</span></span>
<span id="cb58-437"><a href="mod-mc.html#cb58-437" tabindex="-1"></a>      <span class="co">#&#39; @param alpha Step-size (use a fixed step-size).</span></span>
<span id="cb58-438"><a href="mod-mc.html#cb58-438" tabindex="-1"></a>      <span class="at">gpiOnPolicySARSA =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">maxE =</span> <span class="dv">1000</span>, <span class="at">maxEL =</span> <span class="dv">10000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>(), <span class="at">eps =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">verbose =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb58-439"><a href="mod-mc.html#cb58-439" tabindex="-1"></a>         <span class="cf">if</span> (reset) {</span>
<span id="cb58-440"><a href="mod-mc.html#cb58-440" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionValue</span>()      <span class="co"># set to 0</span></span>
<span id="cb58-441"><a href="mod-mc.html#cb58-441" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionCtrValue</span>()</span>
<span id="cb58-442"><a href="mod-mc.html#cb58-442" tabindex="-1"></a>         }</span>
<span id="cb58-443"><a href="mod-mc.html#cb58-443" tabindex="-1"></a>         self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, self<span class="sc">$</span><span class="fu">getStateKeys</span>())</span>
<span id="cb58-444"><a href="mod-mc.html#cb58-444" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxE) {</span>
<span id="cb58-445"><a href="mod-mc.html#cb58-445" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">sample</span>(states, <span class="dv">1</span>)  <span class="co"># pick start state among possible start states</span></span>
<span id="cb58-446"><a href="mod-mc.html#cb58-446" tabindex="-1"></a>            a <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionPi</span>(s)</span>
<span id="cb58-447"><a href="mod-mc.html#cb58-447" tabindex="-1"></a>            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxEL) {  <span class="co"># for episode with s as start (max length 100000)</span></span>
<span id="cb58-448"><a href="mod-mc.html#cb58-448" tabindex="-1"></a>               dat <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getTimeStepData</span>(s,a)  <span class="co"># get next state and reward</span></span>
<span id="cb58-449"><a href="mod-mc.html#cb58-449" tabindex="-1"></a>               r <span class="ot">&lt;-</span> dat<span class="sc">$</span>r</span>
<span id="cb58-450"><a href="mod-mc.html#cb58-450" tabindex="-1"></a>               sN <span class="ot">&lt;-</span> dat<span class="sc">$</span>sN</span>
<span id="cb58-451"><a href="mod-mc.html#cb58-451" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">is.na</span>(sN) <span class="sc">|</span> <span class="fu">is.na</span>(a)) <span class="cf">break</span>  <span class="co"># start generating new episode</span></span>
<span id="cb58-452"><a href="mod-mc.html#cb58-452" tabindex="-1"></a>               aN <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionPi</span>(sN)</span>
<span id="cb58-453"><a href="mod-mc.html#cb58-453" tabindex="-1"></a>               oldQ <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q  </span>
<span id="cb58-454"><a href="mod-mc.html#cb58-454" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">&lt;-</span> oldQ <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> self<span class="sc">$</span>model[[sN]]<span class="sc">$</span>actions[[aN]]<span class="sc">$</span>q <span class="sc">-</span> oldQ)</span>
<span id="cb58-455"><a href="mod-mc.html#cb58-455" tabindex="-1"></a>               <span class="cf">if</span> (verbose) <span class="fu">cat</span>(<span class="st">&quot;(s,a,r,s,a) = (&quot;</span>, s, <span class="st">&quot;,&quot;</span>, a, <span class="st">&quot;,&quot;</span>, r, <span class="st">&quot;,&quot;</span>, sN, <span class="st">&quot;,&quot;</span>, aN, <span class="st">&quot;), r = &quot;</span>, r, <span class="st">&quot; oldQ = &quot;</span>, oldQ, <span class="st">&quot; Q(sN, aN) = &quot;</span>, self<span class="sc">$</span>model[[sN]]<span class="sc">$</span>actions[[aN]]<span class="sc">$</span>q, <span class="st">&quot; newQ = &quot;</span>, self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb58-456"><a href="mod-mc.html#cb58-456" tabindex="-1"></a>               self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, s)</span>
<span id="cb58-457"><a href="mod-mc.html#cb58-457" tabindex="-1"></a>               s <span class="ot">&lt;-</span> sN</span>
<span id="cb58-458"><a href="mod-mc.html#cb58-458" tabindex="-1"></a>               a <span class="ot">&lt;-</span> aN</span>
<span id="cb58-459"><a href="mod-mc.html#cb58-459" tabindex="-1"></a>            }</span>
<span id="cb58-460"><a href="mod-mc.html#cb58-460" tabindex="-1"></a>            <span class="cf">if</span> (i <span class="sc">==</span> maxEL) <span class="cf">break</span></span>
<span id="cb58-461"><a href="mod-mc.html#cb58-461" tabindex="-1"></a>         }</span>
<span id="cb58-462"><a href="mod-mc.html#cb58-462" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with &quot;</span>, ite, <span class="st">&quot; episodes.&quot;</span>)</span>
<span id="cb58-463"><a href="mod-mc.html#cb58-463" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with episode of length &quot;</span>, i, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb58-464"><a href="mod-mc.html#cb58-464" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-465"><a href="mod-mc.html#cb58-465" tabindex="-1"></a>      },</span>
<span id="cb58-466"><a href="mod-mc.html#cb58-466" tabindex="-1"></a>      </span>
<span id="cb58-467"><a href="mod-mc.html#cb58-467" tabindex="-1"></a>      <span class="co">#&#39; @description Generalized policy iteration using off policy Q-learning.  </span></span>
<span id="cb58-468"><a href="mod-mc.html#cb58-468" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getTimeStepData(s,a)` that return a list with elements `r` (reward) and `sN` (next state). </span></span>
<span id="cb58-469"><a href="mod-mc.html#cb58-469" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-470"><a href="mod-mc.html#cb58-470" tabindex="-1"></a>      <span class="co">#&#39; @param maxE Maximum number of episodes generated.</span></span>
<span id="cb58-471"><a href="mod-mc.html#cb58-471" tabindex="-1"></a>      <span class="co">#&#39; @param maxEL Maximum length of episode. If model with continuing tasks use this to set the length of training.</span></span>
<span id="cb58-472"><a href="mod-mc.html#cb58-472" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all action-values to 0.</span></span>
<span id="cb58-473"><a href="mod-mc.html#cb58-473" tabindex="-1"></a>      <span class="co">#&#39; @param states Possible start states of an episode. One selected randomly.</span></span>
<span id="cb58-474"><a href="mod-mc.html#cb58-474" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used for the epsilon-greedy policy.</span></span>
<span id="cb58-475"><a href="mod-mc.html#cb58-475" tabindex="-1"></a>      <span class="co">#&#39; @param alpha Step-size (use a fixed step-size).</span></span>
<span id="cb58-476"><a href="mod-mc.html#cb58-476" tabindex="-1"></a>      <span class="at">gpiOffPolicyQLearning =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">maxE =</span> <span class="dv">1000</span>, <span class="at">maxEL =</span> <span class="dv">10000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>(), <span class="at">eps =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">verbose =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb58-477"><a href="mod-mc.html#cb58-477" tabindex="-1"></a>         <span class="cf">if</span> (reset) {</span>
<span id="cb58-478"><a href="mod-mc.html#cb58-478" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionValue</span>()      <span class="co"># set to 0</span></span>
<span id="cb58-479"><a href="mod-mc.html#cb58-479" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionCtrValue</span>()</span>
<span id="cb58-480"><a href="mod-mc.html#cb58-480" tabindex="-1"></a>         }</span>
<span id="cb58-481"><a href="mod-mc.html#cb58-481" tabindex="-1"></a>         self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, self<span class="sc">$</span><span class="fu">getStateKeys</span>())</span>
<span id="cb58-482"><a href="mod-mc.html#cb58-482" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxE) {</span>
<span id="cb58-483"><a href="mod-mc.html#cb58-483" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">sample</span>(states, <span class="dv">1</span>)  <span class="co"># pick start state among possible start states</span></span>
<span id="cb58-484"><a href="mod-mc.html#cb58-484" tabindex="-1"></a>            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxEL) {  <span class="co"># for episode with s as start (max length 100000)</span></span>
<span id="cb58-485"><a href="mod-mc.html#cb58-485" tabindex="-1"></a>               a <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionPi</span>(s)</span>
<span id="cb58-486"><a href="mod-mc.html#cb58-486" tabindex="-1"></a>               dat <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getTimeStepData</span>(s,a)  <span class="co"># get next state and reward</span></span>
<span id="cb58-487"><a href="mod-mc.html#cb58-487" tabindex="-1"></a>               r <span class="ot">&lt;-</span> dat<span class="sc">$</span>r</span>
<span id="cb58-488"><a href="mod-mc.html#cb58-488" tabindex="-1"></a>               sN <span class="ot">&lt;-</span> dat<span class="sc">$</span>sN</span>
<span id="cb58-489"><a href="mod-mc.html#cb58-489" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">is.na</span>(sN) <span class="sc">|</span> <span class="fu">is.na</span>(a)) <span class="cf">break</span>  <span class="co"># start generating new episode</span></span>
<span id="cb58-490"><a href="mod-mc.html#cb58-490" tabindex="-1"></a>               oldQ <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q  </span>
<span id="cb58-491"><a href="mod-mc.html#cb58-491" tabindex="-1"></a>               maxQ <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getMaxActionValue</span>(sN)</span>
<span id="cb58-492"><a href="mod-mc.html#cb58-492" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">&lt;-</span> oldQ <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> maxQ <span class="sc">-</span> oldQ)</span>
<span id="cb58-493"><a href="mod-mc.html#cb58-493" tabindex="-1"></a>               <span class="cf">if</span> (verbose) <span class="fu">cat</span>(<span class="st">&quot;(s,a,r,s) = (&quot;</span>, s, <span class="st">&quot;,&quot;</span>, a, <span class="st">&quot;,&quot;</span>, r, <span class="st">&quot;,&quot;</span>, sN, <span class="st">&quot;), r = &quot;</span>, r, <span class="st">&quot; oldQ = &quot;</span>, oldQ, <span class="st">&quot; maxQ(sN) = &quot;</span>, maxQ, <span class="st">&quot; newQ = &quot;</span>, self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb58-494"><a href="mod-mc.html#cb58-494" tabindex="-1"></a>               self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, s)</span>
<span id="cb58-495"><a href="mod-mc.html#cb58-495" tabindex="-1"></a>               s <span class="ot">&lt;-</span> sN</span>
<span id="cb58-496"><a href="mod-mc.html#cb58-496" tabindex="-1"></a>            }</span>
<span id="cb58-497"><a href="mod-mc.html#cb58-497" tabindex="-1"></a>            <span class="cf">if</span> (i <span class="sc">==</span> maxEL) <span class="cf">break</span></span>
<span id="cb58-498"><a href="mod-mc.html#cb58-498" tabindex="-1"></a>         }</span>
<span id="cb58-499"><a href="mod-mc.html#cb58-499" tabindex="-1"></a>         self<span class="sc">$</span><span class="fu">setGreedyPolicy</span>()</span>
<span id="cb58-500"><a href="mod-mc.html#cb58-500" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with &quot;</span>, ite, <span class="st">&quot; episodes.&quot;</span>)</span>
<span id="cb58-501"><a href="mod-mc.html#cb58-501" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with episode of length &quot;</span>, i, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb58-502"><a href="mod-mc.html#cb58-502" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-503"><a href="mod-mc.html#cb58-503" tabindex="-1"></a>      },</span>
<span id="cb58-504"><a href="mod-mc.html#cb58-504" tabindex="-1"></a></span>
<span id="cb58-505"><a href="mod-mc.html#cb58-505" tabindex="-1"></a>      <span class="co">#&#39; @description Generalized policy iteration using off policy Q-learning.  </span></span>
<span id="cb58-506"><a href="mod-mc.html#cb58-506" tabindex="-1"></a>      <span class="co">#&#39; @param env The environment which must have a method `getTimeStepData(s,a)` that return a list with elements `r` (reward) and `sN` (next state). </span></span>
<span id="cb58-507"><a href="mod-mc.html#cb58-507" tabindex="-1"></a>      <span class="co">#&#39; @param gamma Discount rate.</span></span>
<span id="cb58-508"><a href="mod-mc.html#cb58-508" tabindex="-1"></a>      <span class="co">#&#39; @param maxE Maximum number of episodes generated.</span></span>
<span id="cb58-509"><a href="mod-mc.html#cb58-509" tabindex="-1"></a>      <span class="co">#&#39; @param maxEL Maximum length of episode. If model with continuing tasks use this to set the length of training.</span></span>
<span id="cb58-510"><a href="mod-mc.html#cb58-510" tabindex="-1"></a>      <span class="co">#&#39; @param reset If true initialize all action-values to 0.</span></span>
<span id="cb58-511"><a href="mod-mc.html#cb58-511" tabindex="-1"></a>      <span class="co">#&#39; @param states Possible start states of an episode. One selected randomly.</span></span>
<span id="cb58-512"><a href="mod-mc.html#cb58-512" tabindex="-1"></a>      <span class="co">#&#39; @param eps Epsilon used for the epsilon-greedy policy.</span></span>
<span id="cb58-513"><a href="mod-mc.html#cb58-513" tabindex="-1"></a>      <span class="co">#&#39; @param alpha Step-size (use a fixed step-size).</span></span>
<span id="cb58-514"><a href="mod-mc.html#cb58-514" tabindex="-1"></a>      <span class="at">gpiOnPolicyExpSARSA =</span> <span class="cf">function</span>(env, <span class="at">gamma =</span> <span class="dv">1</span>, <span class="at">maxE =</span> <span class="dv">1000</span>, <span class="at">maxEL =</span> <span class="dv">10000</span>, <span class="at">reset =</span> <span class="cn">TRUE</span>, <span class="at">states =</span> self<span class="sc">$</span><span class="fu">getStateKeys</span>(), <span class="at">eps =</span> <span class="fl">0.1</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">verbose =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb58-515"><a href="mod-mc.html#cb58-515" tabindex="-1"></a>         <span class="cf">if</span> (reset) {</span>
<span id="cb58-516"><a href="mod-mc.html#cb58-516" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionValue</span>()      <span class="co"># set to 0</span></span>
<span id="cb58-517"><a href="mod-mc.html#cb58-517" tabindex="-1"></a>            self<span class="sc">$</span><span class="fu">setActionCtrValue</span>()</span>
<span id="cb58-518"><a href="mod-mc.html#cb58-518" tabindex="-1"></a>         }</span>
<span id="cb58-519"><a href="mod-mc.html#cb58-519" tabindex="-1"></a>         self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, self<span class="sc">$</span><span class="fu">getStateKeys</span>())</span>
<span id="cb58-520"><a href="mod-mc.html#cb58-520" tabindex="-1"></a>         <span class="cf">for</span> (ite <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxE) {</span>
<span id="cb58-521"><a href="mod-mc.html#cb58-521" tabindex="-1"></a>            s <span class="ot">&lt;-</span> <span class="fu">sample</span>(states, <span class="dv">1</span>)  <span class="co"># pick start state among possible start states</span></span>
<span id="cb58-522"><a href="mod-mc.html#cb58-522" tabindex="-1"></a>            <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>maxEL) {  <span class="co"># for episode with s as start (max length 100000)</span></span>
<span id="cb58-523"><a href="mod-mc.html#cb58-523" tabindex="-1"></a>               a <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getActionPi</span>(s)</span>
<span id="cb58-524"><a href="mod-mc.html#cb58-524" tabindex="-1"></a>               dat <span class="ot">&lt;-</span> env<span class="sc">$</span><span class="fu">getTimeStepData</span>(s,a)  <span class="co"># get next state and reward</span></span>
<span id="cb58-525"><a href="mod-mc.html#cb58-525" tabindex="-1"></a>               r <span class="ot">&lt;-</span> dat<span class="sc">$</span>r</span>
<span id="cb58-526"><a href="mod-mc.html#cb58-526" tabindex="-1"></a>               sN <span class="ot">&lt;-</span> dat<span class="sc">$</span>sN</span>
<span id="cb58-527"><a href="mod-mc.html#cb58-527" tabindex="-1"></a>               <span class="cf">if</span> (<span class="fu">is.na</span>(sN) <span class="sc">|</span> <span class="fu">is.na</span>(a)) <span class="cf">break</span>  <span class="co"># start generating new episode</span></span>
<span id="cb58-528"><a href="mod-mc.html#cb58-528" tabindex="-1"></a>               oldQ <span class="ot">&lt;-</span> self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q  </span>
<span id="cb58-529"><a href="mod-mc.html#cb58-529" tabindex="-1"></a>               expQ <span class="ot">&lt;-</span> self<span class="sc">$</span><span class="fu">getExpActionValue</span>(sN)</span>
<span id="cb58-530"><a href="mod-mc.html#cb58-530" tabindex="-1"></a>               self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q <span class="ot">&lt;-</span> oldQ <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> expQ <span class="sc">-</span> oldQ)</span>
<span id="cb58-531"><a href="mod-mc.html#cb58-531" tabindex="-1"></a>               <span class="cf">if</span> (verbose) <span class="fu">cat</span>(<span class="st">&quot;(s,a,r,s) = (&quot;</span>, s, <span class="st">&quot;,&quot;</span>, a, <span class="st">&quot;,&quot;</span>, r, <span class="st">&quot;,&quot;</span>, sN, <span class="st">&quot;), r = &quot;</span>, r, <span class="st">&quot; oldQ = &quot;</span>, oldQ, <span class="st">&quot; expQ(sN) = &quot;</span>, expQ, <span class="st">&quot; newQ = &quot;</span>, self<span class="sc">$</span>model[[s]]<span class="sc">$</span>actions[[a]]<span class="sc">$</span>q, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb58-532"><a href="mod-mc.html#cb58-532" tabindex="-1"></a>               self<span class="sc">$</span><span class="fu">setEpsGreedyPolicy</span>(eps, s)</span>
<span id="cb58-533"><a href="mod-mc.html#cb58-533" tabindex="-1"></a>               s <span class="ot">&lt;-</span> sN</span>
<span id="cb58-534"><a href="mod-mc.html#cb58-534" tabindex="-1"></a>            }</span>
<span id="cb58-535"><a href="mod-mc.html#cb58-535" tabindex="-1"></a>            <span class="cf">if</span> (i <span class="sc">==</span> maxEL) <span class="cf">break</span></span>
<span id="cb58-536"><a href="mod-mc.html#cb58-536" tabindex="-1"></a>         }</span>
<span id="cb58-537"><a href="mod-mc.html#cb58-537" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with &quot;</span>, ite, <span class="st">&quot; episodes.&quot;</span>)</span>
<span id="cb58-538"><a href="mod-mc.html#cb58-538" tabindex="-1"></a>         <span class="fu">message</span>(<span class="st">&quot;GPI algorithm stopped with episode of length &quot;</span>, i, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb58-539"><a href="mod-mc.html#cb58-539" tabindex="-1"></a>         <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="cn">NULL</span>))</span>
<span id="cb58-540"><a href="mod-mc.html#cb58-540" tabindex="-1"></a>      }</span>
<span id="cb58-541"><a href="mod-mc.html#cb58-541" tabindex="-1"></a>   )</span>
<span id="cb58-542"><a href="mod-mc.html#cb58-542" tabindex="-1"></a>)</span></code></pre></div>
<p>Pay attention to the <code>gpiOnPolicyMC</code> method which uses generalized policy iteration with every-visit estimation using on-policy sampling and an epsilon greedy policy for action selection. Note that <code>gpiOnPolicyMC</code> also takes a set of states as input which are the starting states of the episodes generated (all used in each iteration). Moreover, we do not use a step size equal <span class="math inline">\(1/n_a\)</span> but <span class="math inline">\((1/n_a)^{0.5}\)</span> which decrease slower. Finally, the stopping criteria is added by comparing the the differences in the state-values for each state in an episode. Note the value of <span class="math inline">\(\theta\)</span> gives no guarantee that the action-values will be close to the optimal ones, since we not sample episodes. For instance if two similar episodes are generated early in run then they may be so alike that that the state-values are almost equal and hence the algorithm stops.</p>
<p>We define the RL agent:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="mod-mc.html#cb59-1" tabindex="-1"></a>agent <span class="ot">&lt;-</span> RLAgent<span class="sc">$</span><span class="fu">new</span>()</span>
<span id="cb59-2"><a href="mod-mc.html#cb59-2" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">addStates</span>(env<span class="sc">$</span><span class="fu">getStates</span>())   <span class="co"># add states</span></span>
<span id="cb59-3"><a href="mod-mc.html#cb59-3" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> agent<span class="sc">$</span><span class="fu">getStateKeys</span>()) {  <span class="co"># add actions</span></span>
<span id="cb59-4"><a href="mod-mc.html#cb59-4" tabindex="-1"></a>   agent<span class="sc">$</span><span class="fu">addActions</span>(s, env<span class="sc">$</span><span class="fu">getActions</span>(s))</span>
<span id="cb59-5"><a href="mod-mc.html#cb59-5" tabindex="-1"></a>} </span>
<span id="cb59-6"><a href="mod-mc.html#cb59-6" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getActionKeys</span>(<span class="st">&quot;2,5&quot;</span>)</span>
<span id="cb59-7"><a href="mod-mc.html#cb59-7" tabindex="-1"></a><span class="co">#&gt; [1] &quot;10&quot; &quot;15&quot; &quot;20&quot; &quot;25&quot;</span></span></code></pre></div>
<p>Given the current policy an episode can be extracted using:</p>
<p>Each row contains the state, action and reward for a time-step. Let us try to approximate the optimal state-value and action in state <span class="math inline">\((50,1)\)</span>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="mod-mc.html#cb60-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">232</span>)</span>
<span id="cb60-2"><a href="mod-mc.html#cb60-2" tabindex="-1"></a>time <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb60-3"><a href="mod-mc.html#cb60-3" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb60-4"><a href="mod-mc.html#cb60-4" tabindex="-1"></a>state <span class="ot">=</span> <span class="fu">str_c</span>(i, <span class="st">&quot;,&quot;</span>, time)</span>
<span id="cb60-5"><a href="mod-mc.html#cb60-5" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">2000</span>, <span class="at">maxIte =</span> <span class="dv">50000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> T, <span class="at">eps =</span> <span class="fl">0.2</span>, <span class="at">theta =</span> <span class="fl">0.2</span>)</span>
<span id="cb60-6"><a href="mod-mc.html#cb60-6" tabindex="-1"></a>dfRL <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionValues</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb60-7"><a href="mod-mc.html#cb60-7" tabindex="-1"></a>   <span class="fu">separate</span>(state, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;inv&quot;</span>, <span class="st">&quot;t&quot;</span>), <span class="at">remove =</span> F, <span class="at">convert =</span> T)</span></code></pre></div>
<p>Let us consider the action-values at state <span class="math inline">\((50,1)\)</span></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="mod-mc.html#cb61-1" tabindex="-1"></a>dfRL <span class="sc">%&gt;%</span></span>
<span id="cb61-2"><a href="mod-mc.html#cb61-2" tabindex="-1"></a>   <span class="fu">filter</span>(inv <span class="sc">==</span> i, t <span class="sc">==</span> time) <span class="sc">%&gt;%</span> </span>
<span id="cb61-3"><a href="mod-mc.html#cb61-3" tabindex="-1"></a>   <span class="fu">left_join</span>(dfMDP)</span>
<span id="cb61-4"><a href="mod-mc.html#cb61-4" tabindex="-1"></a><span class="co">#&gt; # A tibble: 4 × 7</span></span>
<span id="cb61-5"><a href="mod-mc.html#cb61-5" tabindex="-1"></a><span class="co">#&gt;   state   inv     t action     q     n     v</span></span>
<span id="cb61-6"><a href="mod-mc.html#cb61-6" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb61-7"><a href="mod-mc.html#cb61-7" tabindex="-1"></a><span class="co">#&gt; 1 50,1     50     1 10      51.1   356   NA </span></span>
<span id="cb61-8"><a href="mod-mc.html#cb61-8" tabindex="-1"></a><span class="co">#&gt; 2 50,1     50     1 15     192.    346   NA </span></span>
<span id="cb61-9"><a href="mod-mc.html#cb61-9" tabindex="-1"></a><span class="co">#&gt; 3 50,1     50     1 20     244.  13156  311.</span></span>
<span id="cb61-10"><a href="mod-mc.html#cb61-10" tabindex="-1"></a><span class="co">#&gt; 4 50,1     50     1 25     222.    378   NA</span></span></code></pre></div>
<p>Note that actions in state <span class="math inline">\((50,1)\)</span> are sampled differently (based on the epsilon-greedy policy). The best action found (the one with the highest action-value) also seems to be the optimal action w.r.t. the MDP.</p>
<p>Let us make a plot of the greedy action for the visited states:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="mod-mc.html#cb62-1" tabindex="-1"></a>visits <span class="ot">&lt;-</span> dfRL <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(state) <span class="sc">%&gt;%</span> <span class="fu">summarise</span>(<span class="at">nS =</span> <span class="fu">sum</span>(n)) </span>
<span id="cb62-2"><a href="mod-mc.html#cb62-2" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> dfRL <span class="sc">%&gt;%</span> </span>
<span id="cb62-3"><a href="mod-mc.html#cb62-3" tabindex="-1"></a>   <span class="fu">group_by</span>(state) <span class="sc">%&gt;%</span></span>
<span id="cb62-4"><a href="mod-mc.html#cb62-4" tabindex="-1"></a>   <span class="fu">slice_max</span>(q, <span class="at">with_ties =</span> F) <span class="sc">%&gt;%</span> </span>
<span id="cb62-5"><a href="mod-mc.html#cb62-5" tabindex="-1"></a>   <span class="fu">left_join</span>(visits) <span class="sc">%&gt;%</span> </span>
<span id="cb62-6"><a href="mod-mc.html#cb62-6" tabindex="-1"></a>   <span class="fu">filter</span>(n <span class="sc">!=</span> <span class="dv">0</span>)  </span>
<span id="cb62-7"><a href="mod-mc.html#cb62-7" tabindex="-1"></a>df1 <span class="sc">%&gt;%</span> </span>
<span id="cb62-8"><a href="mod-mc.html#cb62-8" tabindex="-1"></a>   <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> t, <span class="at">y =</span> inv, <span class="at">size =</span> nS, <span class="at">col =</span> action)) <span class="sc">+</span></span>
<span id="cb62-9"><a href="mod-mc.html#cb62-9" tabindex="-1"></a>   <span class="fu">geom_point</span>() </span></code></pre></div>
<p><img src="06_mc_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Observe that the states are now sampled differently. Some states are visited many times (many episodes visit that state) while others are not visited often. For the states visited often we have a better approximation of best action compared to states which are visited rarely. Here the action is more or less random, e.g. the action in state <span class="math inline">\((5,3)\)</span> should not set the price to 10 (setting the price to 25 would be better since we only have 5 items left).</p>
</div>
</div>
<div id="sec-mc-off-policy" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Off-policy MC prediction<a href="mod-mc.html#sec-mc-off-policy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Until now we have only considered what is denoted <em>on-policy</em> algorithms for finding the optimal policy. Here we both evaluate or improve the policy that is used to make decisions. To ensure infinite exploration we use for instance exploring starts or <span class="math inline">\(\epsilon\)</span>-soft policies. <em>Off-policy</em> methods use a different approach by considering two policies: a policy <span class="math inline">\(b\)</span> used to generate the sample-path (behaviour policy) and a policy <span class="math inline">\(\pi\)</span> that is learned for control (target policy). We update the target policy using the sample-paths from the behaviour policy. The behaviour policy explores the environment for us during training and must ensure infinite exploration. Moreover, the <em>coverage</em> assumption must be satisfied: <span class="math display">\[\pi(a|s) &gt; 0 \rightarrow b(a|s) &gt; 0\]</span> That is, every action in <span class="math inline">\(\pi\)</span> must also be taken, at least occasionally, by <span class="math inline">\(b\)</span>. Put differently, to learn <span class="math inline">\(\pi\)</span> we must sample paths that occur when using <span class="math inline">\(\pi\)</span>. Note target policy <span class="math inline">\(\pi\)</span> may be deterministic by using greedy selection with respect to action-value estimates (greedy in the limit satisfied).</p>
<p>Off-policy learning methods are powerful and more general than on-policy methods (on-policy methods being a special case of off-policy where target and behaviour policies are the same). They can be used to learn from data generated by a conventional non-learning controller or from a human expert.</p>
<p>But how do we estimate the expected return using the target policy when we only have sample-paths from the behaviour policy? For this we need to introduce <em>importance sampling</em>, a general technique for estimating expected values under one distribution given samples from another. Let us first explain it using two distributions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> where we want to estimated the mean of <span class="math inline">\(a\)</span> given data/samples from <span class="math inline">\(b\)</span>, then
<span class="math display">\[
\begin{align}
  \mathbb{E}_{a}[X] &amp;= \sum_{x\in X} a(x)x \\
  &amp;= \sum_{x\in X} a(x)\frac{b(x)}{b(x)}x \\
  &amp;= \sum_{x\in X} b(x)\frac{a(x)}{b(x)}x \\
  &amp;= \sum_{x\in X} b(x)\rho(x)x \\
  &amp;= \mathbb{E}_{b}\left[\rho(X)X\right].
\end{align}
\]</span>
Hence to the mean of <span class="math inline">\(a\)</span> can be found by finding the mean of <span class="math inline">\(\rho(X)X\)</span> where <span class="math inline">\(X\)</span> is has a <span class="math inline">\(b\)</span> distribution and <span class="math inline">\(\rho(x) = a(x)/b(x)\)</span> denote the <em>importance sampling ratio</em>. Note given samples <span class="math inline">\((x_1,\ldots,x_n)\)</span> from <span class="math inline">\(b\)</span> we then can calculate the sample average using
<span class="math display" id="eq:is-approx">\[
\begin{align}
  \mathbb{E}_{a}[X] &amp;= \mathbb{E}_{b}\left[\rho(X)X\right] \\
  &amp;\approx \frac{1}{n}\sum_{i = 1}^n \rho(x_i)x_i \\
\end{align}
\tag{6.1}
\]</span></p>
<!-- Example with two distributions -->
<p>Now let us use importance sampling on the target policy <span class="math inline">\(\pi\)</span> and behaviour policy <span class="math inline">\(b\)</span>. Given state <span class="math inline">\(S_t\)</span> and sample path, we want to find
<span class="math display">\[v_\pi(s) = \mathbb{E}_{\pi}[G_t|S_t = s] = \mathbb{E}_{b}[\rho(G_t)G_t|S_t = s],\]</span>
or since we base our estimates on sample-paths, we are in fact interested in estimating the action-values
<span class="math display">\[q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] = \mathbb{E}_{b}[\rho(G_t)G_t|S_t = s, A_t = a].\]</span>
For this we need the importance sampling ratio given a certain sample-path <span class="math inline">\(S_t, A_t, R_{t+1}, \ldots, R_T, S_T\)</span> with return <span class="math inline">\(G_t\)</span>:
<span class="math display" id="eq:isr">\[
\begin{align}
    \rho(G_t) &amp;= \frac{\Pr{}(S_t, A_t, \dots S_T| S_t = s, A_t = a, \pi)}{\Pr{}(S_t, A_t, \dots, S_T)| S_t = s, A_t = a, b)} \\
                 &amp;= \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)\Pr{}(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)\Pr{}(S_{k+1}|S_k, A_k)}\\
                 &amp;=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}.
\end{align}
\tag{6.2}
\]</span>
Note the transition probabilities cancel out, i.e. the ratio does not depend on the MDP dynamics by only the policies. Moreover, importance sampling ratios are only non-zero for sample-paths where the target-policy has non-zero probability of acting exactly like the behaviour policy <span class="math inline">\(b\)</span>. So, if the behaviour policy takes 10 steps in an sample-path, each of these 10 steps have to have been possible by the target policy, else <span class="math inline">\(\pi(a|s) = 0\)</span> and <span class="math inline">\(\rho_{t:T-1} = 0\)</span>.</p>
<p>We can now approx. <span class="math inline">\(q_\pi(s,a)\)</span> by rewriting Eq. <a href="mod-mc.html#eq:is-approx">(6.1)</a> for <span class="math inline">\(\pi\)</span> given returns from <span class="math inline">\(b\)</span> to
<span class="math display" id="eq:ois">\[
    q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] \approx \frac{1}{n} \sum_{i = 1}^n \rho_iG_i,
    \tag{6.3}
\]</span>
where we assume that given the sample-paths (episodes), have <span class="math inline">\(n\)</span> observations of the return <span class="math inline">\((G_1, \ldots, G_n)\)</span> in state <span class="math inline">\(s\)</span> taking action <span class="math inline">\(a\)</span> with the importance sampling ratio <span class="math inline">\(\rho_i\)</span> calculated using Eq. <a href="mod-mc.html#eq:isr">(6.2)</a>. As a result if we consider the prediction algorithm in Figure <a href="mod-mc.html#fig:mc-prediction-alg">6.1</a> it must be modified by:</p>
<ul>
<li>Generate an sample-path using policy <span class="math inline">\(b\)</span> instead of <span class="math inline">\(\pi\)</span>.</li>
<li>Add a variable W representing the importance sampling ratio which must be set to 1 on line containing <span class="math inline">\(G \leftarrow 0\)</span>.</li>
<li>Modify line <span class="math inline">\(G \leftarrow \gamma G + R_{t+1}\)</span> to <span class="math inline">\(G \leftarrow \gamma WG + R_{t+1}\)</span> since we now need to multiply with the importance sampling ratio.</li>
<li>Add a line after the last with <span class="math inline">\(W \leftarrow W \pi(A_t|S_t)/b(A_t|S_t)\)</span>, i.e. we update the importance sampling ratio.</li>
<li>Note if <span class="math inline">\(\pi(A_t|S_t) = 0\)</span> then we may stop the inner loop earlier (<span class="math inline">\(W=0\)</span> for the remaining <span class="math inline">\(t\)</span>).</li>
<li>Finally, an incremental update of <span class="math inline">\(V\)</span> can be done having a vector counting the number of of returns found for each state. Then the incremental update is
<span class="math display" id="eq:upd">\[
V(s) \leftarrow V(s) + \frac{1}{n} \left[WG - V(s)\right].
\tag{6.4}
\]</span>
where <span class="math inline">\(n\)</span> denote the number of realized returns found for state <span class="math inline">\(s\)</span> and <span class="math inline">\(G\)</span> the current realized return.</li>
</ul>
<div id="weighted-importance-sampling" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Weighted importance sampling<a href="mod-mc.html#weighted-importance-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When using a sample average the importance sampling method is called <em>ordinary importance sampling</em>. Ordinary importance sampling may result in a high variance which is not good. As a result we may use other weights and instead of Eq. <a href="mod-mc.html#eq:ois">(6.3)</a> use the estimate (<em>weighted importance sampling</em>):
<span class="math display">\[
    q_\pi(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] \approx \frac{1}{\sum_{i = 1}^n \rho_i} \sum_{i = 1}^n \rho_iG_i.
\]</span>
An incremental update then becomes:</p>
<p><span class="math display" id="eq:wpd">\[
\begin{align}
    q_\pi(s,a) &amp;\approx V_{n+1} \\
    &amp;= \frac{1}{\sum_{i = 1}^n \rho_i} \sum_{i = 1}^n \rho_iG_i \\
    &amp;= \frac{1}{C_n} \sum_{i = 1}^n W_iG_i \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n-1}\frac{1}{C_{n-1}} \sum_{i = 1}^{n-1} W_iG_i) \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n-1}V_n) \\
    &amp;= \frac{1}{C_n} (W_nG_n + (C_{n} - W_{n}) V_n) \\
    &amp;= \frac{1}{C_n} (W_nG_n + C_{n}V_n - W_{n} V_n) \\
    &amp;= V_n + \frac{W_n}{C_n} (G_n  - V_n),
\end{align}
\tag{6.5}
\]</span>
where <span class="math inline">\(C_n = \sum_{i = 1}^n \rho_i\)</span> is the sum of the ratios and and <span class="math inline">\(W_n\)</span> the ratio for the n’th return. Using weighted importance sampling gives a smaller variance and hence faster convergence. An off-policy prediction algorithm using weighted importance sampling and incremental updates is given in Figure <a href="mod-mc.html#fig:mc-pred-off-policy-alg">6.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mc-pred-off-policy-alg"></span>
<img src="img/mc-off-policy-prediction.png" alt="Off-policy MC prediction [@Sutton18]."  />
<p class="caption">
Figure 6.6: Off-policy MC prediction <span class="citation">(<a href="#ref-Sutton18">Sutton and Barto 2018</a>)</span>.
</p>
</div>
<p>Note both Eq. <a href="mod-mc.html#eq:upd">(6.4)</a> and Eq. <a href="mod-mc.html#eq:wpd">(6.5)</a> follows the general incremental formula:
<span class="math display">\[\begin{equation}
New Estimate \leftarrow Old Estimate + Step Size \left[Observation - Old Estimate \right].
\end{equation}\]</span>
For ordinary importance sampling the step-size is <span class="math inline">\(1/n\)</span> and for weighted importance sampling the step-size is <span class="math inline">\(W_n/C_n\)</span>.</p>
</div>
</div>
<div id="off-policy-control-improvement" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Off-policy control (improvement)<a href="mod-mc.html#off-policy-control-improvement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Having discussed a framework for off-policy MC prediction, we can now give a GPI algorithm for off-policy MC control that estimate <span class="math inline">\(\pi_*\)</span> and <span class="math inline">\(q_*\)</span> by using rewards obtained through behaviour policy <span class="math inline">\(b\)</span>. We will focus on using weighted importance sampling with incremental updates. The algorithm is given in Figure <a href="mod-mc.html#fig:mc-gpi-off-policy-alg">6.7</a>. The target policy <span class="math inline">\(\pi\)</span> is the greedy policy with respect to <span class="math inline">\(Q\)</span>, which is an estimate of <span class="math inline">\(q_\pi\)</span>. This algorithm converges to <span class="math inline">\(q_\pi\)</span> as long as an infinite number of returns are observed for each state-action pair. This can be achieved by making <span class="math inline">\(b\)</span> <span class="math inline">\(\varepsilon\)</span>-soft. The policy <span class="math inline">\(\pi\)</span> converges to <span class="math inline">\(\pi_*\)</span> at all encountered states even if <span class="math inline">\(b\)</span> changes (to another <span class="math inline">\(\varepsilon\)</span>-soft policy) between or within sample-paths. Note we exit the inner loop if <span class="math inline">\(A_t \neq \pi(S_t)\)</span> which implies <span class="math inline">\(W=0\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mc-gpi-off-policy-alg"></span>
<img src="img/mc-off-policy-gpi.png" alt="Off-policy GPI [@Sutton18]."  />
<p class="caption">
Figure 6.7: Off-policy GPI <span class="citation">(<a href="#ref-Sutton18">Sutton and Barto 2018</a>)</span>.
</p>
</div>
<p>Notice that this policy only learns from sample-paths in which <span class="math inline">\(b\)</span> selects only greedy actions after some timestep. This can greatly slow learning.</p>
</div>
<div id="summary-5" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Summary<a href="mod-mc.html#summary-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Read Chapter 5.10 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18">2018</a>)</span>.</p>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Exercises<a href="mod-mc.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Sometimes solutions can be seen by pressing the button besides a question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="ex-mc-seasonal" class="section level3 hasAnchor" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Exercise - Seasonal inventory and sales planning<a href="mod-mc.html#ex-mc-seasonal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the seasonal product in Example <a href="mod-mc.html#mc-seasonal">6.4.4</a>.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="mod-mc.html#cb63-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">876</span>)</span>
<span id="cb63-2"><a href="mod-mc.html#cb63-2" tabindex="-1"></a>prices <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>)</span>
<span id="cb63-3"><a href="mod-mc.html#cb63-3" tabindex="-1"></a>env <span class="ot">&lt;-</span> RLEnvSeasonal<span class="sc">$</span><span class="fu">new</span>(<span class="at">maxInv =</span> <span class="dv">100</span>, <span class="at">maxT =</span> <span class="dv">15</span>, <span class="at">scrapPrice =</span> <span class="dv">5</span>, <span class="at">purchasePrice =</span> <span class="dv">14</span>, prices)</span></code></pre></div>
<!-- Q1 -->
<div id="nsd3QCYiHPGPH3UKIUYD" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="nsd3QCYiHPGPH3UKIUYD-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="nsd3QCYiHPGPH3UKIUYD-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
With a low inventory level, there is a high probability of selling everything even for the highest price. This effect is more dominant if the number of time-steps left is high and decrease as we approach week 15. Contrary with a high inventory level there is a low probability of selling everything (if the price is set to high) and therefore we have to use a lower price. This effect is more dominant if the number of time-steps left is low.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#nsd3QCYiHPGPH3UKIUYD">
Solution
</button>
<ol style="list-style-type: decimal">
<li>Consider the optimal policy for the MDP modelling the problem in Figure <a href="mod-mc.html#fig:seasonal-mdp-plot">6.5</a>. Here the optimal price is high at the lower-left and decrease as we approach the upper-right. Explain why this from an intuitive point of view seems correct.</li>
</ol>
<!-- Q2 -->
<div id="EFEHXBqbuGwrGhbEJMHp" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="EFEHXBqbuGwrGhbEJMHp-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="EFEHXBqbuGwrGhbEJMHp-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
The optimal policy is found using a threshold value, i.e the MDP finds an optimal policy with state-values within the threshold. Hence small differences in state-value may produce different optimal prices. Another reason could be that we have multiple optimal policies.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#EFEHXBqbuGwrGhbEJMHp">
Solution
</button>
<ol start="2" style="list-style-type: decimal">
<li>Consider the optimal policy for the MDP modelling the problem in Figure <a href="mod-mc.html#fig:seasonal-mdp-plot">6.5</a>. For a fixed time <span class="math inline">\(t\)</span> the optimal price does not always decrease as the inventory level increase. Some time two prices oscillate (e.g. first green, then red and then green again). Give possible reasons for this happening.</li>
</ol>
<!-- Q3 -->
<div id="id_8T291cLSEmyzXPhW8qUI" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="8T291cLSEmyzXPhW8qUI-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="8T291cLSEmyzXPhW8qUI-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
In state <span class="math inline">\((1,1)\)</span> the optimal price is 25 (we only have one item left to sell over 15 weeks) and in state <span class="math inline">\((100,15)\)</span> the optimal price is 10 (we can not sell all items in one week so set price low).
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#8T291cLSEmyzXPhW8qUI">
Solution
</button>
<ol start="3" style="list-style-type: decimal">
<li>Let us consider two extreme states <span class="math inline">\((1,1)\)</span> and <span class="math inline">\((100,15)\)</span>. From an intuitively point of view, what is the optimal action/price to do in these states?</li>
</ol>
<!-- Q4 -->
<div id="Qz4cdJ73F0CGfmPHpcDk" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="Qz4cdJ73F0CGfmPHpcDk-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="Qz4cdJ73F0CGfmPHpcDk-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
Since the epsilon-greedy policy selects <span class="math inline">\(a=20\)</span> (at random) first, <span class="math inline">\(a=20\)</span> becomes the greedy action that is selected with highest probability. In this run, when selecting the next action, 20 is always selected and hence only <span class="math inline">\(q\)</span> for <span class="math inline">\(a=20\)</span> is updated 10 times (minimum number of iterations). Afterwards differences in state-values of states in the current episode are compared (<code>oldV</code> and <code>v</code> column). Since they are the same the algorithm stop. That is, we never get a chance to have a view on action <span class="math inline">\(a=25\)</span> or put differently, the stopping criteria may not work well for episodes with small length.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#Qz4cdJ73F0CGfmPHpcDk">
Solution
</button>
<ol start="4" style="list-style-type: decimal">
<li>Consider state <span class="math inline">\((1,1)\)</span> and let us try approximating best epsilon-greedy policy using MC with the verbose option so it prints out info about each episode:</li>
</ol>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="mod-mc.html#cb64-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">79</span>)</span>
<span id="cb64-2"><a href="mod-mc.html#cb64-2" tabindex="-1"></a>time <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb64-3"><a href="mod-mc.html#cb64-3" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb64-4"><a href="mod-mc.html#cb64-4" tabindex="-1"></a>state <span class="ot">=</span> <span class="fu">str_c</span>(i, <span class="st">&quot;,&quot;</span>, time)</span>
<span id="cb64-5"><a href="mod-mc.html#cb64-5" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">10</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> T, <span class="at">eps =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> T)</span>
<span id="cb64-6"><a href="mod-mc.html#cb64-6" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-7"><a href="mod-mc.html#cb64-7" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-8"><a href="mod-mc.html#cb64-8" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-9"><a href="mod-mc.html#cb64-9" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     1     1     0     6     6     0   5.1</span></span>
<span id="cb64-10"><a href="mod-mc.html#cb64-10" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-11"><a href="mod-mc.html#cb64-11" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-12"><a href="mod-mc.html#cb64-12" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-13"><a href="mod-mc.html#cb64-13" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     2     2     6     6     6   5.1   5.1</span></span>
<span id="cb64-14"><a href="mod-mc.html#cb64-14" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-15"><a href="mod-mc.html#cb64-15" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-16"><a href="mod-mc.html#cb64-16" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-17"><a href="mod-mc.html#cb64-17" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     3     3     6     6     6   5.1   5.1</span></span>
<span id="cb64-18"><a href="mod-mc.html#cb64-18" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-19"><a href="mod-mc.html#cb64-19" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-20"><a href="mod-mc.html#cb64-20" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-21"><a href="mod-mc.html#cb64-21" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     4     4     6     6     6   5.1   5.1</span></span>
<span id="cb64-22"><a href="mod-mc.html#cb64-22" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-23"><a href="mod-mc.html#cb64-23" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-24"><a href="mod-mc.html#cb64-24" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-25"><a href="mod-mc.html#cb64-25" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     5     5     6     6     6   5.1   5.1</span></span>
<span id="cb64-26"><a href="mod-mc.html#cb64-26" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-27"><a href="mod-mc.html#cb64-27" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-28"><a href="mod-mc.html#cb64-28" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-29"><a href="mod-mc.html#cb64-29" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     6     6     6     6     6   5.1   5.1</span></span>
<span id="cb64-30"><a href="mod-mc.html#cb64-30" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-31"><a href="mod-mc.html#cb64-31" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-32"><a href="mod-mc.html#cb64-32" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-33"><a href="mod-mc.html#cb64-33" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     7     7     6     6     6   5.1   5.1</span></span>
<span id="cb64-34"><a href="mod-mc.html#cb64-34" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-35"><a href="mod-mc.html#cb64-35" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-36"><a href="mod-mc.html#cb64-36" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-37"><a href="mod-mc.html#cb64-37" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     8     8     6     6     6   5.1   5.1</span></span>
<span id="cb64-38"><a href="mod-mc.html#cb64-38" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-39"><a href="mod-mc.html#cb64-39" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-40"><a href="mod-mc.html#cb64-40" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-41"><a href="mod-mc.html#cb64-41" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     9     9     6     6     6   5.1   5.1</span></span>
<span id="cb64-42"><a href="mod-mc.html#cb64-42" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb64-43"><a href="mod-mc.html#cb64-43" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb64-44"><a href="mod-mc.html#cb64-44" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-45"><a href="mod-mc.html#cb64-45" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6    10    10     6     6     6   5.1   5.1</span></span>
<span id="cb64-46"><a href="mod-mc.html#cb64-46" tabindex="-1"></a>dfRL <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionValues</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb64-47"><a href="mod-mc.html#cb64-47" tabindex="-1"></a>   <span class="fu">separate</span>(state, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;inv&quot;</span>, <span class="st">&quot;t&quot;</span>), <span class="at">remove =</span> F, <span class="at">convert =</span> T)</span>
<span id="cb64-48"><a href="mod-mc.html#cb64-48" tabindex="-1"></a>dfRL <span class="sc">%&gt;%</span></span>
<span id="cb64-49"><a href="mod-mc.html#cb64-49" tabindex="-1"></a>   <span class="fu">filter</span>(inv <span class="sc">==</span> i, t <span class="sc">==</span> time) <span class="sc">%&gt;%</span> </span>
<span id="cb64-50"><a href="mod-mc.html#cb64-50" tabindex="-1"></a>   <span class="fu">left_join</span>(dfMDP)</span>
<span id="cb64-51"><a href="mod-mc.html#cb64-51" tabindex="-1"></a><span class="co">#&gt; # A tibble: 4 × 7</span></span>
<span id="cb64-52"><a href="mod-mc.html#cb64-52" tabindex="-1"></a><span class="co">#&gt;   state   inv     t action     q     n     v</span></span>
<span id="cb64-53"><a href="mod-mc.html#cb64-53" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb64-54"><a href="mod-mc.html#cb64-54" tabindex="-1"></a><span class="co">#&gt; 1 1,1       1     1 10         0     0  NA  </span></span>
<span id="cb64-55"><a href="mod-mc.html#cb64-55" tabindex="-1"></a><span class="co">#&gt; 2 1,1       1     1 15         0     0  NA  </span></span>
<span id="cb64-56"><a href="mod-mc.html#cb64-56" tabindex="-1"></a><span class="co">#&gt; 3 1,1       1     1 20         6    10  NA  </span></span>
<span id="cb64-57"><a href="mod-mc.html#cb64-57" tabindex="-1"></a><span class="co">#&gt; 4 1,1       1     1 25         0     0  11.0</span></span></code></pre></div>
<p>Why do we not estimate the optimal action here and why do the algorithm stop. Hint: You may have a look at method <code>gpiOnPolicyMC</code> in the RLAgent class.</p>
<!-- Q5 -->
<div id="E1UWYwFVTUG6TatvgDC4" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="E1UWYwFVTUG6TatvgDC4-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="E1UWYwFVTUG6TatvgDC4-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
Since the initial action-value is high, all actions will be explored with high probability first. This may help the algorithm to get a good start.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#E1UWYwFVTUG6TatvgDC4">
Solution
</button>
<ol start="5" style="list-style-type: decimal">
<li>Let us try to set the action-value high initially:</li>
</ol>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="mod-mc.html#cb65-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">105</span>)</span>
<span id="cb65-2"><a href="mod-mc.html#cb65-2" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setActionValue</span>(<span class="dv">1000</span>)</span>
<span id="cb65-3"><a href="mod-mc.html#cb65-3" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setActionCtrValue</span>()   <span class="co"># reset counter</span></span>
<span id="cb65-4"><a href="mod-mc.html#cb65-4" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setStateCtrValue</span>()    <span class="co"># reset counter</span></span>
<span id="cb65-5"><a href="mod-mc.html#cb65-5" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getActionValues</span>(<span class="st">&quot;1,1&quot;</span>)</span>
<span id="cb65-6"><a href="mod-mc.html#cb65-6" tabindex="-1"></a><span class="co">#&gt; # A tibble: 4 × 4</span></span>
<span id="cb65-7"><a href="mod-mc.html#cb65-7" tabindex="-1"></a><span class="co">#&gt;   state action     q     n</span></span>
<span id="cb65-8"><a href="mod-mc.html#cb65-8" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-9"><a href="mod-mc.html#cb65-9" tabindex="-1"></a><span class="co">#&gt; 1 1,1   10      1000     0</span></span>
<span id="cb65-10"><a href="mod-mc.html#cb65-10" tabindex="-1"></a><span class="co">#&gt; 2 1,1   15      1000     0</span></span>
<span id="cb65-11"><a href="mod-mc.html#cb65-11" tabindex="-1"></a><span class="co">#&gt; 3 1,1   20      1000     0</span></span>
<span id="cb65-12"><a href="mod-mc.html#cb65-12" tabindex="-1"></a><span class="co">#&gt; 4 1,1   25      1000     0</span></span>
<span id="cb65-13"><a href="mod-mc.html#cb65-13" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">5</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> F, <span class="at">eps =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> T)</span>
<span id="cb65-14"><a href="mod-mc.html#cb65-14" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb65-15"><a href="mod-mc.html#cb65-15" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb65-16"><a href="mod-mc.html#cb65-16" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-17"><a href="mod-mc.html#cb65-17" tabindex="-1"></a><span class="co">#&gt; 1 1,1   15        1     1     1  1000     1     1  1000  950.</span></span>
<span id="cb65-18"><a href="mod-mc.html#cb65-18" tabindex="-1"></a><span class="co">#&gt; # A tibble: 2 × 10</span></span>
<span id="cb65-19"><a href="mod-mc.html#cb65-19" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb65-20"><a href="mod-mc.html#cb65-20" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-21"><a href="mod-mc.html#cb65-21" tabindex="-1"></a><span class="co">#&gt; 1 1,1   25      -14     1     2  1000    11    11  950.  901.</span></span>
<span id="cb65-22"><a href="mod-mc.html#cb65-22" tabindex="-1"></a><span class="co">#&gt; 2 1,2   25       25     1     1  1000    25    25 1000   951.</span></span>
<span id="cb65-23"><a href="mod-mc.html#cb65-23" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb65-24"><a href="mod-mc.html#cb65-24" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb65-25"><a href="mod-mc.html#cb65-25" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-26"><a href="mod-mc.html#cb65-26" tabindex="-1"></a><span class="co">#&gt; 1 1,1   10       -4     1     3  1000    -4    -4  901.  850.</span></span>
<span id="cb65-27"><a href="mod-mc.html#cb65-27" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 10</span></span>
<span id="cb65-28"><a href="mod-mc.html#cb65-28" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb65-29"><a href="mod-mc.html#cb65-29" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-30"><a href="mod-mc.html#cb65-30" tabindex="-1"></a><span class="co">#&gt; 1 1,1   20        6     1     4  1000     6     6  850.   9.5</span></span>
<span id="cb65-31"><a href="mod-mc.html#cb65-31" tabindex="-1"></a><span class="co">#&gt; # A tibble: 2 × 10</span></span>
<span id="cb65-32"><a href="mod-mc.html#cb65-32" tabindex="-1"></a><span class="co">#&gt;   s     a         r    nA    nS  oldQ     q     g  oldV     v</span></span>
<span id="cb65-33"><a href="mod-mc.html#cb65-33" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-34"><a href="mod-mc.html#cb65-34" tabindex="-1"></a><span class="co">#&gt; 1 1,1   25      -14     2     5    11    11    11   9.5   9.5</span></span>
<span id="cb65-35"><a href="mod-mc.html#cb65-35" tabindex="-1"></a><span class="co">#&gt; 2 1,2   25       25     2     2    25    25    25 951.  951.</span></span>
<span id="cb65-36"><a href="mod-mc.html#cb65-36" tabindex="-1"></a>dfRL <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionValues</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb65-37"><a href="mod-mc.html#cb65-37" tabindex="-1"></a>   <span class="fu">separate</span>(state, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;inv&quot;</span>, <span class="st">&quot;t&quot;</span>), <span class="at">remove =</span> F, <span class="at">convert =</span> T)</span>
<span id="cb65-38"><a href="mod-mc.html#cb65-38" tabindex="-1"></a>dfRL <span class="sc">%&gt;%</span></span>
<span id="cb65-39"><a href="mod-mc.html#cb65-39" tabindex="-1"></a>   <span class="fu">filter</span>(inv <span class="sc">==</span> i, t <span class="sc">==</span> time) <span class="sc">%&gt;%</span> </span>
<span id="cb65-40"><a href="mod-mc.html#cb65-40" tabindex="-1"></a>   <span class="fu">left_join</span>(dfMDP)</span>
<span id="cb65-41"><a href="mod-mc.html#cb65-41" tabindex="-1"></a><span class="co">#&gt; # A tibble: 4 × 7</span></span>
<span id="cb65-42"><a href="mod-mc.html#cb65-42" tabindex="-1"></a><span class="co">#&gt;   state   inv     t action     q     n     v</span></span>
<span id="cb65-43"><a href="mod-mc.html#cb65-43" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb65-44"><a href="mod-mc.html#cb65-44" tabindex="-1"></a><span class="co">#&gt; 1 1,1       1     1 10        -4     1  NA  </span></span>
<span id="cb65-45"><a href="mod-mc.html#cb65-45" tabindex="-1"></a><span class="co">#&gt; 2 1,1       1     1 15         1     1  NA  </span></span>
<span id="cb65-46"><a href="mod-mc.html#cb65-46" tabindex="-1"></a><span class="co">#&gt; 3 1,1       1     1 20         6     1  NA  </span></span>
<span id="cb65-47"><a href="mod-mc.html#cb65-47" tabindex="-1"></a><span class="co">#&gt; 4 1,1       1     1 25        11     2  11.0</span></span></code></pre></div>
<p>What happens with the sequence of episodes?</p>
<!-- Q6 -->
<div id="H1htBI4rrFF4QyQbhqTQ" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="H1htBI4rrFF4QyQbhqTQ-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="H1htBI4rrFF4QyQbhqTQ-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
Since we do simulations (we here do 3) each simulation may produce different results depending on how the algorithm starts. That is, we need more iterations for converging to the correct state-value.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#H1htBI4rrFF4QyQbhqTQ">
Solution
</button>
<ol start="6" style="list-style-type: decimal">
<li>Consider state <span class="math inline">\((1,1)\)</span> and let us try approximating best epsilon-greedy policy using MC using more episodes:</li>
</ol>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="mod-mc.html#cb66-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">749</span>)</span>
<span id="cb66-2"><a href="mod-mc.html#cb66-2" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">100</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> T, <span class="at">eps =</span> <span class="fl">0.2</span>)</span>
<span id="cb66-3"><a href="mod-mc.html#cb66-3" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getStateValueQ</span>(state)  <span class="co"># get the state-value v(s) of the epsilon-greedy policy</span></span>
<span id="cb66-4"><a href="mod-mc.html#cb66-4" tabindex="-1"></a><span class="co">#&gt;   10 </span></span>
<span id="cb66-5"><a href="mod-mc.html#cb66-5" tabindex="-1"></a><span class="co">#&gt; 9.34</span></span>
<span id="cb66-6"><a href="mod-mc.html#cb66-6" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">1000</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> T, <span class="at">eps =</span> <span class="fl">0.2</span>)</span>
<span id="cb66-7"><a href="mod-mc.html#cb66-7" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getStateValueQ</span>(state)  <span class="co"># get the state-value v(s) of the epsilon-greedy policy</span></span>
<span id="cb66-8"><a href="mod-mc.html#cb66-8" tabindex="-1"></a><span class="co">#&gt;   10 </span></span>
<span id="cb66-9"><a href="mod-mc.html#cb66-9" tabindex="-1"></a><span class="co">#&gt; 9.03</span></span>
<span id="cb66-10"><a href="mod-mc.html#cb66-10" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">2000</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> T, <span class="at">eps =</span> <span class="fl">0.2</span>)</span>
<span id="cb66-11"><a href="mod-mc.html#cb66-11" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getStateValueQ</span>(state)  <span class="co"># get the state-value v(s) of the epsilon-greedy policy</span></span>
<span id="cb66-12"><a href="mod-mc.html#cb66-12" tabindex="-1"></a><span class="co">#&gt;   10 </span></span>
<span id="cb66-13"><a href="mod-mc.html#cb66-13" tabindex="-1"></a><span class="co">#&gt; 9.49</span></span></code></pre></div>
<p>Why is the state-value not increasing monotone as number of iterations increase?</p>
<!-- Q7 -->
<div id="nTZymV5TdwF3iCGuz0Kj" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="nTZymV5TdwF3iCGuz0Kj-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="nTZymV5TdwF3iCGuz0Kj-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
No, the q-values are the averages over all the generated episodes (also those generated using bad policies). With enough simulations these numbers will converge against the action-values for the best epsilon-greedy policy (<span class="math inline">\(\epsilon = 0.5\)</span> here), not the optimal greedy policy.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#nTZymV5TdwF3iCGuz0Kj">
Solution
</button>
<ol start="7" style="list-style-type: decimal">
<li>Consider state <span class="math inline">\((25,8)\)</span> and let us approximate the best epsilon-greedy policy:</li>
</ol>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="mod-mc.html#cb67-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">281</span>)</span>
<span id="cb67-2"><a href="mod-mc.html#cb67-2" tabindex="-1"></a>time <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb67-3"><a href="mod-mc.html#cb67-3" tabindex="-1"></a>i <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb67-4"><a href="mod-mc.html#cb67-4" tabindex="-1"></a>state <span class="ot">=</span> <span class="fu">str_c</span>(i, <span class="st">&quot;,&quot;</span>, time)</span>
<span id="cb67-5"><a href="mod-mc.html#cb67-5" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setActionValue</span>(<span class="dv">1000</span>)</span>
<span id="cb67-6"><a href="mod-mc.html#cb67-6" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setActionCtrValue</span>()   <span class="co"># reset counter</span></span>
<span id="cb67-7"><a href="mod-mc.html#cb67-7" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setStateCtrValue</span>()    <span class="co"># reset counter</span></span>
<span id="cb67-8"><a href="mod-mc.html#cb67-8" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">gpiOnPolicyMC</span>(env, <span class="at">minIte =</span> <span class="dv">5000</span>, <span class="at">maxIte =</span> <span class="dv">100000</span>, <span class="at">states =</span> state, <span class="at">reset =</span> F, <span class="at">eps =</span> <span class="fl">0.5</span>)</span>
<span id="cb67-9"><a href="mod-mc.html#cb67-9" tabindex="-1"></a>dfRL <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getActionValues</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb67-10"><a href="mod-mc.html#cb67-10" tabindex="-1"></a>   <span class="fu">separate</span>(state, <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">into =</span> <span class="fu">c</span>(<span class="st">&quot;inv&quot;</span>, <span class="st">&quot;t&quot;</span>), <span class="at">remove =</span> F, <span class="at">convert =</span> T)</span>
<span id="cb67-11"><a href="mod-mc.html#cb67-11" tabindex="-1"></a>dfRL <span class="sc">%&gt;%</span></span>
<span id="cb67-12"><a href="mod-mc.html#cb67-12" tabindex="-1"></a>   <span class="fu">filter</span>(inv <span class="sc">==</span> i, t <span class="sc">==</span> time) <span class="sc">%&gt;%</span> </span>
<span id="cb67-13"><a href="mod-mc.html#cb67-13" tabindex="-1"></a>   <span class="fu">left_join</span>(dfMDP)</span>
<span id="cb67-14"><a href="mod-mc.html#cb67-14" tabindex="-1"></a><span class="co">#&gt; # A tibble: 4 × 7</span></span>
<span id="cb67-15"><a href="mod-mc.html#cb67-15" tabindex="-1"></a><span class="co">#&gt;   state   inv     t action     q     n     v</span></span>
<span id="cb67-16"><a href="mod-mc.html#cb67-16" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb67-17"><a href="mod-mc.html#cb67-17" tabindex="-1"></a><span class="co">#&gt; 1 25,8     25     8 10      295.   148   NA </span></span>
<span id="cb67-18"><a href="mod-mc.html#cb67-18" tabindex="-1"></a><span class="co">#&gt; 2 25,8     25     8 15      436.  1491   NA </span></span>
<span id="cb67-19"><a href="mod-mc.html#cb67-19" tabindex="-1"></a><span class="co">#&gt; 3 25,8     25     8 20      480.  3566  502.</span></span>
<span id="cb67-20"><a href="mod-mc.html#cb67-20" tabindex="-1"></a><span class="co">#&gt; 4 25,8     25     8 25      438.   135   NA</span></span></code></pre></div>
<p>As can be seen the the optimal state value for the MDP is not equal the action-value for that action. Should that have been the case if run enough simulations?</p>
<!-- Q8 -->
<ol start="8" style="list-style-type: decimal">
<li>Consider the very optimistic policy which set the price to 25 (where possible):</li>
</ol>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="mod-mc.html#cb68-1" tabindex="-1"></a><span class="do">## Set the policy to price 25</span></span>
<span id="cb68-2"><a href="mod-mc.html#cb68-2" tabindex="-1"></a>states <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getStateKeys</span>() <span class="sc">%&gt;%</span> <span class="fu">setdiff</span>(<span class="st">&quot;0&quot;</span>) </span>
<span id="cb68-3"><a href="mod-mc.html#cb68-3" tabindex="-1"></a>states <span class="ot">&lt;-</span> states[<span class="sc">!</span><span class="fu">str_detect</span>(states, <span class="st">&quot;,15&quot;</span>)]</span>
<span id="cb68-4"><a href="mod-mc.html#cb68-4" tabindex="-1"></a>pi <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;25&quot;</span> <span class="ot">=</span> <span class="dv">1</span>)</span>
<span id="cb68-5"><a href="mod-mc.html#cb68-5" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setPolicy</span>(states, pi)</span>
<span id="cb68-6"><a href="mod-mc.html#cb68-6" tabindex="-1"></a><span class="co"># set last time-step</span></span>
<span id="cb68-7"><a href="mod-mc.html#cb68-7" tabindex="-1"></a>states <span class="ot">&lt;-</span> agent<span class="sc">$</span><span class="fu">getStateKeys</span>() <span class="sc">%&gt;%</span> <span class="fu">setdiff</span>(<span class="st">&quot;0&quot;</span>) </span>
<span id="cb68-8"><a href="mod-mc.html#cb68-8" tabindex="-1"></a>states <span class="ot">&lt;-</span> states[<span class="fu">str_detect</span>(states, <span class="st">&quot;,15&quot;</span>)]</span>
<span id="cb68-9"><a href="mod-mc.html#cb68-9" tabindex="-1"></a>pi <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;5&quot;</span> <span class="ot">=</span> <span class="dv">1</span>)</span>
<span id="cb68-10"><a href="mod-mc.html#cb68-10" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setPolicy</span>(states, pi)</span>
<span id="cb68-11"><a href="mod-mc.html#cb68-11" tabindex="-1"></a><span class="co"># set dummy</span></span>
<span id="cb68-12"><a href="mod-mc.html#cb68-12" tabindex="-1"></a>pi <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;dummy&quot;</span> <span class="ot">=</span> <span class="dv">1</span>)</span>
<span id="cb68-13"><a href="mod-mc.html#cb68-13" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">setPolicy</span>(<span class="st">&quot;0&quot;</span>, pi)</span>
<span id="cb68-14"><a href="mod-mc.html#cb68-14" tabindex="-1"></a><span class="do">## Check</span></span>
<span id="cb68-15"><a href="mod-mc.html#cb68-15" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getPolicy</span>()</span>
<span id="cb68-16"><a href="mod-mc.html#cb68-16" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1,501 × 3</span></span>
<span id="cb68-17"><a href="mod-mc.html#cb68-17" tabindex="-1"></a><span class="co">#&gt;    state action    pr</span></span>
<span id="cb68-18"><a href="mod-mc.html#cb68-18" tabindex="-1"></a><span class="co">#&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;</span></span>
<span id="cb68-19"><a href="mod-mc.html#cb68-19" tabindex="-1"></a><span class="co">#&gt;  1 0     dummy      1</span></span>
<span id="cb68-20"><a href="mod-mc.html#cb68-20" tabindex="-1"></a><span class="co">#&gt;  2 1,1   25         1</span></span>
<span id="cb68-21"><a href="mod-mc.html#cb68-21" tabindex="-1"></a><span class="co">#&gt;  3 1,10  25         1</span></span>
<span id="cb68-22"><a href="mod-mc.html#cb68-22" tabindex="-1"></a><span class="co">#&gt;  4 1,11  25         1</span></span>
<span id="cb68-23"><a href="mod-mc.html#cb68-23" tabindex="-1"></a><span class="co">#&gt;  5 1,12  25         1</span></span>
<span id="cb68-24"><a href="mod-mc.html#cb68-24" tabindex="-1"></a><span class="co">#&gt;  6 1,13  25         1</span></span>
<span id="cb68-25"><a href="mod-mc.html#cb68-25" tabindex="-1"></a><span class="co">#&gt;  7 1,14  25         1</span></span>
<span id="cb68-26"><a href="mod-mc.html#cb68-26" tabindex="-1"></a><span class="co">#&gt;  8 1,15  5          1</span></span>
<span id="cb68-27"><a href="mod-mc.html#cb68-27" tabindex="-1"></a><span class="co">#&gt;  9 1,2   25         1</span></span>
<span id="cb68-28"><a href="mod-mc.html#cb68-28" tabindex="-1"></a><span class="co">#&gt; 10 1,3   25         1</span></span>
<span id="cb68-29"><a href="mod-mc.html#cb68-29" tabindex="-1"></a><span class="co">#&gt; # ℹ 1,491 more rows</span></span></code></pre></div>
<p>We may generate an episode using:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="mod-mc.html#cb69-1" tabindex="-1"></a>env<span class="sc">$</span><span class="fu">getEpisodePi</span>(agent, <span class="st">&quot;5,1&quot;</span>)</span>
<span id="cb69-2"><a href="mod-mc.html#cb69-2" tabindex="-1"></a><span class="co">#&gt; # A tibble: 7 × 3</span></span>
<span id="cb69-3"><a href="mod-mc.html#cb69-3" tabindex="-1"></a><span class="co">#&gt;   s     a         r</span></span>
<span id="cb69-4"><a href="mod-mc.html#cb69-4" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;</span></span>
<span id="cb69-5"><a href="mod-mc.html#cb69-5" tabindex="-1"></a><span class="co">#&gt; 1 5,1   25      -45</span></span>
<span id="cb69-6"><a href="mod-mc.html#cb69-6" tabindex="-1"></a><span class="co">#&gt; 2 4,2   25       25</span></span>
<span id="cb69-7"><a href="mod-mc.html#cb69-7" tabindex="-1"></a><span class="co">#&gt; 3 3,3   25        0</span></span>
<span id="cb69-8"><a href="mod-mc.html#cb69-8" tabindex="-1"></a><span class="co">#&gt; 4 3,4   25        0</span></span>
<span id="cb69-9"><a href="mod-mc.html#cb69-9" tabindex="-1"></a><span class="co">#&gt; 5 3,5   25       25</span></span>
<span id="cb69-10"><a href="mod-mc.html#cb69-10" tabindex="-1"></a><span class="co">#&gt; 6 2,6   25       25</span></span>
<span id="cb69-11"><a href="mod-mc.html#cb69-11" tabindex="-1"></a><span class="co">#&gt; 7 1,7   25       25</span></span></code></pre></div>
<p>Let us try to evaluate this policy in states <span class="math inline">\((5,1)\)</span> and <span class="math inline">\((100,10)\)</span>:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="mod-mc.html#cb70-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6778</span>)</span>
<span id="cb70-2"><a href="mod-mc.html#cb70-2" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">policyEvalMC</span>(env, <span class="at">states =</span> <span class="st">&quot;5,1&quot;</span>)</span>
<span id="cb70-3"><a href="mod-mc.html#cb70-3" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getStateValues</span>(<span class="st">&quot;5,1&quot;</span>)</span>
<span id="cb70-4"><a href="mod-mc.html#cb70-4" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 2</span></span>
<span id="cb70-5"><a href="mod-mc.html#cb70-5" tabindex="-1"></a><span class="co">#&gt; # Rowwise: </span></span>
<span id="cb70-6"><a href="mod-mc.html#cb70-6" tabindex="-1"></a><span class="co">#&gt;   state     v</span></span>
<span id="cb70-7"><a href="mod-mc.html#cb70-7" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;dbl&gt;</span></span>
<span id="cb70-8"><a href="mod-mc.html#cb70-8" tabindex="-1"></a><span class="co">#&gt; 1 5,1      55</span></span>
<span id="cb70-9"><a href="mod-mc.html#cb70-9" tabindex="-1"></a>dfMDP <span class="sc">%&gt;%</span> <span class="fu">filter</span>(state <span class="sc">==</span> <span class="st">&quot;5,1&quot;</span>)</span>
<span id="cb70-10"><a href="mod-mc.html#cb70-10" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 5</span></span>
<span id="cb70-11"><a href="mod-mc.html#cb70-11" tabindex="-1"></a><span class="co">#&gt;   state   inv     t     v action</span></span>
<span id="cb70-12"><a href="mod-mc.html#cb70-12" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; </span></span>
<span id="cb70-13"><a href="mod-mc.html#cb70-13" tabindex="-1"></a><span class="co">#&gt; 1 5,1       5     1  55.0 25</span></span>
<span id="cb70-14"><a href="mod-mc.html#cb70-14" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">policyEvalMC</span>(env, <span class="at">states =</span> <span class="st">&quot;100,10&quot;</span>)</span>
<span id="cb70-15"><a href="mod-mc.html#cb70-15" tabindex="-1"></a>agent<span class="sc">$</span><span class="fu">getStateValues</span>(<span class="st">&quot;100,10&quot;</span>)</span>
<span id="cb70-16"><a href="mod-mc.html#cb70-16" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 2</span></span>
<span id="cb70-17"><a href="mod-mc.html#cb70-17" tabindex="-1"></a><span class="co">#&gt; # Rowwise: </span></span>
<span id="cb70-18"><a href="mod-mc.html#cb70-18" tabindex="-1"></a><span class="co">#&gt;   state      v</span></span>
<span id="cb70-19"><a href="mod-mc.html#cb70-19" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt;  &lt;dbl&gt;</span></span>
<span id="cb70-20"><a href="mod-mc.html#cb70-20" tabindex="-1"></a><span class="co">#&gt; 1 100,10  578.</span></span>
<span id="cb70-21"><a href="mod-mc.html#cb70-21" tabindex="-1"></a>dfMDP <span class="sc">%&gt;%</span> <span class="fu">filter</span>(state <span class="sc">==</span> <span class="st">&quot;100,10&quot;</span>)</span>
<span id="cb70-22"><a href="mod-mc.html#cb70-22" tabindex="-1"></a><span class="co">#&gt; # A tibble: 1 × 5</span></span>
<span id="cb70-23"><a href="mod-mc.html#cb70-23" tabindex="-1"></a><span class="co">#&gt;   state    inv     t     v action</span></span>
<span id="cb70-24"><a href="mod-mc.html#cb70-24" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; </span></span>
<span id="cb70-25"><a href="mod-mc.html#cb70-25" tabindex="-1"></a><span class="co">#&gt; 1 100,10   100    10 1001. 15</span></span></code></pre></div>
<div id="Y0jNdC3WCMCtd5h79qoX" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="Y0jNdC3WCMCtd5h79qoX-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="Y0jNdC3WCMCtd5h79qoX-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
For state <span class="math inline">\((5,1)\)</span> it is optimal to set the price high for all states in the sample-path and we estimate the optimal policy. For state <span class="math inline">\((100,10)\)</span> it is not optimal to set the price high, i.e. the state-value we estimate here is not the optimal one.
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#Y0jNdC3WCMCtd5h79qoX">
Solution
</button>
<p>Why do we have state-values close and not close to the optimal values for the MDP?</p>

<!-- Various algorithms for the RL course -->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-dp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-td-pred.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/06_mc.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
