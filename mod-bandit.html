<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 2 Multi-armed bandits | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 2 Multi-armed bandits | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bss-osca.github.io/rl/" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 2 Multi-armed bandits | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mod-rl-intro.html"/>
<link rel="next" href="mod-r-setup.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.21/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li><a href="index.html#mod-intro">About the course notes<span></span></a>
<ul>
<li><a href="index.html#learning-outcomes">Learning outcomes<span></span></a></li>
<li><a href="index.html#purpose-of-the-course">Purpose of the course<span></span></a></li>
<li><a href="index.html#learning-goals-of-the-course">Learning goals of the course<span></span></a></li>
<li><a href="index.html#reinforcement-learning-textbook">Reinforcement learning textbook<span></span></a></li>
<li><a href="index.html#course-organization">Course organization<span></span></a></li>
<li><a href="index.html#programming-software">Programming software<span></span></a></li>
<li><a href="index.html#ack">Acknowledgements and license<span></span></a></li>
<li><a href="index.html#sec-intro-ex">Exercises<span></span></a>
<ul>
<li><a href="index.html#sec-intro-ex-annotate">Exercise - How to annotate<span></span></a></li>
<li><a href="index.html#sec-intro-ex-templates">Exercise - Templates<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.11</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.11.1</b> Exercise - Self-Play<span></span></a></li>
<li class="chapter" data-level="1.11.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---symmetries"><i class="fa fa-check"></i><b>1.11.2</b> Exercise - Symmetries<span></span></a></li>
<li class="chapter" data-level="1.11.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---greedy-play"><i class="fa fa-check"></i><b>1.11.3</b> Exercise - Greedy Play<span></span></a></li>
<li class="chapter" data-level="1.11.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---learning-from-exploration"><i class="fa fa-check"></i><b>1.11.4</b> Exercise - Learning from Exploration<span></span></a></li>
<li class="chapter" data-level="1.11.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---other-improvements"><i class="fa fa-check"></i><b>1.11.5</b> Exercise - Other Improvements<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods<span></span></b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#incremential-value-estimation"><i class="fa fa-check"></i><b>2.5</b> Incremential value estimation<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#the-role-of-the-step-size"><i class="fa fa-check"></i><b>2.6</b> The role of the step-size<span></span></a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.7</b> Optimistic initial values<span></span></a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.8</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="mod-bandit.html"><a href="mod-bandit.html#exercise---self-play"><i class="fa fa-check"></i><b>2.8.1</b> Exercise - Self-Play<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R<span></span></a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups<span></span></a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code<span></span></a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes<span></span></a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help<span></span></a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals<span></span></a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon<span></span></a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-bandit" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Module 2</span> Multi-armed bandits<a href="mod-bandit.html#mod-bandit" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This module consider the k-armed bandit problem which is a sequential decision problem with one state and <span class="math inline">\(k\)</span> actions. The problem is used to illustrate different learning methods used in RL.</p>
<div id="learning-outcomes-1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Learning outcomes<a href="mod-bandit.html#learning-outcomes-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<!-- Define reward -->
<!-- Understand the temporal nature of the bandit problem -->
<!-- Define k-armed bandit -->
<!-- Define action-values -->
<!-- Define action-value estimation methods -->
<!-- Define exploration and exploitation -->
<!-- Select actions greedily using an action-value function -->
<!-- Define online learning -->
<!-- Understand a simple online sample-average action-value estimation method -->
<!-- Define the general online update equation -->
<!-- Understand why we might use a constant stepsize in the case of non-stationarity -->
<!-- Define epsilon-greedy -->
<!-- Compare the short-term benefits of exploitation and the long-term benefits of exploration -->
<!-- Understand optimistic initial values -->
<!-- Describe the benefits of optimistic initial values for early exploration -->
<!-- Explain the criticisms of optimistic initial values -->
<!-- Describe the upper confidence bound action selection method -->
<!-- Define optimism in the face of uncertainty -->
<!-- * Describe what VBA is. -->
<!-- * Setup Excel for VBA. -->
<!-- * Know how the macro recorder works. -->
<!-- * Make your first program. -->
<!-- * Have an overview over what VBA can do. -->
<!-- * Recorded you first macro using the macro recorder -->
<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings-1" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Textbook readings<a href="mod-bandit.html#textbook-readings-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 2 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module.</p>
</div>
<div id="the-k-armed-bandit-problem" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> The k-armed bandit problem<a href="mod-bandit.html#the-k-armed-bandit-problem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multi-armed bandits attempt to find the best option among a collection of alternatives by learning through trial and error. The name derives from “one-armed bandit,” a slang term for a slot machine — which is a perfect analogy for how these algorithms work.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bandit"></span>
<img src="img/bandit.png" alt="A 4-armed bandit." width="100%" />
<p class="caption">
Figure 2.1: A 4-armed bandit.
</p>
</div>
<p>Imagine you are facing a wall with <span class="math inline">\(k\)</span> slot machines (see Figure <a href="mod-bandit.html#fig:bandit">2.1</a>), and each one pays out at a different rate. A natural way to figure out how to make the most money (rewards) would be to try each at random for a while (exploration), and start playing the higher paying ones once you have gained some experience (exploitation). That is, from an agent/environment point of view the agent considers a single state at time <span class="math inline">\(t\)</span> and have to choose among <span class="math inline">\(k\)</span> actions given the environment representing the <span class="math inline">\(k\)</span> bandits. Only the rewards from the <span class="math inline">\(k\)</span> bandits are unknown, but the agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. The objective is to find an optimal policy that maximize the total expected reward. Note since the process only have a single state, this is the same as finding an optimal policy <span class="math inline">\(\pi^*(s) = \pi^* = a^*\)</span> that chooses the action with the highest expected reward. Due to uncertainty, there is an exploration vs exploitation dilemma. The agent have one action that seems to be most valuable at a time point, but it is highly likely, at least initially, that there are actions yet to explore that are more valuable.</p>
<!-- For finding the optimal action a learning strategy that balance the exploration vs. exploitation trade-off. -->
<!-- To summarize: -->
<!-- * The agent are faced repeatedly with a choice of $k$ actions. -->
<!-- * After each choice, you receive a reward from a stationary probability distribution. -->
<!-- * Objective is to maximise total reward over some time period, say 100 time steps. -->
<!-- * Each action has an expected or mean reward based on its probability distribution. We shall call thjs the \textit{value} of the action. We do not know these values with certainty. -->
<!-- * Because of this uncertainty, there is always an exploration vs exploitation problem. We always have one action that we deem to be most valuable at any instant, but it is highly likely, at least initially, that there are actions we are yet to explore that are more valuable. -->
<p>Multi-armed bandits can be used in e.g. <a href="https://research.facebook.com/blog/2021/4/auto-placement-of-ad-campaigns-using-multi-armed-bandits/">digital advertising</a>. Suppose you are an advertiser seeking to optimize which ads (<span class="math inline">\(k\)</span> to choose among) to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bandit-choose"></span>
<img src="img/bandit-choose.png" alt="Which ad to choose?" width="100%" />
<p class="caption">
Figure 2.2: Which ad to choose?
</p>
</div>
<p>It is reasonable to assume that each of these ads will have different effects, and some will be more engaging than others. That is, each ad has some theoretical — but unknown — click-through-rate (CTR) that is assumed to not change over time. How do we go about solving which ad we should choose (see Figure <a href="mod-bandit.html#fig:bandit-choose">2.2</a>)?</p>
</div>
<div id="estimating-the-value-of-an-action" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Estimating the value of an action<a href="mod-bandit.html#estimating-the-value-of-an-action" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How can the value of an action be estimated, i.e. the expected reward of an action <span class="math inline">\(q_*(a) = \mathbb{E}[R_t | A_t = a]\)</span>. Assume that at time <span class="math inline">\(t\)</span> action <span class="math inline">\(a\)</span> has been chosen <span class="math inline">\(N_t(a)\)</span> times. Then the estimated action value is
<span class="math display">\[\begin{equation} 
    Q_t(a) = \frac{R_1+R_2+\cdots+R_{N_t(a)}}{N_t(a)},
\end{equation}\]</span>
Storing <span class="math inline">\(Q_t(a)\)</span> this way is cumbersome since memory and computation requirements grow over time. Instead an incremental solution is better. If we assume that <span class="math inline">\(N_t(a) = n-1\)</span> and set <span class="math inline">\(Q_t(a) = Q_n\)</span> then <span class="math inline">\(Q_{n+1}\)</span> becomes:
<span class="math display">\[\begin{align}
  Q_{n+1} &amp;= \frac{1}{n}\sum_{i=1}^{n}R_i \nonumber \\
            &amp;= \frac{1}{n}\left( R_{n} + \sum_{i=1}^{n-1} R_i \right) \nonumber \\
            &amp;= \frac{1}{n}\left( R_{n} + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i \right) \nonumber \\
            &amp;= \frac{1}{n}\left( R_{n} + (n-1)Q_n \right) \nonumber \\
           &amp;= Q_n + \frac{1}{n} \left[R_n - Q_n\right].
\end{align}\]</span>
That os, we can update the estimate of the value of <span class="math inline">\(a\)</span> using the previous estimate, the observed reward and how many times the action has occurred (<span class="math inline">\(n\)</span>).</p>
<p>A greedy approach for selecting the next action is
<span class="math display">\[\begin{equation}
A_t =\arg \max_a Q_t(a).
\end{equation}\]</span>
Here <span class="math inline">\(\arg\max_a\)</span> means the value of <span class="math inline">\(a\)</span> for which <span class="math inline">\(Q_t(a)\)</span> is maximised. A pure greedy approach do not explore other actions. Instead an <span class="math inline">\(\varepsilon\)</span>-greedy ppproach is used in which with probability <span class="math inline">\(\varepsilon\)</span> we take a random draw from all of the actions (choosing each action with equal probability) and hereby providing some exploration.</p>
<!-- \begin{itemize} -->
<!--    \item Simplest action selection rule is to select the action with the highest estimated value. -->
<!--    \item \(\epsilon\)-greedy methods are where the agent selects the greedy option most of the time, and selects a random action with probability \(\epsilon\). -->
<!--    \item Three algorithms are tried: one with \(e\)=0 (pure greedy), one with \(e\)=0.01 and another with \(e\)=0.1 -->
<!--    \item Greedy method gets stuck performing sub-optimal actions. -->
<!--    \item \(e\)=0.1 explores more and usually finds the optimal action earlier, but never selects it more that 91\% of the time. -->
<!--    \item \(e\)=0.01 method improves more slowly, but eventually performs better than the e=0.1 method on both performance measures. -->
<!--    \item It is possible to reduce \(e\) over time to try to get the best of both high and low values. -->
<!-- \end{itemize} -->
<!-- Note this is the general formula  -->
<!-- \begin{equation} -->
<!-- NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate \right] -->
<!-- \end{equation} -->
<p>Let us try to implement the algorithm using an R6 agent and environment class. First we define the agent that do actions based on an <span class="math inline">\(\epsilon\)</span>-greedy strategy, stores the estimated <span class="math inline">\(Q\)</span> values and the number of times an action has been chosen:</p>
<p>Next, the environment generating rewards. The true mean reward <span class="math inline">\(q_*(a)\)</span> of an action were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. The observed reward was then generated using a normal distribution with mean <span class="math inline">\(q_*(a)\)</span> and variance 1:</p>
<p>To test the RL algorithm we use a function returning two plots that compare the performance:</p>
<p>We test the performance using 2000 runs over 1000 time steps.</p>
<p><img src="02_bandit_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /><img src="02_bandit_files/figure-html/unnamed-chunk-6-2.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="incremential-value-estimation" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Incremential value estimation<a href="mod-bandit.html#incremential-value-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="the-role-of-the-step-size" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> The role of the step-size<a href="mod-bandit.html#the-role-of-the-step-size" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="optimistic-initial-values" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Optimistic initial values<a href="mod-bandit.html#optimistic-initial-values" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="sec-bandit-ex" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Exercises<a href="mod-bandit.html#sec-bandit-ex" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Some of the solutions to each exercise can be seen by pressing the button at each question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="exercise---self-play" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Exercise - Self-Play<a href="mod-bandit.html#exercise---self-play" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="c4ZGLrADqBCDcUASLG5T" class="modal fade bs-example-modal-lg" tabindex="-1" role="dialog" aria-labelledby="c4ZGLrADqBCDcUASLG5T-title">
<div class="modal-dialog modal-lg" role="document">
<div class="modal-content">
<div class="modal-header">
<button type="button" class="close" data-dismiss="modal" aria-label="Close">
<span aria-hidden="true">×</span>
</button>
<h4 class="modal-title" id="c4ZGLrADqBCDcUASLG5T-title">
Solution
</h4>
</div>
<div class="modal-body">
<p>
If the exploration parameter is non-zero, the algorithm will continue to adapt until it reaches an equilibrium (either fixed or cyclical).
</p>
</div>
<div class="modal-footer">
<button class="btn btn-default" data-dismiss="modal">
Close
</button>
</div>
</div>
</div>
</div>
<button class="btn btn-default btn-xs" style="float:right" data-toggle="modal" data-target="#c4ZGLrADqBCDcUASLG5T">
Solution
</button>
<p>Consider Tic-Tac-Toe and assume that instead of an RL player against a random opponent, the reinforcement learning algorithm described above
played against itself. What do you think would happen in this case? Would it learn a different way of playing?</p>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-rl-intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-r-setup.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/02_bandit.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
