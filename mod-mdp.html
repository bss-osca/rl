<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 3 Markov decision processes | Reinforcement Learning for Business (RL)</title>
  <meta name="description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 3 Markov decision processes | Reinforcement Learning for Business (RL)" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bss-osca.github.io/rl/" />
  <meta property="og:image" content="https://bss-osca.github.io/rl//img/logo.png" />
  <meta property="og:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="github-repo" content="bss-osca/rl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 3 Markov decision processes | Reinforcement Learning for Business (RL)" />
  
  <meta name="twitter:description" content="Course notes for ‘Reinforcement Learning for Business’" />
  <meta name="twitter:image" content="https://bss-osca.github.io/rl//img/logo.png" />

<meta name="author" content="Lars Relund Nielsen" />


<meta name="date" content="2022-05-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/favicon.ico" type="image/x-icon" />
<link rel="prev" href="mod-bandit.html"/>
<link rel="next" href="mod-r-setup.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.21/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<html>
<head>

<script id="code-folding-options" type="application/json">
  {"initial-state": "hide"}
</script>
   
<script>
  $(document).ready(function() {

  // Section anchors
  $('.section h1, .section h2, .section h3, .section h4, .section h5').each(function() {
    anchor = '#' + $(this).parent().attr('id');
    $(this).addClass("hasAnchor").prepend('<a href="' + anchor + '" class="anchor"></a>');
  });
});

// code folding
document.addEventListener("DOMContentLoaded", function() {
  const languages = ['r', 'python', 'bash', 'sql', 'cpp', 'stan', 'julia', 'foldable'];
  const options = JSON.parse(document.getElementById("code-folding-options").text);
  const show = options["initial-state"] !== "hide";
  Array.from(document.querySelectorAll("pre.sourceCode")).map(function(pre) {
    const classList = pre.classList;
    if (languages.some(x => classList.contains(x))) {
      const div = pre.parentElement;
      const state = show || classList.contains("fold-show") && !classList.contains("fold-hide") ? " open" : "";
      div.outerHTML = `<details${state}><summary></summary>${div.outerHTML}</details>`;
    }
  });
});
</script>

<script src="https://hypothes.is/embed.js" async></script>

<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

</head>
</html>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="img/logo.png"></a></li>

<li class="divider"></li>
<li><a href="index.html#mod-intro">About the course notes<span></span></a>
<ul>
<li><a href="index.html#learning-outcomes">Learning outcomes<span></span></a></li>
<li><a href="index.html#purpose-of-the-course">Purpose of the course<span></span></a></li>
<li><a href="index.html#learning-goals-of-the-course">Learning goals of the course<span></span></a></li>
<li><a href="index.html#reinforcement-learning-textbook">Reinforcement learning textbook<span></span></a></li>
<li><a href="index.html#course-organization">Course organization<span></span></a></li>
<li><a href="index.html#programming-software">Programming software<span></span></a></li>
<li><a href="index.html#ack">Acknowledgements and license<span></span></a></li>
<li><a href="index.html#sec-intro-ex">Exercises<span></span></a>
<ul>
<li><a href="index.html#sec-intro-ex-annotate">Exercise - How to annotate<span></span></a></li>
<li><a href="index.html#sec-intro-ex-templates">Exercise - Templates<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Introduction to RL<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html"><i class="fa fa-check"></i><b>1</b> An introduction to RL<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#mod-rl-intro-lo"><i class="fa fa-check"></i><b>1.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#textbook-readings"><i class="fa fa-check"></i><b>1.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#what-is-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> What is reinforcement learning<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-business-analytics"><i class="fa fa-check"></i><b>1.4</b> RL and Business Analytics<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-different-research-deciplines"><i class="fa fa-check"></i><b>1.5</b> RL in different research deciplines<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-and-machine-learning"><i class="fa fa-check"></i><b>1.6</b> RL and machine learning<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#the-rl-data-stream"><i class="fa fa-check"></i><b>1.7</b> The RL data-stream<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#states-actions-rewards-and-policies"><i class="fa fa-check"></i><b>1.8</b> States, actions, rewards and policies<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exploitation-vs-exploration"><i class="fa fa-check"></i><b>1.9</b> Exploitation vs Exploration<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-in-action-tic-tac-toe"><i class="fa fa-check"></i><b>1.10</b> RL in action (Tic-Tac-Toe)<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#players-and-learning-to-play"><i class="fa fa-check"></i><b>1.10.1</b> Players and learning to play<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#gameplay"><i class="fa fa-check"></i><b>1.10.2</b> Gameplay<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#rl-intro-tic-learn"><i class="fa fa-check"></i><b>1.10.3</b> Learning by a sequence of games<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#sec-rl-intro-ex"><i class="fa fa-check"></i><b>1.11</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#ex-r-intro-self"><i class="fa fa-check"></i><b>1.11.1</b> Exercise - Self-Play<span></span></a></li>
<li class="chapter" data-level="1.11.2" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---symmetries"><i class="fa fa-check"></i><b>1.11.2</b> Exercise - Symmetries<span></span></a></li>
<li class="chapter" data-level="1.11.3" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---greedy-play"><i class="fa fa-check"></i><b>1.11.3</b> Exercise - Greedy Play<span></span></a></li>
<li class="chapter" data-level="1.11.4" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---learning-from-exploration"><i class="fa fa-check"></i><b>1.11.4</b> Exercise - Learning from Exploration<span></span></a></li>
<li class="chapter" data-level="1.11.5" data-path="mod-rl-intro.html"><a href="mod-rl-intro.html#exercise---other-improvements"><i class="fa fa-check"></i><b>1.11.5</b> Exercise - Other Improvements<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Tabular methods<span></span></b></span></li>
<li class="chapter" data-level="2" data-path="mod-bandit.html"><a href="mod-bandit.html"><i class="fa fa-check"></i><b>2</b> Multi-armed bandits<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="mod-bandit.html"><a href="mod-bandit.html#learning-outcomes-1"><i class="fa fa-check"></i><b>2.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="mod-bandit.html"><a href="mod-bandit.html#textbook-readings-1"><i class="fa fa-check"></i><b>2.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="mod-bandit.html"><a href="mod-bandit.html#the-k-armed-bandit-problem"><i class="fa fa-check"></i><b>2.3</b> The k-armed bandit problem<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="mod-bandit.html"><a href="mod-bandit.html#estimating-the-value-of-an-action"><i class="fa fa-check"></i><b>2.4</b> Estimating the value of an action<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="mod-bandit.html"><a href="mod-bandit.html#the-role-of-the-step-size"><i class="fa fa-check"></i><b>2.5</b> The role of the step-size<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="mod-bandit.html"><a href="mod-bandit.html#optimistic-initial-values"><i class="fa fa-check"></i><b>2.6</b> Optimistic initial values<span></span></a></li>
<li class="chapter" data-level="2.7" data-path="mod-bandit.html"><a href="mod-bandit.html#upper-confidence-bound-action-selection"><i class="fa fa-check"></i><b>2.7</b> Upper-Confidence Bound Action Selection<span></span></a></li>
<li class="chapter" data-level="2.8" data-path="mod-bandit.html"><a href="mod-bandit.html#sec-bandit-ex"><i class="fa fa-check"></i><b>2.8</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="mod-bandit.html"><a href="mod-bandit.html#exercise---advertising"><i class="fa fa-check"></i><b>2.8.1</b> Exercise - Advertising<span></span></a></li>
<li class="chapter" data-level="2.8.2" data-path="mod-bandit.html"><a href="mod-bandit.html#exercise---a-coin-game"><i class="fa fa-check"></i><b>2.8.2</b> Exercise - A coin game<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mod-mdp.html"><a href="mod-mdp.html"><i class="fa fa-check"></i><b>3</b> Markov decision processes<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="mod-mdp.html"><a href="mod-mdp.html#learning-outcomes-2"><i class="fa fa-check"></i><b>3.1</b> Learning outcomes<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="mod-mdp.html"><a href="mod-mdp.html#textbook-readings-2"><i class="fa fa-check"></i><b>3.2</b> Textbook readings<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="mod-mdp.html"><a href="mod-mdp.html#a-mdp-as-a-model-for-the-agent-environment"><i class="fa fa-check"></i><b>3.3</b> A MDP as a model for the agent-environment<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="mod-mdp.html"><a href="mod-mdp.html#rewards-and-the-objective-function-goal"><i class="fa fa-check"></i><b>3.4</b> Rewards and the objective function (goal)<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="mod-mdp.html"><a href="mod-mdp.html#policies-and-value-functions"><i class="fa fa-check"></i><b>3.5</b> Policies and value functions<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="mod-mdp.html"><a href="mod-mdp.html#optimal-policies-and-value-functions"><i class="fa fa-check"></i><b>3.6</b> Optimal policies and value functions<span></span></a></li>
<li class="chapter" data-level="3.7" data-path="mod-mdp.html"><a href="mod-mdp.html#optimality-vs-approximation"><i class="fa fa-check"></i><b>3.7</b> Optimality vs approximation<span></span></a></li>
<li class="chapter" data-level="3.8" data-path="mod-mdp.html"><a href="mod-mdp.html#semi-mdps-non-fixed-time-length"><i class="fa fa-check"></i><b>3.8</b> semi-MDPs (non-fixed time length)<span></span></a></li>
<li class="chapter" data-level="3.9" data-path="mod-mdp.html"><a href="mod-mdp.html#sec-mdp-ex"><i class="fa fa-check"></i><b>3.9</b> Exercises<span></span></a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="mod-mdp.html"><a href="mod-mdp.html#exercise--"><i class="fa fa-check"></i><b>3.9.1</b> Exercise -<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="mod-r-setup.html"><a href="mod-r-setup.html"><i class="fa fa-check"></i><b>A</b> Setting up R<span></span></a></li>
<li class="chapter" data-level="B" data-path="groups.html"><a href="groups.html"><i class="fa fa-check"></i><b>B</b> Working in groups<span></span></a></li>
<li class="chapter" data-level="C" data-path="coding-convention.html"><a href="coding-convention.html"><i class="fa fa-check"></i><b>C</b> Coding/naming convention<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="coding-convention.html"><a href="coding-convention.html#commenting-your-code"><i class="fa fa-check"></i><b>C.1</b> Commenting your code<span></span></a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="annotate.html"><a href="annotate.html"><i class="fa fa-check"></i><b>D</b> Annotate the course notes<span></span></a></li>
<li class="chapter" data-level="E" data-path="help.html"><a href="help.html"><i class="fa fa-check"></i><b>E</b> Getting help<span></span></a></li>
<li class="chapter" data-level="F" data-path="mod-lg-course.html"><a href="mod-lg-course.html"><i class="fa fa-check"></i><b>F</b> Learning goals<span></span></a></li>
<li class="chapter" data-level="G" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i><b>G</b> Colophon<span></span></a></li>
<li class="divider"></li>
<li><center>
  <a rel="license" href="./index.html#ack">
    License: CC BY-NC-SA<br>
    <i class = "fab fa-creative-commons fa-2x"></i>
    <i class = "fab fa-creative-commons-by fa-2x"></i>
    <i class = "fab fa-creative-commons-nc fa-2x"></i>
    <i class = "fab fa-creative-commons-sa fa-2x"></i>
  </a>
</li></center>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning for Business (RL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mod-mdp" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Module 3</span> Markov decision processes<a href="mod-mdp.html#mod-mdp" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This module gives an introduction to Markov decision processes (MDPs) with a finite number of states and actions. This gives us a full model of a sequential decision problem. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also what will be the next state, and hence future rewards. Thus MDPs involve delayed reward and the need to consider the trade-off between immediate and delayed reward. MDPs are a mathematically idealized form of the RL problem where a full description is known and the optimal policy can be found. Often in a RL problem some parts of this description is unknown and we hereby have to estimate the best policy by learning. For example, in the bandit problem the rewards was unknown.</p>
<div id="learning-outcomes-2" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Learning outcomes<a href="mod-mdp.html#learning-outcomes-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>By the end of this module, you are expected to:</p>
<!-- Understand Markov Decision Processes (MDP) -->
<!-- Describe how the dynamics of an MDP are defined -->
<!-- Understand the graphical representation of a Markov Decision Process -->
<!-- Explain how many diverse processes can be written in terms of the MDP framework -->
<!-- Describe how rewards relate to the goal of an agent -->
<!-- Understand episodes and identify episodic tasks -->
<!-- Formulate returns for continuing tasks using discounting -->
<!-- Describe how returns at successive time steps are related to each other -->
<!-- Understand when to formalize a task as episodic or continuing -->
<!-- The learning outcomes relate to the [overall learning goals](#mod-lg-course) number 2 and 4 of the course. -->
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</div>
<div id="textbook-readings-2" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Textbook readings<a href="mod-mdp.html#textbook-readings-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this week, you will need to read Chapter 3-3.8 in <span class="citation">Sutton and Barto (<a href="#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module. You do not have to put to much focus on the examples. A summary of the book notation can be seen <a href="sutton-notation">here</a>.</p>
</div>
<div id="a-mdp-as-a-model-for-the-agent-environment" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> A MDP as a model for the agent-environment<a href="mod-mdp.html#a-mdp-as-a-model-for-the-agent-environment" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us recall the RL problem which considers an agent in an environment:</p>
<ul>
<li>Agent: The one who takes the action (computer, robot, decision maker), i.e. the decision making component of a system. Everything else is the environment. A general rule is that anything that the agent does not have absolute control over forms part of the environment.</li>
<li>Environment: The system/world where observations and rewards are found.</li>
</ul>
<p>At time step <span class="math inline">\(t\)</span> the agent is in state <span class="math inline">\(S_t\)</span> and takes action <span class="math inline">\(A_{t}\)</span> and observe the new state <span class="math inline">\(S_{t+1}\)</span> and reward <span class="math inline">\(R_{t+1}\)</span>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="03_mdp_files/figure-html/unnamed-chunk-4-1.png" alt="Agent-environment representation." width="672" />
<p class="caption">
Figure 1.3: Agent-environment representation.
</p>
</div>
<p>Note we here assume that the <em>Markov property</em> is satisfied and the current state holds just as much information as the history of observations. That is, given the present state the future is independent of the past:</p>
<p><span class="math display">\[\Pr(S_{t+1} | S_t, A_t) = \Pr(S_{t+1} | S_1,...,S_t, A_t).\]</span>
That is, the probability of seeing some next state <span class="math inline">\(S_{t+1}\)</span> given the current state is exactly equal to the probability of that next state given the entire history of states.</p>
<p>A Markov decision process (MDP) is a mathematical model that for each time-step <span class="math inline">\(t\)</span> have defined states <span class="math inline">\(S_t \in \mathcal{S}\)</span>, possible actions <span class="math inline">\(A_t \in \mathcal{A}(s)\)</span> given a state and rewards <span class="math inline">\(R_t \in \mathcal{R} \subset \mathbb{R}\)</span>. Consider the example in Figure <a href="mod-mdp.html#fig:hgf1">3.1</a>. Each time-step have five states <span class="math inline">\(\mathcal{S} = \{1,2,3,4,5\}\)</span>. Assume that the agent start in state <span class="math inline">\(s_0\)</span> with two actions to choose among <span class="math inline">\(\mathcal{A}(s_0) = \{a_1, a_2\}\)</span>. After choosing <span class="math inline">\(a_1\)</span> a transition to <span class="math inline">\(s_1\)</span> happens with reward <span class="math inline">\(R_1 = r_1\)</span>. Next, in state <span class="math inline">\(s_1\)</span> the agent chooses action <span class="math inline">\(a_2\)</span> and a transition to <span class="math inline">\(s_2\)</span> happens with reward <span class="math inline">\(r_2\)</span>. This continues as time evolves.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hgf1"></span>
<img src="03_mdp_files/figure-html/hgf1-1.png" alt="State-expanded hypergraph" width="100%" />
<p class="caption">
Figure 3.1: State-expanded hypergraph
</p>
</div>
<p>In a <em>finite</em> MDP, the sets of states, actions, and rewards all have a finite number of elements. In this case, the random variables have well defined discrete probability distributions dependent only on the preceding state and action which defines the  of the system:
<span class="math display">\[\begin{equation}
    p(s&#39;, r | s, a) = \Pr(S_t = s&#39;, R_t = r | S_{t-1} = s, A_{t-1} = a),
\end{equation}\]</span>
which can be used to find the <em>transition probabilities</em>:
<span class="math display">\[\begin{equation}
    p(s&#39; | s, a) = \Pr(S_t = s&#39;| S_{t-1} = s, A_{t-1}=A) = \sum_{r \in \mathcal{R}} p(s&#39;, r | s, a), 
\end{equation}\]</span>
and the <em>expected reward</em>:
<span class="math display">\[\begin{equation}
    r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s&#39; \in \mathcal{S}} p(s&#39;, r | s, a).
\end{equation}\]</span></p>
<p>That is, to define an MDP the following are needed:</p>
<ul>
<li>All states and actions are assumed known.</li>
<li>A finite number of states and actions. That is, we can store values using tabular methods.</li>
<li>The transition probabilities and expected rewards are given.</li>
</ul>
<p>Moreover, for now a <em>stationary</em> MDP is considered, i.e. at each time-step all states, actions and probabilities are the same and hence the time index can be dropped.</p>
</div>
<div id="rewards-and-the-objective-function-goal" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Rewards and the objective function (goal)<a href="mod-mdp.html#rewards-and-the-objective-function-goal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The  is a central assumption in reinforcement learning:</p>
<blockquote>
<p>All of what we mean by goals and purposes can be well thought of as the maximisation of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<p>This assumption can be questioned but it this course we assume it holds. The reward signal is our way of communicating to the agent what we want to achieve not how we want to achieve it.</p>
<p>The return <span class="math inline">\(G_t\)</span> can be defined as the sum of future rewards; however, if the time horizon is infinite the return is also infinite. Hence we use a <em>discount factor</em> <span class="math inline">\(0 \leq \gamma \leq 1\)</span> and define the return as</p>
<p><span class="math display">\[\begin{equation}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} 
\end{equation}\]</span></p>
<p>Discounting is important since it allows us to work with finite returns because if <span class="math inline">\(\gamma &lt; 1\)</span> and the reward is bounded by a number <span class="math inline">\(B\)</span> then the return is always finite:</p>
<p><span class="math display">\[\begin{equation}
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \leq B \sum_{k=0}^{\infty} \gamma^k  = B \frac{1}{1 - \gamma}
\end{equation}\]</span></p>
<p>Note gamma close to one put weight on future rewards while a gamma close to zero put weight on present rewards. Moreover, an infinite time-horizon is assumed.</p>
<p>An MDP modelling a problem over a finite time-horizon can be transformed into an infinite time-horizon using an <em>absorbing state</em> with transitions only to itself and a reward of zero. This breaks the agent-environment interaction into <em>episodes</em> (e.g playing a board game). Each episode ends in the absorbing state, possibly with a different reward. Each starts independently of the last, with some distribution of starting states. Sequences of interaction without an absorbing state are called <em>continuing tasks</em>.</p>
<p>The <em>objective function</em> is to choose actions such that the expected return is maximized. Let us formalize this mathematically in the next sections.</p>
</div>
<div id="policies-and-value-functions" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Policies and value functions<a href="mod-mdp.html#policies-and-value-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <em>policy</em> <span class="math inline">\(\pi\)</span> is a distribution over actions, given some state:</p>
<p><span class="math display">\[\pi(a | s) = \Pr(A_t = a | S_t = s).\]</span>
Since the MDP is stationary the policy is time-independent, i.e. given a state, we choose the same action no matter the time-step. If <span class="math inline">\(\pi(a | s) = 1\)</span> for each state, i.e. an action is chosen with probability one always then the policy is called <em>deterministic</em>. Otherwise a policy is called <em>stochastic</em>.</p>
<p>Given a policy we can define some value functions. The <em>state-value function</em> <span class="math inline">\(v_\pi(s)\)</span> denote the expected return starting from state <span class="math inline">\(s\)</span> when following the policy <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
\begin{align}
  v_\pi(s) &amp;= \mathbb{E}_\pi[G_t | S_t = s] \\
  &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s].
\end{align}
\]</span>
Note the last equal sign comes from <span class="math inline">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>.</p>
<p>The <em>action-value function</em> <span class="math inline">\(q_\pi(s, a)\)</span>, denote the expected return starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span> and from thereon following policy <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
\begin{align}
  q_\pi(s, a) &amp;= \mathbb{E}_\pi[G_t | S_t = s, A_t = a] \\
  &amp;= \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a].
\end{align}
\]</span></p>
<p>This action-value, also known as “q-value”, is very important, as it tells us directly what action to pick in a particular state. Given the definition of q-values, the state-value function is an average over the q-values of all actions we could take in that state:</p>
<p><span class="math display" id="eq:vq">\[\begin{equation}
v_\pi(s) = \sum_{a \in \mathcal{A}}\pi(a|s)q_\pi(s, a)
\tag{3.1}
\end{equation}\]</span></p>
<p>A q-value (action-value) is equal to the expected reward <span class="math inline">\(r(s,a)\)</span> that we get from choosing action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, plus a discounted amount of the average state-value of all the future states:</p>
<p><span class="math display">\[q_\pi(s, a) = r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_\pi(s&#39;)\]</span></p>
<p>Joining the equations, the state-value of a particular state <span class="math inline">\(s\)</span> now becomes the sum of weighted state-values of all possible
subsequent states <span class="math inline">\(s&#39;\)</span>, where the weights are the policy probabilities:</p>
<p><span class="math display">\[
\begin{align}
  v_\pi(s) &amp;= \sum_{a \in \mathcal{A}}\pi(a | s)q_\pi(s, a) \\
  &amp;= \sum_{a \in \mathcal{A}}\pi(a | s)\left( r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_\pi(s&#39;)\right),
\end{align}
\]</span>
which is known as the <em>Bellman equation</em>.
<!-- in exactly the same way we can define a q-value as a weighted sum of the -->
<!-- q-values of all states we could reach given we pick the action of the q-value: --></p>
<!-- $$ -->
<!-- \begin{align} -->
<!-- q_\pi(s, a) &= \mathcal{R}_s^a + \gamma\sum_{s' \in \mathcal{S}} P_{ss'}^a v_\pi(s') \\ -->
<!-- &= \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a'|s')q_\pi(s',a') -->
<!-- \end{align} -->
<!-- $$ -->
</div>
<div id="optimal-policies-and-value-functions" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Optimal policies and value functions<a href="mod-mdp.html#optimal-policies-and-value-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The objective function of an MDP can now be stated mathematically which is to find an optimal policy <span class="math inline">\(\pi_*\)</span> with state-value function:</p>
<p><span class="math display">\[v_*(s) = \max_\pi v_\pi(s),\]</span>
That is, a policy <span class="math inline">\(\pi &#39;\)</span> is defined as better than policy <span class="math inline">\(\pi\)</span> if its expected return is higher for all states. If the MDP has the right properties (details are not given here), there exists an optimal deterministic policy <span class="math inline">\(\pi_*\)</span> that is better than or equal to all other policies. For all such optimal policies (there may be more than one), we only need to find one optimal policy that have the <em>optimal state-value function</em> <span class="math inline">\(v_*\)</span>.</p>
<p>We may rewrite <span class="math inline">\(v_*(s)\)</span> using Eq. <a href="mod-mdp.html#eq:vq">(3.1)</a>:</p>
<p><span class="math display">\[\begin{align}
  v_*(s) &amp;= \max_\pi v_\pi(s) \\
         &amp;= \max_\pi \sum_{a \in \mathcal{A}}\pi(a|s)q_\pi(s, a) \\
         &amp;= \max_\pi \max_a q_\pi(s, a)\qquad \text{(set $\pi(a|s) = 1$ where $q_\pi$ is largest)} \\
         &amp;= \max_a \max_\pi q_\pi(s, a) \\
         &amp;= \max_a q_*(s, a), \\
\end{align}\]</span></p>
<p>where the <em>optimal q-value/action-value function</em> <span class="math inline">\(q_*\)</span> is:</p>
<p><span class="math display">\[\begin{align}
q_*(s, a) &amp;= \max_\pi q_\pi(s, a) \\
          &amp;= r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_*(s&#39;) \\
          &amp;= r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) \max_{a&#39;} q_*(s&#39;, a&#39;).
\end{align}\]</span></p>
<p>This is the the <em>Bellman optimality equation</em> for <span class="math inline">\(q_*\)</span> and the optimal policy is:</p>
<p><span class="math display">\[
\pi_*(a | s) =
\begin{cases}
1 \text{ if } a = \arg\max_{a \in \mathcal{A}} q_*(s, a) \\
0 \text { otherwise.}
\end{cases}
\]</span></p>
<p>Similar we can write the <em>Bellman optimality equation</em> for <span class="math inline">\(v_*\)</span>:</p>
<p><span class="math display">\[\begin{align}
  v_*(s) &amp;= \max_a q_*(s, a) \\
         &amp;= \max_a r(s,a) + \gamma\sum_{s&#39; \in \mathcal{S}} p(s&#39; | s, a) v_*(s&#39;)
\end{align}\]</span></p>
<p>Using <span class="math inline">\(v_*\)</span> the optimal expected long term return is turned into a quantity that is immediately available for each state. On the other hand if we do not store <span class="math inline">\(q_*\)</span>, we can find <span class="math inline">\(v_*\)</span> by a one-step-ahead search, acting greedily.</p>
</div>
<div id="optimality-vs-approximation" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Optimality vs approximation<a href="mod-mdp.html#optimality-vs-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
Fully solving the Bellman optimality equations can be hugely expensive, especially if the number of states is huge, as is the case with most interesting problems.
Solving the Bellman optimality equation is akin to exhaustive search. We play out  possible scenario until the terminal state and collect their expected reward. Our policy then defines the action that maximises this expected reward.
<p>In the continuous case the Bellman optimality equation is unsolvable as the recursion on the next state’s value function would never end.
\end{itemize}</p>
<p>curse of dimdasf</p>
</div>
<div id="semi-mdps-non-fixed-time-length" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> semi-MDPs (non-fixed time length)<a href="mod-mdp.html#semi-mdps-non-fixed-time-length" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="sec-mdp-ex" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Exercises<a href="mod-mdp.html#sec-mdp-ex" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="help.html#help">help page</a>. Some of the solutions to each exercise can be seen by pressing the button at each question. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<div id="exercise--" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Exercise -<a href="mod-mdp.html#exercise--" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Sutton18" class="csl-entry">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mod-bandit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mod-r-setup.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bss-osca/rl/edit/master/book/03_mdp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
