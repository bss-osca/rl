<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; On-policy prediction with approximation – Reinforcement Learning for Business (RL)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13_approx-control.html" rel="next">
<link href="./11_td-control.html" rel="prev">
<link href="./assets/favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-0dd2bd5de344125cf763a379ddc3eb04.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="assets/style.css">
<meta property="og:title" content="12&nbsp; On-policy prediction with approximation – Reinforcement Learning for Business (RL)">
<meta property="og:description" content="Course notes for the ‘Reinforcement Learning for Business’ course.">
<meta property="og:image" content="https://github.com/bss-osca/rl/raw/master/book/img/logo.png">
<meta property="og:site_name" content="Reinforcement Learning for Business (RL)">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./12_approx-pred.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-policy prediction with approximation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/logo.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="./img/logo.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Reinforcement Learning for Business (RL)</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/bss-osca/rl/tree/master/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Introduction</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About the course notes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_rl-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">An introduction to RL</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_rl-in-action.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">RL in action</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">An introduction to Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Tabular methods</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_bandit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Multi-armed bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_mdp-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markov decision processes (MDPs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_mdp-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Policies and value functions for MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Dynamic programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_mc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo methods for prediction and control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_td-pred.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Temporal difference methods for prediction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_td-control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Temporal difference methods for control</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_approx-pred.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-policy prediction with approximation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_approx-control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Policy Control with Approximation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99_1_appdx.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99_2_appdx.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Getting help</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#learning-outcomes" id="toc-learning-outcomes" class="nav-link active" data-scroll-target="#learning-outcomes"><span class="header-section-number">12.1</span> Learning outcomes</a></li>
  <li><a href="#textbook-readings" id="toc-textbook-readings" class="nav-link" data-scroll-target="#textbook-readings"><span class="header-section-number">12.2</span> Textbook readings</a></li>
  <li><a href="#the-prediction-objective" id="toc-the-prediction-objective" class="nav-link" data-scroll-target="#the-prediction-objective"><span class="header-section-number">12.3</span> The prediction objective</a></li>
  <li><a href="#stochastic-gradient-and-semi-gradient-methods" id="toc-stochastic-gradient-and-semi-gradient-methods" class="nav-link" data-scroll-target="#stochastic-gradient-and-semi-gradient-methods"><span class="header-section-number">12.4</span> Stochastic-gradient and semi-gradient methods</a></li>
  <li><a href="#linear-methods" id="toc-linear-methods" class="nav-link" data-scroll-target="#linear-methods"><span class="header-section-number">12.5</span> Linear methods</a></li>
  <li><a href="#a-random-walk" id="toc-a-random-walk" class="nav-link" data-scroll-target="#a-random-walk"><span class="header-section-number">12.6</span> A random walk</a></li>
  <li><a href="#feature-construction-for-linear-models" id="toc-feature-construction-for-linear-models" class="nav-link" data-scroll-target="#feature-construction-for-linear-models"><span class="header-section-number">12.7</span> Feature construction for linear models</a></li>
  <li><a href="#selecting-step-size-parameters-manually" id="toc-selecting-step-size-parameters-manually" class="nav-link" data-scroll-target="#selecting-step-size-parameters-manually"><span class="header-section-number">12.8</span> Selecting step-size parameters manually</a></li>
  <li><a href="#nonlinear-function-approximation-artificial-neural-networks" id="toc-nonlinear-function-approximation-artificial-neural-networks" class="nav-link" data-scroll-target="#nonlinear-function-approximation-artificial-neural-networks"><span class="header-section-number">12.9</span> Nonlinear function approximation: artificial neural networks</a></li>
  <li><a href="#least-squares-temporal-difference-learning" id="toc-least-squares-temporal-difference-learning" class="nav-link" data-scroll-target="#least-squares-temporal-difference-learning"><span class="header-section-number">12.10</span> Least-squares temporal-difference learning</a></li>
  <li><a href="#memory-based-function-approximation" id="toc-memory-based-function-approximation" class="nav-link" data-scroll-target="#memory-based-function-approximation"><span class="header-section-number">12.11</span> Memory-based function approximation</a></li>
  <li><a href="#on-policy-learning-with-interestemphasis" id="toc-on-policy-learning-with-interestemphasis" class="nav-link" data-scroll-target="#on-policy-learning-with-interestemphasis"><span class="header-section-number">12.12</span> On-policy learning with interest/emphasis</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">12.13</span> Summary</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">12.14</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/bss-osca/rl/edit/master/book/12_approx-pred.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bss-osca/rl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/bss-osca/rl/blob/master/book/12_approx-pred.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-approx-pred" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-policy prediction with approximation</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Tabular methods assume we can store a separate value for each state. In large or continuous state spaces, this is impossible. We therefore approximate the value function using a parametrised model <span class="math inline">\(\hat v(s;\mathbf w)\)</span>. That is, we approximate the value function using a function with parameters <span class="math inline">\(\mathbf w \in \mathbb {R} ^d\)</span>. In this module, a <em>on-policy</em> policy <span class="math inline">\(\pi\)</span> is assumed (we evaluate the value of the same policy we use to generate experience).</p>
<p>Most function approximation techniques are examples of <em>supervised learning</em> that require parameters <span class="math inline">\(\textbf{w}\)</span> to be found using training examples. In our case, we update the value approximation online (our estimate of <span class="math inline">\(\textbf{w}\)</span>) when new training arrive, i.e. <span class="math inline">\(s \rightarrowtail u\)</span> where <span class="math inline">\(u\)</span> is the update of the value function we’d like to make at state <span class="math inline">\(s\)</span>.</p>
<section id="learning-outcomes" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="learning-outcomes"><span class="header-section-number">12.1</span> Learning outcomes</h2>
<p>After studying this chapter, you should be able to:</p>
<ul>
<li>Explain why function approximation is needed beyond tabular RL and define the prediction problem.</li>
<li>Write the <em>mean-squared value error</em> objective and derive <em>semi-gradient</em> updates.</li>
<li>Implement <em>Gradient Monte Carlo</em> and <em>semi-gradient TD(0)</em> for value prediction with function approximation.</li>
<li>Compare and solve algorithms for linear function approximation.</li>
<li>Motivate and construct feature representations (polynomial/Fourier basis, tile coding) and discuss their trade-offs.</li>
<li>Explain causes of instability (e.g., step-size sensitivity).</li>
<li>Describe other methods for function approximation, such as memory-based and interest/emphasis.</li>
</ul>
<!-- SOLO increasing: identify · memorise · name · do simple procedure · collect data · -->
<!-- enumerate · describe · interpret · formulate · list · paraphrase · combine · do -->
<!-- algorithms · compare · contrast · explain causes · analyse · relate · derive · -->
<!-- evaluate · apply · argue · theorise · generalise · hypothesise · solve · reflect -->
</section>
<section id="textbook-readings" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="textbook-readings"><span class="header-section-number">12.2</span> Textbook readings</h2>
<p>For this module, you will need to read Chapter 9-9.11 in <span class="citation" data-cites="Sutton18">Sutton and Barto (<a href="references.html#ref-Sutton18" role="doc-biblioref">2018</a>)</span>. Read it before continuing this module. A summary of the book notation can be seen <a href="https://bss-osca.github.io/rl/misc/sutton-notation.pdf">here</a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
Slides for this module can be seen
<a href="https://bss-osca.github.io/rl/slides/12_approx-pred-slides.html" target="_blank">here.</a>
You do not have to look at them before the lecture!
</div>
</div>
</div>
</section>
<section id="the-prediction-objective" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="the-prediction-objective"><span class="header-section-number">12.3</span> The prediction objective</h2>
<p>The <em>mean-squared value error (MSVE)</em> is often used as an objective for prediction: <span class="math display">\[
e(\mathbf w) = \overline{VE}(\mathbf w) = \sum_s \mu(s)\,\big[v_\pi(s) - \hat v(s;\mathbf w)\big]^2.
\]</span></p>
<p>Here <span class="math inline">\(\mu(s)\)</span> denotes a non-negative weight of state <span class="math inline">\(s\)</span> that indicates how much we care about precision in state <span class="math inline">\(s\)</span>. Often, we focus on precision in states visited more often than others. That is, <span class="math inline">\(\mu(s)\)</span> denote the probability of visiting <span class="math inline">\(s\)</span> (<span class="math inline">\(\sum_s \mu(s) = 1\)</span>). If <span class="math inline">\(\mu(s)\)</span> is unknown we store the number of times <span class="math inline">\(s\)</span> have been visited (including if <span class="math inline">\(s\)</span> have been used as starting state for episodic tasks) and <span class="math inline">\(\mu(s)\)</span> becomes the fraction of time spent in <span class="math inline">\(s\)</span>.</p>
<p>The square root <span class="math inline">\(\sqrt{e}\)</span> of the MSVE gives a rough measure of how much the approximate values differ from the true values and is often used in plots (often denoted <em>RMSVE</em> or just <em>RMS</em>). Note, since <span class="math inline">\(|\mathbf w| \ll |{\cal S}|\)</span>, adjusting <span class="math inline">\(\mathbf{w}\)</span> to reduce error at visited states can increase error elsewhere.</p>
<p>It is not completely clear that <span class="math inline">\(e\)</span> is the right performance objective. Remember that our goal is to find a better policy. The best value function for this purpose is not necessarily minimizing <span class="math inline">\(e\)</span>. Nevertheless, it is not yet clear what a more useful alternative goal might be.</p>
<p>Given objective <span class="math inline">\(e\)</span>, the goal is to minimize <span class="math inline">\(e\)</span>. That is, to find a global optimum, a weight vector <span class="math inline">\(\mathbf w^*\)</span> for which <span class="math inline">\(e(\mathbf w^*) \leq e(\mathbf w)\)</span> for all possible <span class="math inline">\(\mathbf w\)</span>. This is, not always possible. Complex function approximations may converge to a local optimum. Although this guarantee is only slightly reassuring, it is typically the best that can be done, and often it is enough.</p>
</section>
<section id="stochastic-gradient-and-semi-gradient-methods" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="stochastic-gradient-and-semi-gradient-methods"><span class="header-section-number">12.4</span> Stochastic-gradient and semi-gradient methods</h2>
<p>To minimise <span class="math inline">\(e\)</span>, we use <em>stochastic-gradient descent</em> (<em>SGD</em>) methods. SGD methods are widely used for function approximation and can be used in an online setting. Here we update <span class="math inline">\(\mathbf w\)</span> each time a new data sample <span class="math inline">\(S_t \mapsto v_\pi(S_t)\)</span> arrives (an example). That is, we have a sequence of updates <span class="math inline">\(\mathbf w_t, \mathbf w_{t+1}, \mathbf w_{t+2}, \ldots\)</span> and we assume that states appear in samples with the same distribution, <span class="math inline">\(\mu\)</span>. A good strategy in this case is to try to minimize error on the observed examples and adjust the weight vector after each sample by a small amount (<span class="math inline">\(\alpha&gt;0\)</span>): <span class="math display">\[
\begin{align}
\mathbf w_{t+1} &amp;= \mathbf w_t - \frac{1}{2}\alpha\nabla_{\mathbf w}\big[v_\pi(s) - \hat v(S_t;\mathbf w_t)\big]^2\\
  &amp;= \mathbf w_t + \alpha\,\big[v_\pi(s) - \hat v(S_t;\mathbf w_t)\big]\,\nabla_{\mathbf w}\hat v(S_t;\mathbf w_t).
\end{align}
\]</span> where <span class="math inline">\(\nabla_{\mathbf w}\hat v(S_t;\mathbf w_t)\)</span> denote the partial derivative vector (the gradient).</p>
<p>Note that since we approximate <span class="math inline">\(v_\pi(s)\)</span>, we do not know the exact value of <span class="math inline">\(v_\pi(s)\)</span>. Instead, we have a target output <span class="math inline">\(U_t\)</span> (an estimate of <span class="math inline">\(v_\pi(s)\)</span>). That is, our sample is now <span class="math inline">\(S_t \mapsto U_t\)</span> instead and our updates becomes:</p>
<p><span class="math display">\[
\mathbf w_{t+1} = \mathbf w_t + \alpha\,\big[U_t - \hat v(S_t;\mathbf w_t)\big]\,\nabla_{\mathbf w}\hat v(S_t;\mathbf w_t).
\]</span> If <span class="math inline">\(U_t\)</span> is an <em>unbiased estimate</em>, that is, if <span class="math inline">\(\mathbb E[U_t|S_t=s] = v_\pi(s)\)</span>, for all t, then <span class="math inline">\(w_t\)</span> is guaranteed to converge to a local optimum for decreasing <span class="math inline">\(\alpha\)</span>. In this case it is called a <em>gradient</em> descent method. If <span class="math inline">\(U_t\)</span> is a <em>biased estimate</em> (not independent on <span class="math inline">\(\mathbf w_t\)</span>) it is not guaranteed to converge to the true local optimum. We call these methods <em>semi-gradient</em> methods. We may choose <span class="math inline">\(U_t\)</span> as</p>
<ul>
<li>Gradient Monte Carlo: <span class="math inline">\(U_t = G_t\)</span> (full return). Unbiased in expectation and requires episode completion. High variance because the full return depends on all future rewards up to the end of the episode. Any randomness in transitions or rewards propagates through the entire sequence.</li>
<li>Semi-gradient TD(0): <span class="math inline">\(U_t = R_{t+1} + \gamma\,\hat v(S_{t+1};\mathbf w_t)\)</span>. Target depends on <span class="math inline">\(\mathbf w_t\)</span> (bias via bootstrapping). Lower variance since the target depends only on one reward and the current estimate of the expected reward of the next state value.</li>
</ul>
<p>Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case discussed in the next section. Moreover, they offer important advantages e.g significantly faster learning.</p>
</section>
<section id="linear-methods" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="linear-methods"><span class="header-section-number">12.5</span> Linear methods</h2>
<p>One important function class for function approximation is when the approximate function is a linear function of the weight vector.</p>
<p>Let <span class="math inline">\(\mathbf x(s)\in\mathbb R^d\)</span> be features and define <span id="eq-linear-approx"><span class="math display">\[
\hat v(s;\mathbf w) = \mathbf w^\top \mathbf x(s) = \sum_{i=1}^d w_ix_i(s).
\tag{12.1}\]</span></span></p>
<p>Note here, the gradient (vector with partial derivatives) is easy to calculate <span class="math display">\[
\nabla_{\mathbf w}\hat v(s;\mathbf w) = \mathbf x(s),
\]</span> and our update formula becomes</p>
<p><span class="math display">\[
\mathbf w_{t+1} = \mathbf w_t + \alpha\,\big[U_t - \hat v(S_t;\mathbf w_t)\big]\mathbf x(s).
\]</span> In the linear case the MSVE <span class="math inline">\(e(\textbf w)\)</span> is convex and has only one optimum, and thus any method that is guaranteed to converge is guaranteed to converge to the global optimum. That is, under gradient Monte Carlo <span class="math inline">\(\mathbf w_{t}\)</span> converges to the global optimum if <span class="math inline">\(\alpha\)</span> is reduced over time according to the usual conditions.</p>
<p>A semi-gradient method also converges, but not necessarily to the global optimum, but rather a point near the global optimum. The update at each time <span class="math inline">\(t\)</span> given the semi-gradient TD(0) becomes</p>
<p><span class="math display">\[\begin{align}
\mathbf w_{t+1}
      =&amp; \mathbf w_t + \alpha\left( R_{t+1} + \gamma\mathbf w_t^\top \mathbf x(S_{t+1}) - \mathbf w_t^\top \mathbf x(S_t) \right) \mathbf x(S_t) \\
      =&amp; \mathbf w_t + \alpha\left( R_{t+1}\mathbf x(S_t) - \mathbf x(S_t) \left(\mathbf x(S_t) - \gamma\mathbf x(S_{t+1})\right)^\top \mathbf w_t \right).

\end{align}\]</span></p>
<p>In expectation we have <span class="math display">\[\begin{align}
\mathbb E\left( \mathbf w_{t+1} | \mathbf w_{t}\right) =&amp; \mathbb E\left( \mathbf w_t + \alpha\left( R_{t+1}\mathbf x(S_t) - \mathbf x(S_t) \left(\mathbf x(S_t) - \gamma\mathbf x(S_{t+1})\right)^\top \mathbf w_t \right) \right) \\
    =&amp; \mathbf w_t + \alpha\left( \mathbf b - \mathbf A \mathbf w_t \right) \\

\end{align}\]</span></p>
<p>where <span class="math display">\[
\mathbf A = \mathbb E\big[ \mathbf x(S_t) \left(\mathbf x(S_t) - \gamma\mathbf x(S_{t+1})\right)^\top \big],\;
\mathbf b = \mathbb E\big[R_{t+1}\mathbf x(S_t)\big].
\]</span> If the system converges, it must converge to the weight vector <span class="math inline">\(w^*\)</span> satisfying <span id="eq-td-fixed-point"><span class="math display">\[
\mathbf b − \mathbf A\mathbf w^* = \mathbf 0 \Leftrightarrow \mathbf w^* = \mathbf A^{-1}\mathbf b
\tag{12.2}\]</span></span> The estimate <span class="math inline">\(w^*\)</span>is called the <em>TD fixed point</em>. Linear semi-gradient TD(0) converges to this point and the error is at most (in the continuing case) <span class="math display">\[
e(\textbf w^*) \leq \frac{1}{1-\gamma}\min_{\textbf w} e(\textbf w).
\]</span> Critical to these convergence results is that states are updated according to the on-policy distribution. For other update distributions the approximation may actually diverge to infinity.</p>
<p>In the next sections we will focus on different way of selecting features, but first let us present an example to be used.</p>
</section>
<section id="a-random-walk" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="a-random-walk"><span class="header-section-number">12.6</span> A random walk</h2>
<p>Consider a large random walk environment with 1,000 non-terminal states labelled 1 through 1000. Two terminal states lie just outside this range: state 0 on the left, which yields a reward of −1, and state 1001 on the right, which yields a reward of +1. Each episode begins in the center state, 500.</p>
<p>From any non-terminal state <span class="math inline">\(s\)</span>, the agent flips a fair coin to choose left or right, then samples a jump size <span class="math inline">\(k\)</span> uniformly from <span class="math inline">\(\{1, 2, \dots, 100\}\)</span>. The agent then moves to state <span class="math inline">\(s - k\)</span> or <span class="math inline">\(s + k\)</span>. If the resulting state lies outside <span class="math inline">\([1,1000]\)</span>, the episode terminates immediately with the corresponding terminal reward. All intermediate transitions give reward 0. The policy is fixed (a random policy as described) and uniform, with left and right chosen equally likely.</p>
<p>The goal is to estimate the value function <span class="math inline">\(v(s)\)</span> under this policy using function approximation.</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>During the lecture for this module, we will work with the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>. We implement different functions approximation algorithms. You may have a look at the notebook and the example before the lecture.</p>
</div>
</div>
</section>
<section id="feature-construction-for-linear-models" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="feature-construction-for-linear-models"><span class="header-section-number">12.7</span> Feature construction for linear models</h2>
<p>Linear methods are interesting because of their convergence guarantees, but also because in practice they can be very efficient in terms of both data and computation. This depends on how the states are represented in terms of features which are investigated in this section. Choosing features appropriate to the task is an important way of adding prior domain knowledge about the system. The features should correspond to the aspects of the state space along which generalization may be appropriate. We here consider different ways of constructing features.</p>
<section id="state-aggregation" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="state-aggregation"><span class="header-section-number">12.7.1</span> State aggregation</h3>
<p>State aggregation is a special case of linear function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. That is, the value of a state is estimated as its group’s value. Let <span class="math inline">\(g(s)\)</span> be a mapping that assigns each state <span class="math inline">\(s\)</span> to a group index. Then the features are <span class="math inline">\(\mathbf x_i(s) = 1\)</span> if <span class="math inline">\(g(s) = i\)</span> and <span class="math inline">\(\mathbf x_j(s)  = 0\)</span> for <span class="math inline">\(j\neq i\)</span> and <a href="#eq-linear-approx" class="quarto-xref">Equation&nbsp;<span>12.1</span></a> becomes <span class="math display">\[
\hat v(s; \mathbf{w}) = w_{g(s)}.
\]</span></p>
<p>Because <span class="math inline">\(\boldsymbol{x}(s)\)</span> is a unit vector with all entries equal to zero except entry <span class="math inline">\(g(s)\)</span>, then <span class="math display">\[
w_{g(s)} \leftarrow w_{g(s)} + \alpha \,\big(U_t - w_{g(s)}\big),
\]</span> and all other <span class="math inline">\(w_j\)</span> remain unchanged (the gradient is 1 for group <span class="math inline">\(g(s)\)</span> and 0 for the other components). That is, the approximation is a <em>piecewise constant function</em> that is constant for each group.</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an example see the corresponding section in the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>.</p>
</div>
</div>
</section>
<section id="polynomials" class="level3" data-number="12.7.2">
<h3 data-number="12.7.2" class="anchored" data-anchor-id="polynomials"><span class="header-section-number">12.7.2</span> Polynomials</h3>
<p>Polynomial features approximate <span class="math inline">\(v_\pi(s)\)</span> by fitting a low-degree polynomial of <em>normalized</em> state variables. Note the model remains linear in parameters <span class="math inline">\(\textbf w\)</span> even though it is non-linear in <span class="math inline">\(s\)</span>.</p>
<p>Polynomials can capture non-linear relationships in the value function and provide a smoother approximation compared to methods like state aggregation, especially when the true value function is smooth. However, choosing the appropriate degree of the polynomial is important. Too low a degree might not capture the complexity of the value function, while too high a degree can lead to over-fitting and poor generalization. Polynomials can also have difficulty approximating value functions with sharp changes or discontinuities.</p>
<p>Let us first consider a state <span class="math inline">\(s\)</span> where <span class="math inline">\(s\)</span> is a scalar. Here, the approximation is</p>
<p><span class="math display">\[
\hat{v} (s;\mathbf w) = \mathbf w^\top \mathbf x(s) = w_0 + w_1s + w_2s^2 + \ldots + w_{d}s^d.
\]</span></p>
<p>That is, the gradient becomes polynomial of degree <span class="math inline">\(d\)</span> <span class="math display">\[
\mathbf x(s) = (1,s,s^2,\dots,s^d).
\]</span></p>
<p>Second, if we have <em>multiple state variables</em> <span class="math inline">\(\mathbf s=(s_1,\dots,s_K)\)</span> and want to consider polynomials of degree <span class="math inline">\(d\)</span>, then each term/feature in the polynomial becomes</p>
<p><span class="math display">\[
\prod_{k=1}^K s_k^{i_k},\quad \sum_k i_k \le d.
\]</span></p>
<p>If all are included then the number of features are <span class="math inline">\(\binom{K+d}{d}\)</span>. Note the interactions (e.g., <span class="math inline">\(s_1s_2^3\)</span>) capture cross-effects (e.g., inventory × demand, tenure × engagement).</p>
<p><strong>Numerical instabilities</strong></p>
<p>A good strategy is always to normalise the state values because if <span class="math inline">\(s\)</span> has a high value, then <span class="math inline">\(s^d\)</span> may be very large, yielding numerical instabilities. If we normalize so state variables are in the interval <span class="math inline">\([-1,1]\)</span> then <span class="math inline">\(s^d\)</span> also lies in this interval. This can be done by:</p>
<ul>
<li><p>If <span class="math inline">\(s\in\{1,\dots,N\}\)</span>: use <span class="math inline">\(z = \dfrac{s}{N+1}\in(0,1)\)</span> or <span class="math inline">\(u = 2z-1 \in (-1,1)\)</span>.</p></li>
<li><p>If <span class="math inline">\(s\in[a,b]\)</span>: use <span class="math inline">\(u = \dfrac{2s-(a+b)}{b-a}\in(-1,1)\)</span>.</p></li>
</ul>
<p>Moreover, using <em>orthogonal polynomials</em> (e.g.&nbsp;the Legendre basis) may improve instabilities (not within the scope of the course).</p>
<p>To find the right step-size <span class="math inline">\(\alpha\)</span>, keep track of RMSVE. If it oscillates, then the step-size <span class="math inline">\(\alpha\)</span> is too high.</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an example see the corresponding section in the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>.</p>
</div>
</div>
</section>
<section id="fourier-basis" class="level3" data-number="12.7.3">
<h3 data-number="12.7.3" class="anchored" data-anchor-id="fourier-basis"><span class="header-section-number">12.7.3</span> Fourier basis</h3>
<p>The <em>Fourier basis</em> approximates <span class="math inline">\(v_\pi(s)\)</span> using global sine/cosine waves over a <em>normalized</em> state. The model remains <em>linear in parameters</em> even though it can represent highly non-linear shapes. The Fourier series/transforms are widely used because if a function to be approximated is known, then essentially any function can be approximated as accurately as desired. In reinforcement learning, where the functions to be approximated are unknown, Fourier basis functions are of interest because they are easy to use and can perform well.</p>
<p>Let us first consider a state <span class="math inline">\(s\)</span> where <span class="math inline">\(s\)</span> is a scalar. Here the Fourier series is a function having period <span class="math inline">\(\tau\)</span> which is a linear combination of sine and cosine functions that are each periodic with periods that multiplum of <span class="math inline">\(1/\tau\)</span>. Note, we are interested in approximating an aperiodic function defined over an interval. Hence, by setting <span class="math inline">\(\tau\)</span> to the length of the interval, the function of interest is then just one period of the periodic linear combination of the sine and cosine features. Furthermore, if you set <span class="math inline">\(\tau\)</span> to twice the length of the interval of interest and restrict attention to the approximation over the half interval [0, <span class="math inline">\(\tau/2\)</span>], then you can use just the cosine functions (see details in the book). Following this logic and letting <span class="math inline">\(\tau = 2\)</span>, the interval becomes <span class="math inline">\(s\in[0, 1]\)</span>.</p>
<p>The one-dimensional order-<span class="math inline">\(d\)</span> Fourier cosine basis consists of the <span class="math inline">\(d + 1\)</span> features. Here the approximation is</p>
<p><span class="math display">\[
\hat{v} (s;\mathbf w) = \mathbf w^\top \mathbf x(s) = w_0 + w_1\cos(\pi z) + w_2\cos(2\pi z) + \ldots + w_{d}\cos(d\pi z),
\]</span> where <span class="math inline">\(z\)</span> is the normalised state. That is, the entries in the gradient are <span class="math inline">\(x_i(z)=\cos(i\pi z), j=0,1,\dots,n,\)</span> where <span class="math inline">\(j=0\)</span> is the <em>bias</em> term (<span class="math inline">\(\cos(0)=1\)</span>).</p>
<p>If we have <em>multiple normalized state variables</em> <span class="math inline">\(\mathbf z=(z_1,\dots,z_K)\)</span>, we choose <em>frequency vectors</em> <span class="math inline">\(\mathbf c=(c_1,\dots,c_K)\)</span> with non-negative integers. Combinations can be limited by considering <span class="math inline">\(c\)</span> where <span class="math inline">\(\sum_k c_k\le n\)</span>, i.e.&nbsp;then number of features are <span class="math inline">\(\binom{K+n}{n}\)</span>. Each feature now becomes</p>
<p><span class="math display">\[
x_{\mathbf c}(\mathbf z)=\prod_{k=1}^K \cos\!\big(\pi c_k z_k\big).
\]</span></p>
<p><strong>Normalization of states</strong></p>
<p>Since we want the states normalized so <span class="math inline">\(s\in[0,1]\)</span> we do</p>
<ul>
<li><p>If <span class="math inline">\(s\in\{1,\dots,N\}\)</span>: use <span class="math inline">\(z = \dfrac{s}{N+1}\in(0,1)\)</span>.</p></li>
<li><p>If <span class="math inline">\(s\in[a,b]\)</span>: use <span class="math inline">\(z = \dfrac{s-a}{b-a}\in[0,1]\)</span>.</p></li>
</ul>
<!-- **Practical guidance** -->
<!-- - Normalize each dimension to $[0,1]$. -->
<!-- - Start with *cosines only*, $n\in[4,16]$ per dimension (problem-dependent). -->
<!-- - In higher-D, prefer a *total-frequency budget* $\sum_k c_k\le n$. -->
<!-- - Track *RMSVE*; if oscillatory, reduce $n$ and/or $\alpha$. -->
<!-- - If the target has strong *odd/asymmetric* components, include *sines*. -->
<!-- - With linear TD(0), stability tracks feature conditioning. If you raise $n$: -->
<!--   - lower the step size $\alpha$, -->
<!--   - optionally normalize features (mean/std), -->
<!--   - and prefer a *frequency budget* to curb very high $j$. -->
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an example see the corresponding section in the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>.</p>
</div>
</div>
</section>
<section id="coarse-coding" class="level3" data-number="12.7.4">
<h3 data-number="12.7.4" class="anchored" data-anchor-id="coarse-coding"><span class="header-section-number">12.7.4</span> Coarse coding</h3>
<p>Here, the idea is to cover the state space with many overlapping regions (often called <em>receptive fields</em> or <em>features</em>). A state activates the features whose regions contain it, producing a sparse binary vector. Coarse coding create <em>sparse</em>, <em>localized</em> features so that nearby states share parameters and thus generalize to each other.</p>
<p>If <span class="math inline">\(s\)</span> is a normalized scalar we cover the values of s with several intervals. For instance, at <span class="math inline">\(s=0.5\)</span>, perhaps three overlapping intervals contain <span class="math inline">\(0.5\)</span>, so three features turn on.</p>
<p>If <span class="math inline">\(s\)</span> have multiple state variables, regions might be discs/ellipses (2-D), balls/ellipsoids (3D), or any shapes convenient for the problem. Feature <span class="math inline">\(i\)</span> is “on” if <span class="math inline">\(s\)</span> falls inside the region.</p>
<p>The Linear approximation is</p>
<p><span class="math display">\[
\hat v(s;\mathbf w) \;=\; \mathbf w^\top \mathbf x(s) = \sum_{\{i | x_i(s) = 1\}} w_i,
\]</span></p>
<p>where <span class="math inline">\(x_i(s)\in\{0,1\}\)</span> (1 if region <span class="math inline">\(i\)</span> covers <span class="math inline">\(s\)</span>).</p>
<p>Because only a few fields cover any given state, <span class="math inline">\(\mathbf x(s)\)</span> is <em>sparse</em> and updates are cheap.</p>
<!-- *Generalization behavior.* If two states lie in many of the *same* receptive fields, they will share parameters and end up with similar predictions. Overlap controls the *radius of generalization*.

*Bias–variance dial.* Large fields (wide bumps) → smoother, more biased approximations. Smaller fields → finer detail but higher variance and more parameters. Overlap (how many fields fire at once) trades stability vs. resolution. -->
</section>
<section id="tile-coding" class="level3" data-number="12.7.5">
<h3 data-number="12.7.5" class="anchored" data-anchor-id="tile-coding"><span class="header-section-number">12.7.5</span> Tile coding</h3>
<p>Tile coding can be viewed as a structured, grid-based special case of coarse coding that is simple and fast. Here hyper-rectangular receptive fields arranged on offset grids. Overlap comes from having multiple tilings; each tiling itself does not overlap (it is a partition of the state space), but the union of tilings does.</p>
<p>That is, first create a <em>tiling</em> that partitions the space into non-overlapping <em>tiles</em>. Next, offset the tiling and hereby creating multiple tilings. Each tiling is offset slightly, so two nearby states are likely to share some tiles even if they fall on different sides of a grid boundary in one tiling. Steps for the scalar case becomes:</p>
<ul>
<li>Choose an integer number of tilings <span class="math inline">\(n\)</span> (e.g., <span class="math inline">\(n=8\)</span>).</li>
<li>In each tiling, split <span class="math inline">\([0,1]\)</span> into <span class="math inline">\(m\)</span> equal tiles (e.g., <span class="math inline">\(m=50\)</span>).</li>
<li>Offset each tiling by a small shift (e.g., <span class="math inline">\(0, \tfrac{1}{mn}, \tfrac{2}{mn}, \dots\)</span>) or randomly.</li>
<li>For a state <span class="math inline">\(s\)</span>, exactly one tile is active in each tiling; across <span class="math inline">\(n\)</span> tilings, you get <span class="math inline">\(n\)</span> active features.</li>
</ul>
<p>In multiple dimensions (<span class="math inline">\(k\)</span>), the only change is that each tiling is a Cartesian grid with <span class="math inline">\(n_1\times \cdots \times n_K\)</span> tiles.</p>
<p>Note that offsets matter since a single grid creates discontinuities at tile boundaries. Multiple offset grids ensure that two nearby states share most of their active features, smoothing the representation and reducing boundary artefacts.</p>
<p>Because each update touches only <span class="math inline">\(n\)</span> parameters, tile/coarse coding works <em>extremely well</em> with incremental semi-gradient TD: <span class="math display">\[
\mathbf w \leftarrow \mathbf w + \alpha\,\delta_t\,\mathbf x(S_t),\qquad
\delta_t = R_{t+1} + \gamma\,\hat v(S_{t+1}) - \hat v(S_t).
\]</span></p>
<p><em>Step-size scaling.</em> A common heuristic is to scale the per-tiling step size like <span class="math inline">\(\alpha \approx \frac{\alpha_0}{n}\)</span> (since <span class="math inline">\(n\)</span> weights are updated per step). This keeps the <em>total</em> update magnitude roughly controlled as you add more tilings.</p>
<p>Note that features that are polynomials or Fourier basis are <em>global</em>. That is, each update affects the entire space. Coarse/tile coding is <em>local</em>. That is, the value function is not globally smooth, has local structure and thus less prone to global oscillations.</p>
<p><strong>Choosing design parameters</strong></p>
<p><em>Number of tilings (</em><span class="math inline">\(m\)</span>). More tilings increase overlap and stability but cost more memory/compute.</p>
<p><em>Tiles per dimension.</em> Finer grids reduce bias (higher resolution) but increase variance and memory. Note that the function is piecewise-constant within each tile (per tiling). Summing across tilings reduces the effective piecewise step size, but if tiles are still large relative to the variation in <span class="math inline">\(v_\pi\)</span>, you will see bias.Smaller tiles and fewer overlapping tilings increase variance (fewer shared samples per parameter). More tilings and/or larger tiles reduce variance by pooling data.</p>
<p><em>Normalization.</em> Always normalize each state coordinate to a fixed range (e.g., <span class="math inline">\([0,1]\)</span>) before tiling so that “one tile” has a consistent meaning across features and tasks.</p>
<!-- *A practical loop.* Start with moderate $m$ (e.g., 8) and moderate resolution. If learning is noisy or unstable, *increase $m$* or *reduce $\alpha$*. If predictions are too blocky (high bias), *increase resolution* (more tiles per dimension) or *add tilings*. -->
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an example see the corresponding section in the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>.</p>
</div>
</div>
</section>
</section>
<section id="selecting-step-size-parameters-manually" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="selecting-step-size-parameters-manually"><span class="header-section-number">12.8</span> Selecting step-size parameters manually</h2>
<p>The goal is to pick a step size <span class="math inline">\(\alpha\)</span> that removes TD error quickly <em>without</em> causing instability.</p>
<p>In tabular prediction, a single update with target <span class="math inline">\(U_t\)</span> is <span class="math display">\[V(S_t) \leftarrow V(S_t) + \alpha\,[\,U_t - V(S_t)\,].\]</span> Think of <span class="math inline">\(\alpha\)</span> as how much you trust the newest target. Big enough to make visible progress, small enough to avoid creating noise. A step size of <span class="math inline">\(\alpha = 1/10\)</span> would take about 10 experiences to converge approximately to their mean target, and if we wanted to learn in 100 experiences we would use <span class="math inline">\(\alpha = 1/100\)</span>.</p>
<p>With general function approximation there is not such a clear notion of number of experiences with a state, as each state may be similar to and dissimilar from all the others to various degrees. Suppose you wanted to learn in about <span class="math inline">\(\tau\)</span> experiences with substantially the same feature vector. Then, good rule of thumb for setting the step-size parameter to</p>
<p><span class="math display">\[\begin{equation}
    \alpha = (\tau \mathbb{E}[\textbf{x}^T\textbf{x}])^{-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\textbf{x}\)</span> is a random feature vector chosen from the same distribution as input vectors will be in the SGD. This method works best if the feature vectors do not vary greatly in length; ideally <span class="math inline">\(\textbf{x}^T\textbf{x}\)</span> is a constant.</p>
<p>For instance, for tile coding with <span class="math inline">\(n\)</span> tilings we have that <span class="math display">\[\textbf{x}^T\textbf{x}=n,\]</span> and <span class="math display">\[\alpha = (\tau n)^{-1}.\]</span></p>
<p>Given a scalar state and a polynomial of degree <span class="math inline">\(d\)</span>, we have that <span class="math display">\[\textbf{x}^T\textbf{x}=(1, s, s^2, \ldots s^d)^T(1, s, s^2, \ldots s^d) = \sum_{i=0}^{d} (s^i)^2.\]</span> If the state is normalised to [-1,1] and we assume a uniform distribution then <span class="math display">\[\mathbb{E}[(s^i)^2] = 1/(2i + 1),\]</span> and the step-size becomes <span class="math display">\[\alpha = (\tau\sum_{i=0}^{d} 1/(2i+1))^{-1}.\]</span></p>
<!-- -   *Dense bases (polynomial/Fourier):* start with smaller $\alpha$ -->
<!--     since many coordinates move each step. -->
<p>If features activate unevenly, damp steps by visit counts: <span class="math display">\[
\alpha_i \;=\; \frac{\alpha_0}{1+n_i},
\]</span> where <span class="math inline">\(n_i\)</span> is how often feature <span class="math inline">\(i\)</span> has been active. This mimics tabular <span class="math inline">\(1/n\)</span> averaging and reduces variance for frequent features.</p>
<p><strong>Practical comments</strong></p>
<ul>
<li><p>Try a small grid of <span class="math inline">\(\alpha\)</span> (log-spaced). For each candidate: - run a short training phase, - track an error proxy (e.g., RMSVE), - pick the largest <span class="math inline">\(\alpha\)</span> that yields a smooth, monotone decline.</p></li>
<li><p>If curves spike or blow up, reduce <span class="math inline">\(\alpha\)</span> by a factor of <span class="math inline">\(2\)</span>–<span class="math inline">\(10\)</span>; if flat, increase with a factor <span class="math inline">\(2\)</span>–<span class="math inline">\(10\)</span>.</p></li>
<li><p>Feature specific</p>
<ul>
<li><em>Tile coding:</em> with <span class="math inline">\(n\in\{4,8,16\}\)</span>, try <span class="math inline">\(\alpha_0\in[0.1,0.4]\)</span> and use <span class="math inline">\(\alpha=\alpha_0/n\)</span>.<br>
Example: <span class="math inline">\(m=8 \Rightarrow \alpha\in[0.0125,0.05]\)</span> per weight.</li>
<li><em>Fourier (cos-only), max freq</em> <span class="math inline">\(n\in[4,16]\)</span>: start <span class="math inline">\(\alpha\in[10^{-4},10^{-3}]\)</span>. Lower <span class="math inline">\(n\)</span> allows larger <span class="math inline">\(\alpha\)</span>.</li>
<li><em>Polynomials (degree</em> <span class="math inline">\(3\)</span>–<span class="math inline">\(5\)</span>): start <span class="math inline">\(\alpha\in[10^{-4},10^{-3}]\)</span>; if you see boundary oscillations, lower <span class="math inline">\(\alpha\)</span>.</li>
</ul></li>
</ul>
<div class="callout callout-style-simple callout-note callout-titled" title="Colab - Before the lecture">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Colab - Before the lecture
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an example see the corresponding section in the <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">tutorial</a>.</p>
</div>
</div>
</section>
<section id="nonlinear-function-approximation-artificial-neural-networks" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="nonlinear-function-approximation-artificial-neural-networks"><span class="header-section-number">12.9</span> Nonlinear function approximation: artificial neural networks</h2>
<p><em>Artificial neural networks (ANNs)</em> are widely used nonlinear function approximators capable of representing complex mappings between inputs and outputs. In reinforcement learning, ANNs are employed to approximate value functions, policies, and models of the environment.</p>
<p>Deep neural networks, composed of multiple hidden layers, can automatically construct hierarchical representations of input data. Each successive layer extracts more abstract features, enabling the network to learn complex relationships without requiring handcrafted features.</p>
<p>ANNs are trained using stochastic gradient methods, where each connection weight is updated in a direction that improves the performance measure or objective function. In supervised learning, this typically involves minimizing the expected prediction error over labeled examples. In reinforcement learning, the objective function may involve minimizing temporal-difference (TD) errors when approximating value functions or maximizing expected reward in policy-gradient methods.</p>
<p>To adjust weights, the learning algorithm must estimate how small changes in each weight influence the overall performance. The backpropagation algorithm efficiently computes these partial derivatives for networks with differentiable activation functions. It alternates between a forward pass, computing activations through the network, and a backward pass, propagating errors to compute gradients for each weight. These gradients serve as stochastic estimates of the true gradient used to update the weights.</p>
<p>Although backpropagation can train networks with one or two hidden layers effectively, it struggles with deeper architectures. Increasing the number of layers can lead to poorer performance due to several factors such as overfitting (many parameters makes it difficult to generalize from limited training data).</p>
<p>Even though there are difficulties, research in deep ANNs is a hot topic, and big advancements have been made since the book was published. Using deep ANNs in RL is denoted <em>deep reinforcement learning</em>, and many RL courses focus on this. Moreover, there are many implementations of deep RL algorithms.</p>
</section>
<section id="least-squares-temporal-difference-learning" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="least-squares-temporal-difference-learning"><span class="header-section-number">12.10</span> Least-squares temporal-difference learning</h2>
<p>Consider <a href="#eq-td-fixed-point" class="quarto-xref">Equation&nbsp;<span>12.2</span></a> for calculating the TD fixed point. <em>Least-squares TD (LSTD)</em> estimate the weights using this equation by estimating matrix <span class="math inline">\(\textbf A\)</span> and vector <span class="math inline">\(\textbf b\)</span> incrementally from experience.</p>
<p>The key advantages of LSTD are:</p>
<ul>
<li>it eliminates the need for a step-size parameter,</li>
<li>it converges faster than incremental TD for stationary problems,</li>
<li>it produces an exact least-squares solution under linear function approximation.</li>
</ul>
<p>However, LSTD also has important limitations:</p>
<ul>
<li>it lacks a forgetting mechanism, making it unsuitable when the target policy changes (as in control problems),</li>
<li>it can be sensitive to numerical instabilities,</li>
<li>it requires storage proportional to <span class="math inline">\(d\^2\)</span>, which can be prohibitive for high-dimensional features.</li>
</ul>
</section>
<section id="memory-based-function-approximation" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="memory-based-function-approximation"><span class="header-section-number">12.11</span> Memory-based function approximation</h2>
<p>Memory-based function approximation have different approach to estimating the state value. Here no set of parameters are adjusted, as in parametric methods. Instead, the agent stores examples of experience pairs <span class="math inline">\(s \mapsto g\)</span> of states and their estimated returns (the state value). When an estimate of the state value in a <em>query state</em> <span class="math inline">\(s'\)</span> is wanted, the estimates in memory is used directly. That is, there is no parameters and calculation is <em>lazy</em>, since learning is deferred until an estimate is required.</p>
<p>When the value of a query state is requested, the algorithm retrieves from memory a set of stored examples with a short ‘distance’ to the query state. Here, the distance can be defined as a <em>kernel</em> (function) that assigns a number between two states (see Section 9.10 in <span class="citation" data-cites="Sutton18">Sutton and Barto (<a href="references.html#ref-Sutton18" role="doc-biblioref">2018</a>)</span> for further details).</p>
<p>Given the kernel, different methods can be used to calculate the state value of the query state. For instance, <em>local search</em> based methods could be <em>nearest neighbour</em>, where the value of the query state is taken to be that of the most similar stored example or <em>weighted average</em> methods, which generalise this by using several neighbours and computing a distance-weighted mean. Another method is <em>locally weighted regression</em> that fits a parametric model to the neighbourhood of the query state.</p>
<p>These methods can be advantageous in reinforcement learning because they focus approximation effort on regions of the state space actually encountered by the agent, avoiding the need for a global model. However, their main drawback is computational cost: retrieving and comparing neighbours becomes increasingly expensive as more examples are stored, particularly in high-dimensional spaces.</p>
</section>
<section id="on-policy-learning-with-interestemphasis" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="on-policy-learning-with-interestemphasis"><span class="header-section-number">12.12</span> On-policy learning with interest/emphasis</h2>
<p>In traditional on-policy learning methods such as TD(0), all states visited under the policy receive equal attention. Each state contributes equally to the learning process, regardless of how important it may be for accurate value prediction or control. Section 9.11 introduces the ideas of <em>interest</em> and <em>emphasis</em> to refine how learning updates are distributed across states. Here, <em>interest</em> <span class="math inline">\(I_t\)</span> denotes external weighting of states/times) and <em>emphasis</em> <span class="math inline">\(M_t\)</span> the accumulated weighting. As a result updates prioritize these parts of the on-policy distribution.</p>
</section>
<section id="summary" class="level2" data-number="12.13">
<h2 data-number="12.13" class="anchored" data-anchor-id="summary"><span class="header-section-number">12.13</span> Summary</h2>
<p>Read Chapter 9.12 in <span class="citation" data-cites="Sutton18">Sutton and Barto (<a href="references.html#ref-Sutton18" role="doc-biblioref">2018</a>)</span>.</p>
</section>
<section id="exercises" class="level2" data-number="12.14">
<h2 data-number="12.14" class="anchored" data-anchor-id="exercises"><span class="header-section-number">12.14</span> Exercises</h2>
<p>Below you will find a set of exercises. Always have a look at the exercises before you meet in your study group and try to solve them yourself. Are you stuck, see the <a href="99_2_appdx.html">help page</a>. Sometimes hints and solutions can be revealed. Beware, you will not learn by giving up too early. Put some effort into finding a solution!</p>
<p>You may solve the exercises in the corresponding sections in this <a href="https://colab.research.google.com/drive/1-kh0SiNucJrzUUnIOLSidcA2RO5J1rvY?usp=sharing">Colab notebook</a>.</p>
<section id="exercise" class="level3" data-number="12.14.1">
<h3 data-number="12.14.1" class="anchored" data-anchor-id="exercise"><span class="header-section-number">12.14.1</span> Exercise</h3>
<p>Show that tabular methods, such as those presented in Part I of this book, are a special case of linear function approximation. What would the feature vectors be?</p>
</section>
<section id="exercise-1" class="level3" data-number="12.14.2">
<h3 data-number="12.14.2" class="anchored" data-anchor-id="exercise-1"><span class="header-section-number">12.14.2</span> Exercise</h3>
<p>Suppose we believe that one of the two state dimensions (let us say <span class="math inline">\(x_1\)</span>) is more likely to have an effect on the value function than is the other (<span class="math inline">\(x_2\)</span>)</p>
<p>What kind of tilings could be used to take advantage of this prior knowledge?</p>
</section>
<section id="exercise-2" class="level3" data-number="12.14.3">
<h3 data-number="12.14.3" class="anchored" data-anchor-id="exercise-2"><span class="header-section-number">12.14.3</span> Exercise</h3>
<p>Suppose you are using tile coding to transform a seven-dimensional continuous state space into binary feature vectors to estimate a state value function. You believe that the dimensions do not interact strongly, so you decide to use eight tilings of each dimension separately. That is, <span class="math inline">\(7 \cdot 8 = 56\)</span> tilings. In addition, in case there are some pairwise interactions between the dimensions, you also take all pairs of dimensions and tile each pair conjunctively. You make two tilings for each pair of dimensions, making a grand total of 21 + 56 = 98 tilings.</p>
<p>Given these feature vectors, you suspect that you still have to average out some noise, so you decide that you want learning to be gradual, taking about 10 presentations with the same feature vector first.</p>
<p>What step-size parameter should you use? Why?</p>
</section>
<section id="exercise-3" class="level3" data-number="12.14.4">
<h3 data-number="12.14.4" class="anchored" data-anchor-id="exercise-3"><span class="header-section-number">12.14.4</span> Exercise</h3>
<p>Consider the car rental problem described in <a href="07_mdp-2.html#sec-mdp-2-car" class="quarto-xref"><span>Module 7.8.2</span></a>.</p>
<p>Approximate the state value under the policy <span class="math display">\[
a = \begin{cases}
\lfloor x/5 + 1 \rfloor &amp; x &gt; 5, y &lt; 5 \\
-\lfloor y/5 + 1 \rfloor &amp; y &gt; 5, x &lt; 5 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Sutton18" class="csl-entry" role="listitem">
Sutton, R. S., and A. G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second Edition. MIT Press. <a href="http://incompleteideas.net/book/the-book.html">http://incompleteideas.net/book/the-book.html</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11_td-control.html" class="pagination-link" aria-label="Temporal difference methods for control">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Temporal difference methods for control</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13_approx-control.html" class="pagination-link" aria-label="On-Policy Control with Approximation">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Policy Control with Approximation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bss-osca/rl/edit/master/book/12_approx-pred.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bss-osca/rl/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/bss-osca/rl/blob/master/book/12_approx-pred.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div></div></footer><script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>